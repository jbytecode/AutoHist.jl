<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Supported Methods · AutoHist.jl</title><meta name="title" content="Supported Methods · AutoHist.jl"/><meta property="og:title" content="Supported Methods · AutoHist.jl"/><meta property="twitter:title" content="Supported Methods · AutoHist.jl"/><meta name="description" content="Documentation for AutoHist.jl."/><meta property="og:description" content="Documentation for AutoHist.jl."/><meta property="twitter:description" content="Documentation for AutoHist.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">AutoHist.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Introdution</a></li><li class="is-active"><a class="tocitem" href>Supported Methods</a><ul class="internal"><li><a class="tocitem" href="#Irregular-histograms"><span>Irregular histograms</span></a></li><li><a class="tocitem" href="#Regular-histograms"><span>Regular histograms</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../api/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Supported Methods</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Supported Methods</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/oskarhs/AutoHist.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/oskarhs/AutoHist.jl/blob/main/docs/src/methods.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Supported-Methods"><a class="docs-heading-anchor" href="#Supported-Methods">Supported Methods</a><a id="Supported-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Supported-Methods" title="Permalink"></a></h1><p>This page provides background on each histogram method supported through the <code>rule</code> argument. Our presentation is intended to be rather brief, and we do as such not cover the theoretical underpinnings of each method in any detail. For some further background on automatic histogram procedures and the theory behind them, we recommend the excellent reviews contained in the articles of Birgé and Rozenholc (2006) and Davies et al. (2009).</p><p>For ease of exposition, we present all methods covered here in the context of estimating the density of a sample <span>$\boldsymbol{x} = (x_1, x_2, \ldots, x_n)$</span> on the unit interval, but note that extending the procedures presented here to other compact intervals is possible through a suitable affine transformation. In particular, if a density estimate with support <span>$[a,b]$</span> is desired, we can scale the data to the unit interval through <span>$z_i = (x_i - a)/(b-a)$</span>, and apply the methods on this transformed sample and rescale the resulting density estimate to <code>[a,b]</code>. In cases where the support of the density is unknown, a natural choice is <span>$a = x_{(1)}$</span> and <span>$b = x_{(2)}$</span>. Cases where only the lower or upper bound is known can be handled similarly. The transformation used to construct the histogram can be controlled through the <code>support</code> keyword, where the default argument <code>support=(-Inf, Inf)</code> uses the order statistics-based approach described above.</p><p>Before we describe the methods included here in more detail, we introduce some notation. We let <span>$\mathcal{I} = (\mathcal{I}_1, \mathcal{I}_2, \ldots, \mathcal{I}_k)$</span> denote a partition of <span>$[0,1]$</span> into <span>$k$</span> intervals and write <span>$|\mathcal{I}_j|$</span> for the length of interval <span>$\mathcal{I}_j$</span>. We can then write a histogram density estimate by</p><p class="math-container">\[\widehat{f}(x) = \sum_{j=1}^k \frac{\widehat{\theta}_j}{|\mathcal{I}_j|}\mathbf{1}_{\mathcal{I}_j}(x), \quad x\in [0,1],\]</p><p>where <span>$\mathbf{1}_{\mathcal{I}_j}$</span> is the indicator function, <span>$\widehat{\theta}_j \geq 0$</span> for all <span>$j$</span> and <span>$\sum_{j=1}^k \widehat{\theta}_j = 1$</span>.</p><p>For most of the methods considered here, the estimated bin probabilities are the maximum likelihood estimates <span>$\widehat{\theta}_j = N_j/n$</span>, where <span>$N_j = \sum_{i=1}^n \mathbb{1}_{\mathcal{I}_j}(x_i)$</span> is number of observations landing in interval <span>$\mathcal{I}_j$</span> . The exception to this rule is are the two Bayesian approaches, which uses the Bayes estimator <span>$\widehat{\theta}_j = (a_j + N_j)/(a+n)$</span> for <span>$(a_1, \ldots, a_k) \in (0,\infty)$</span> and <span>$a = \sum_{j=1}^k a_j$</span> instead.</p><p>The goal of an automatic histogram procedure is to find a partition <span>$\mathcal{I}$</span> based on the sample alone which produces a reasonable density estimate. Regular histogram procedures only consider regular partitions, where all intervals in the partition are of equal length, so that one only needs to determine the number <span>$k$</span> of bins. Irregular histograms allow for partitions with intervals of unequal length, and try to determine both the number of bins and the locations of the cutpoints between the intervals. In all the irregular procedures covered here, we attempt to find best partition according to a criterion among all partitions with endpoints belonging to a given discrete mesh.</p><h2 id="Irregular-histograms"><a class="docs-heading-anchor" href="#Irregular-histograms">Irregular histograms</a><a id="Irregular-histograms-1"></a><a class="docs-heading-anchor-permalink" href="#Irregular-histograms" title="Permalink"></a></h2><p>The following section describes how each value of the <code>rule</code> keyword supported by the <code>histogram_irregular</code> function selects the optimal histogram partition. In each case, the best partition is selected among the subset of partitions which have cut points belonging to a discrete set of <span>$k_n$</span> possible cut points.</p><h4 id="bayes:"><a class="docs-heading-anchor" href="#bayes:">bayes:</a><a id="bayes:-1"></a><a class="docs-heading-anchor-permalink" href="#bayes:" title="Permalink"></a></h4><p>Consists of maximizing the log-marginal likelihood conditional on the partition <span>$\mathcal{I} = (\mathcal{I}_1, \ldots, \mathcal{I}_k)$</span>,</p><p class="math-container">\[    \sum_{j=1}^k \big\{\log \Gamma(a_j + N_j) - \log \Gamma(a_j) - N_j\log|\mathcal{I}_j|\big\} + \log p_n(k) - \log \binom{k_n-1}{k-1}\]</p><p>Here <span>$p_n(k)$</span> is the prior distribution on the number <span>$k$</span> of bins, which can be controlled by supplying a function to the <code>logprior</code> keyword argument. The default value is <span>$p_n(k) \propto 1$</span>. Here, <span>$a_j = a/k$</span>, for a scalar <span>$a &gt; 0$</span> which can be controlled by the user through the keyword argument <code>a</code>.</p><p>This approach to irregular histograms was pioneered by Simensen et al. (2025).</p><h4 id="penb:"><a class="docs-heading-anchor" href="#penb:">penb:</a><a id="penb:-1"></a><a class="docs-heading-anchor-permalink" href="#penb:" title="Permalink"></a></h4><p>Consists of maximizing a penalized log-likelihood,</p><p class="math-container">\[    \sum_{j=1}^k N_j \log (N_j/|\mathcal{I}_j|) - \log \binom{k_n-1}{k-1} - k - \log^{2.5}(k).\]</p><p>This approach was suggested by Rozenholc et al. (2010).</p><h4 id="penr:"><a class="docs-heading-anchor" href="#penr:">penr:</a><a id="penr:-1"></a><a class="docs-heading-anchor-permalink" href="#penr:" title="Permalink"></a></h4><p>Consists of maximizing a penalized log-likelihood,</p><p class="math-container">\[    \sum_{j=1}^k N_j \log (N_j/|\mathcal{I}_j|) - \frac{1}{2n}\sum_{j=1}^k \frac{N_j}{|\mathcal{I}_j|} - \log \binom{k_n-1}{k-1} - \log^{2.5}(k).\]</p><p>This criterion was also suggested by Rozenholc et al. (2010).</p><h4 id="l2cv:"><a class="docs-heading-anchor" href="#l2cv:">l2cv:</a><a id="l2cv:-1"></a><a class="docs-heading-anchor-permalink" href="#l2cv:" title="Permalink"></a></h4><p>Consists of maximization of an L2 leave-one-out cross-validation criterion,</p><p class="math-container">\[    \frac{n+1}{n}\sum_{j=1}^k \frac{N_j^2}{|\mathcal{I}_j|} - 2\sum_{j=1}^k \frac{N_j}{|\mathcal{I}_j|}.\]</p><p>This approach dates back to Rudemo (1982).</p><h4 id="klcv:"><a class="docs-heading-anchor" href="#klcv:">klcv:</a><a id="klcv:-1"></a><a class="docs-heading-anchor-permalink" href="#klcv:" title="Permalink"></a></h4><p>Consists of maximization of a Kullback-Leibler cross-validation criterion,</p><p class="math-container">\[    \sum_{j=1}^k N_j\log(N_j-1) - \sum_{j=1}^k N_j\log |I_j|,\]</p><p>where the maximmization is over all partitions with <span>$N_j \geq 2$</span> for all <span>$j$</span>. This approach was, to our knowledge, first pursued by Simensen et al. (2025).</p><h4 id="nml:"><a class="docs-heading-anchor" href="#nml:">nml:</a><a id="nml:-1"></a><a class="docs-heading-anchor-permalink" href="#nml:" title="Permalink"></a></h4><p>Consists of maximization of a penalized likelihood,</p><p class="math-container">\[\begin{aligned}
    &amp;\sum_{j=1}^k N_j\log \frac{N_j}{|\mathcal{I}_j|} - \frac{k-1}{2}\log(n/2) - \log\frac{\sqrt{\pi}}{\Gamma(k/2)} - n^{-1/2}\frac{\sqrt{2}k\Gamma(k/2)}{3\Gamma(k/2-1/2)} \\
    &amp;- n^{-1}\left(\frac{3+k(k-2)(2k+1)}{36} - \frac{\Gamma(k/2)^2 k^2}{9\Gamma(k/2-1/2)^2} \right)  - \log \binom{k_n-1}{k-1}
\end{aligned}\]</p><p>This a variant of this criterion first suggested by Kontkanen and Myllymäki (2007). The above criterion uses an asymptotic expansion of their proposed penalty term, as their proposed penalty can be quite expensive to evaluate.</p><h2 id="Regular-histograms"><a class="docs-heading-anchor" href="#Regular-histograms">Regular histograms</a><a id="Regular-histograms-1"></a><a class="docs-heading-anchor-permalink" href="#Regular-histograms" title="Permalink"></a></h2><p>The following section details how each value of the <code>rule</code> keyword supported by the <code>histogram_regular</code> function selects the number <span>$k$</span> of bins to draw a histogram automatically based on a random sample. In the following, <span>$\mathcal{I} = (\mathcal{I}_1, \mathcal{I}_2, \ldots, \mathcal{I}_k)$</span> is the corresponding partition of <span>$[0,1]$</span> consisting of <span>$k$</span> equal-length bins.</p><h4 id="bayes:-2"><a class="docs-heading-anchor" href="#bayes:-2">bayes:</a><a class="docs-heading-anchor-permalink" href="#bayes:-2" title="Permalink"></a></h4><p>Consists of maximizing the log-marginal likelihood for given <span>$k$</span>,</p><p class="math-container">\[   n\log (k) + \sum_{j=1}^k \big\{\log \Gamma(a_j + N_j) - \log \Gamma(a_j)\big\} + \log p_n(k).\]</p><p>Here <span>$p_n(k)$</span> is the prior distribution on the number <span>$k$</span> of bins, which can be controlled by supplying a function to the <code>logprior</code> keyword argument. The default value is <span>$p_n(k) \propto 1$</span>. Here, <span>$a_j = a/k$</span>, for a scalar <span>$a &gt; 0$</span>, possibly depending on <span>$k$</span>. The value of <span>$a$</span> can be set by supplying a fixed, positive scalar or a function <span>$a(k)$</span> to the keyword argument <code>a</code>.</p><p>The particular choices <span>$a_j = 0.5$</span> and <span>$p_n(k)\propto 1$</span> were suggested by Knuth (2019).</p><h4 id="aic:"><a class="docs-heading-anchor" href="#aic:">aic:</a><a id="aic:-1"></a><a class="docs-heading-anchor-permalink" href="#aic:" title="Permalink"></a></h4><p>Consists of maximizing a penalized log-likelihood,</p><p class="math-container">\[    n\log (k) + \sum_{j=1}^k N_j \log (N_j/n) - k.\]</p><p>The aic criterion was proposed by Taylor (1987) for histograms.</p><h4 id="bic:"><a class="docs-heading-anchor" href="#bic:">bic:</a><a id="bic:-1"></a><a class="docs-heading-anchor-permalink" href="#bic:" title="Permalink"></a></h4><p>Consists of maximizing a penalized log-likelihood,</p><p class="math-container">\[    n\log (k) + \sum_{j=1}^k N_j \log (N_j/n) - \frac{k}{2}\log(n).\]</p><h4 id="br:"><a class="docs-heading-anchor" href="#br:">br:</a><a id="br:-1"></a><a class="docs-heading-anchor-permalink" href="#br:" title="Permalink"></a></h4><p>Consists of maximizing a penalized log-likelihood,</p><p class="math-container">\[    n\log (k) + \sum_{j=1}^k N_j \log (N_j/n) - k - \log^{2.5}(k).\]</p><p>This criterion was proposed by Birgé and Rozenholc (2006).</p><h4 id="l2cv:-2"><a class="docs-heading-anchor" href="#l2cv:-2">l2cv:</a><a class="docs-heading-anchor-permalink" href="#l2cv:-2" title="Permalink"></a></h4><p>Consists of maximizing a L2 leave-one-out cross-validation criterion,</p><p class="math-container">\[    -2k + k\frac{n+1}{n^2}\sum_{j=1}^k N_j^2.\]</p><p>This approach to histogram density estimation was first considered by Rudemo (1982).</p><h4 id="klcv:-2"><a class="docs-heading-anchor" href="#klcv:-2">klcv:</a><a class="docs-heading-anchor-permalink" href="#klcv:-2" title="Permalink"></a></h4><p>Consists of maximizing a Kullback-Leibler leave-one-out cross-validation criterion,</p><p class="math-container">\[    n\log(k) + \sum_{j=1}^k N_j\log (N_j-1),\]</p><p>where the maximmization is over all partitions with <span>$N_j \geq 2$</span> for all <span>$j$</span>. This approach was first studied by Hall (1990).</p><h4 id="mdl:"><a class="docs-heading-anchor" href="#mdl:">mdl:</a><a id="mdl:-1"></a><a class="docs-heading-anchor-permalink" href="#mdl:" title="Permalink"></a></h4><p>Consists of finding the model providing the shortest encoding of the data, which is equivalent to maximization of</p><p class="math-container">\[    n\log(k) + \sum_{j=1}^k \big(N_j-\frac{1}{2}\big)\log\big(N_j-\frac{1}{2}\big) - \big(n-\frac{k}{2}\big)\log\big(n-\frac{k}{2}\big) - \frac{k}{2}\log(n),\]</p><p>where the maximmization is over all partitions with <span>$N_j \geq 1$</span> for all <span>$j$</span>. The minimum description length principle was first applied to histogram estimation by Hall and Hannan (1988).</p><h4 id="nml:-2"><a class="docs-heading-anchor" href="#nml:-2">nml:</a><a class="docs-heading-anchor-permalink" href="#nml:-2" title="Permalink"></a></h4><p>Consists of maximization of a penalized likelihood,</p><p class="math-container">\[\begin{aligned}
    &amp;\sum_{j=1}^k N_j\log \frac{N_j}{|\mathcal{I}_j|} - \frac{k-1}{2}\log(n/2) - \log\frac{\sqrt{\pi}}{\Gamma(k/2)} - n^{-1/2}\frac{\sqrt{2}k\Gamma(k/2)}{3\Gamma(k/2-1/2)} \\
    &amp;- n^{-1}\left(\frac{3+k(k-2)(2k+1)}{36} - \frac{\Gamma(k/2)^2 k^2}{9\Gamma(k/2-1/2)^2} \right).
\end{aligned}\]</p><p>This is a regular variant of the normalized maximum likelihood criterion considered by Kontkanen and Myllymäki (2007).</p><h4 id="Sturges&#39;-rule:"><a class="docs-heading-anchor" href="#Sturges&#39;-rule:">Sturges&#39; rule:</a><a id="Sturges&#39;-rule:-1"></a><a class="docs-heading-anchor-permalink" href="#Sturges&#39;-rule:" title="Permalink"></a></h4><p>The number <span>$k$</span> of bins is computed according to the formula</p><p class="math-container">\[    k = \lceil \log_2(n) \rceil + 1.\]</p><p>This classical rule, due to Sturges (1926), is the default for determining the number of bins in R.</p><h4 id="Freedman-and-Diaconis&#39;-rule:"><a class="docs-heading-anchor" href="#Freedman-and-Diaconis&#39;-rule:">Freedman and Diaconis&#39; rule:</a><a id="Freedman-and-Diaconis&#39;-rule:-1"></a><a class="docs-heading-anchor-permalink" href="#Freedman-and-Diaconis&#39;-rule:" title="Permalink"></a></h4><p>The number <span>$k$</span> of bins is computed according to the formula</p><p class="math-container">\[    k = \big\lceil\frac{n^{1/3}}{2\mathrm{IQR}(\boldsymbol{x})}\big\rceil,\]</p><p>where <span>$\mathrm{IQR}(\boldsymbol{x})$</span> is the sample interquartile range. This rule dates back to Freedman and Diaconis (1982) and is the default bin selection rule used by the <code>histogram()</code> function from Plots.jl.</p><h4 id="Scott&#39;s-rule:"><a class="docs-heading-anchor" href="#Scott&#39;s-rule:">Scott&#39;s rule:</a><a id="Scott&#39;s-rule:-1"></a><a class="docs-heading-anchor-permalink" href="#Scott&#39;s-rule:" title="Permalink"></a></h4><p>The number <span>$k$</span> of bins is computed according to the formula</p><p class="math-container">\[    k = \big\lceil \hat{\sigma}^{-1}(24\sqrt{\pi})^{-1/3}n^{1/3}\big\rceil,\]</p><p>where <span>$\hat{\sigma}$</span> is the sample standard deviation. Scott&#39;s normal reference rule was first proposed by Scott (1979).</p><h4 id="Wand&#39;s-rule"><a class="docs-heading-anchor" href="#Wand&#39;s-rule">Wand&#39;s rule</a><a id="Wand&#39;s-rule-1"></a><a class="docs-heading-anchor-permalink" href="#Wand&#39;s-rule" title="Permalink"></a></h4><p>A more sophisticated version of Scott&#39;s rule, Wand&#39;s rule proceeds by determining the bin width <span>$h$</span> as</p><p class="math-container">\[    h = \Big(\frac{6}{\hat{C}(f_0) n}\Big)^{1/3},\]</p><p>where <span>$\hat{C}(f_0)$</span> is an estimate of a functional <span>$C(f_0)$</span>. The corresponding number of bins <span>$k = \lceil h^{-1}\rceil$</span>. The full details on this method are given in Wand (1997). The density estimate is computed based on a scale estimate, which can be controlled through the <code>scale</code> keyword argument. Possible choices are <code>:stdev</code>, <code>:iqr</code> which uses an estimate based on the sample standard deviation or the sample interquartile range as a scale estimate. The default choice <code>:minim</code> uses the minimum of the above estimates. The <code>level</code> keyword controls the number of stages of functional estimation used to compute <span>$\hat{C}$</span>, and can take values <code>0, 1, 2, 3, 4, 5</code>, with the default value being <code>level=2</code>. The choice <code>level=0</code> corresponds to Scott&#39;s rule under the chosen scale estimate.</p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><p>Simensen, O. H., Christensen, D. &amp; Hjort, N. L. (2025). Random Irregular Histograms. <em>arXiv preprint</em>. doi: <a href="https://doi.org/10.48550/ARXIV.2505.22034">10.48550/ARXIV.2505.22034</a></p><p>Taylor, C. C. (1987). Akaike’s information criterion and the histogram. <em>Biometrika</em>, <strong>74</strong>, 636–639. doi: <a href="https://doi.org/10.1093/biomet/74.3.636">10.1093/biomet/74.3.636</a></p><p>Rozenholc, Y., Mildenberger, T., &amp; Gather, U. (2010). Combining regular and irregular histograms by penalized likelihood. <em>Computational Statistics &amp; Data Analysis</em>, <strong>54</strong>, 3313–3323. doi: <a href="https://doi.org/10.1016/j.csda.2010.04.021">10.1016/j.csda.2010.04.021</a></p><p>Birgé, L., &amp; Rozenholc, Y. (2006). How many bins should be put in a regular histogram. <em>ESAIM: Probability and Statistics</em>, <strong>10</strong>, 24–45. doi: <a href="https://doi.org/10.1051/ps:2006001">10.1051/ps:2006001</a></p><p>Rudemo, M. (1982). Empirical choice of histograms and kernel density estimators. <em>Scandinavian Journal of Statistics</em>, <strong>9</strong>, 65-78</p><p>Hall, P. (1990). Akaike’s information criterion and Kullback–Leibler loss for histogram density estimation. <em>Probability Theory and Related Fields</em>, <strong>85</strong>, 449–467. doi: <a href="https://doi.org/10.1007/BF01203164">10.1007/BF01203164</a></p><p>Hall, P. and Hannan, E. J. (1988). On stochastic complexity and nonparametric density estimation. <em>Biometrika</em>, <strong>75</strong>, 705–714. doi: <a href="https://doi.org/10.1093/biomet/75.4.705">10.1093/biomet/75.4.705</a></p><p>Knuth, K. H. (2019). Optimal data-based binning for histograms and histogram-based probability density models. <em>Digital Signal Processing</em>, <strong>95</strong>, doi: <a href="https://doi.org/10.1016/j.dsp.2019.102581">10.1016/j.dsp.2019.102581</a></p><p>Kontkanen, P. and Myllymäki, P. (2007). Mdl histogram density estimation. <em>Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics</em>, <strong>2</strong>, 219–226</p><p>Sturges, H. A. (1926). The choice of a class interval. <em>Journal of the American Statistical Association</em>, <strong>21</strong>, 65–66. doi: <a href="https://doi.org/10.1080/01621459.1926.10502161">10.1080/01621459.1926.10502161</a>.</p><p>Freedman, D. and Diaconis, P. (1981) On the histogram as a density estimator: L2 theory. <em>Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete</em>, <strong>57</strong>, 453–476. doi: <a href="https://doi.org/10.1007/BF01025868">10.1007/BF01025868</a>.</p><p>Scott, D. W. (1979). On optimal and data-based histograms. <em>Biometrika</em>, <strong>66</strong>, 605–610, doi: <a href="https://doi.org/10.1093/biomet/66.3.605">10.1093/biomet/66.3.605</a>.</p><p>Wand, M. P. (1997). Data-based choice of histogram bin width. <em>The American Statistician</em>, <strong>51</strong>, 59–64. doi: <a href="https://doi.org/10.2307/2684697">10.2307/2684697</a></p><p>Davies, P. L., Gather, U., Nordman, D., and Weinert, H. (2009). A comparison of automatic histogram constructions. <em>ESAIM: Probability and Statistics</em>, <strong>13</strong>, 181–196. doi: <a href="https://doi.org/10.1051/ps:2008005">10.1051/ps:2008005</a>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Introdution</a><a class="docs-footer-nextpage" href="../api/">API »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.11.4 on <span class="colophon-date" title="Wednesday 4 June 2025 11:47">Wednesday 4 June 2025</span>. Using Julia version 1.10.9.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
