<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Supported Methods · AutoHist.jl</title><meta name="title" content="Supported Methods · AutoHist.jl"/><meta property="og:title" content="Supported Methods · AutoHist.jl"/><meta property="twitter:title" content="Supported Methods · AutoHist.jl"/><meta name="description" content="Documentation for AutoHist.jl."/><meta property="og:description" content="Documentation for AutoHist.jl."/><meta property="twitter:description" content="Documentation for AutoHist.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">AutoHist.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Introdution</a></li><li class="is-active"><a class="tocitem" href>Supported Methods</a><ul class="internal"><li><a class="tocitem" href="#Irregular-histograms"><span>Irregular histograms</span></a></li><li><a class="tocitem" href="#Regular-histograms"><span>Regular histograms</span></a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/density_estimation/">Density estimation</a></li><li><a class="tocitem" href="../examples/algorithm_choice/">Choice of algorithm</a></li></ul></li><li><a class="tocitem" href="../api/">API</a></li><li><a class="tocitem" href="../algorithms/">Algorithms</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Supported Methods</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Supported Methods</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/oskarhs/AutoHist.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/oskarhs/AutoHist.jl/blob/main/docs/src/methods.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Supported-Methods"><a class="docs-heading-anchor" href="#Supported-Methods">Supported Methods</a><a id="Supported-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Supported-Methods" title="Permalink"></a></h1><p>This page provides background on each histogram method supported through the <code>rule</code> argument. Our presentation is intended to be rather brief, and we do as such not cover the theoretical underpinnings of each method in great detail. For some further background on automatic histogram procedures and the theory behind them, we recommend the excellent reviews contained in the articles of <a href="https://doi.org/10.1016/j.csda.2010.04.021">Birgé and Rozenholc (2006)</a> and <a href="https://doi.org/10.1051/ps:2008005">Davies et al. (2009)</a>.</p><p>For ease of exposition, we present all methods covered here in the context of estimating the density of a sample <span>$\boldsymbol{x} = (x_1, x_2, \ldots, x_n)$</span> on the unit interval, but note that extending the procedures presented here to other compact intervals is possible through a suitable affine transformation. In particular, if a density estimate with support <span>$[a,b]$</span> is desired, we can scale the data to the unit interval through <span>$z_i = (x_i - a)/(b-a)$</span>, and apply the methods on this transformed sample and rescale the resulting density estimate to <span>$[a,b]$</span>. In cases where the support of the density is unknown, a natural choice is <span>$a = x_{(1)}$</span> and <span>$b = x_{(n)}$</span>. Cases where only the lower or upper bound is known can be handled similarly. The transformation used to construct the histogram can be controlled through the <code>support</code> keyword, where the default argument <code>support=(-Inf, Inf)</code> uses the order statistics-based approach described above.</p><h4 id="Notation"><a class="docs-heading-anchor" href="#Notation">Notation</a><a id="Notation-1"></a><a class="docs-heading-anchor-permalink" href="#Notation" title="Permalink"></a></h4><p>Before we describe the methods included here in more detail, we introduce some notation. We let <span>$\mathcal{I} = (\mathcal{I}_1, \mathcal{I}_2, \ldots, \mathcal{I}_k)$</span> denote a partition of <span>$[0,1]$</span> into <span>$k$</span> intervals and write <span>$|\mathcal{I}_j|$</span> for the length of interval <span>$\mathcal{I}_j$</span>. The intervals in the partition <span>$\mathcal{I}$</span> can be either right- or left-closed. Whether a left- or right-closed partition is used to draw the histogram is controlled by the keyword argument <code>closed</code>, with options <code>:left</code> and <code>:right</code> (default). This choice is somewhat arbitrary, but is unlikely to matter much in practical applications.</p><p>Based on a partition <span>$\mathcal{I}$</span>, we can write down the corresponding histogram density estimate by</p><p class="math-container">\[\widehat{f}(x) = \sum_{j=1}^k \frac{\widehat{\theta}_j}{|\mathcal{I}_j|}\mathbf{1}_{\mathcal{I}_j}(x), \quad x\in [0,1],\]</p><p>where <span>$\mathbf{1}_{\mathcal{I}_j}$</span> is the indicator function, <span>$\widehat{\theta}_j \geq 0$</span> for all <span>$j$</span> and <span>$\sum_{j=1}^k \widehat{\theta}_j = 1$</span>. </p><p>For most of the methods considered here, the estimated bin probabilities are the maximum likelihood estimates <span>$\widehat{\theta}_j = N_j/n$</span>, where <span>$N_j = \sum_{i=1}^n \mathbb{1}_{\mathcal{I}_j}(x_i)$</span> is number of observations landing in interval <span>$\mathcal{I}_j$</span> . The exception to this rule is are the two Bayesian approaches, which uses the Bayes estimator <span>$\widehat{\theta}_j = (a_j + N_j)/(a+n)$</span> for <span>$(a_1, \ldots, a_k) \in (0,\infty)^k$</span> and <span>$a = \sum_{j=1}^k a_j$</span> instead.</p><p>The goal of an automatic histogram procedure is to find a partition <span>$\mathcal{I}$</span> based on the sample alone which produces a reasonable density estimate. Regular histogram procedures only consider regular partitions, where all intervals in the partition are of equal length, so that one only needs to determine the number <span>$k$</span> of bins. Irregular histograms allow for partitions with intervals of unequal length, and try to determine both the number of bins and the locations of the cutpoints between the intervals. In all the irregular procedures covered here, we attempt to find best partition according to a criterion among all partitions with endpoints belonging to a given discrete mesh.</p><h2 id="Irregular-histograms"><a class="docs-heading-anchor" href="#Irregular-histograms">Irregular histograms</a><a id="Irregular-histograms-1"></a><a class="docs-heading-anchor-permalink" href="#Irregular-histograms" title="Permalink"></a></h2><p>The following section describes how each value of the <code>rule</code> keyword supported by the <code>histogram_irregular</code> function selects the optimal histogram partition. In each case, the best partition is selected among the subset of interval partitions of the unit interval that have cut points belonging to a discrete set of cardinality <span>$k_n-1$</span>. The construction of the candidate cut point set can be controlled through the <code>grid</code> keyword argument, with options <code>:regular</code> (default), <code>:data</code> and <code>:quantile</code>.</p><h4 id="bayes"><a class="docs-heading-anchor" href="#bayes">bayes</a><a id="bayes-1"></a><a class="docs-heading-anchor-permalink" href="#bayes" title="Permalink"></a></h4><p>Consists of maximizing the log-marginal likelihood conditional on the partition <span>$\mathcal{I} = (\mathcal{I}_1, \ldots, \mathcal{I}_k)$</span>,</p><p class="math-container">\[    \sum_{j=1}^k \big\{\log \Gamma(a_j + N_j) - \log \Gamma(a_j) - N_j\log|\mathcal{I}_j|\big\} + \log p_n(k) - \log \binom{k_n-1}{k-1}\]</p><p>Here <span>$p_n(k)$</span> is the prior distribution on the number <span>$k$</span> of bins, which can be controlled by supplying a function to the <code>logprior</code> keyword argument. The default value is <span>$p_n(k) \propto 1$</span>. Here, <span>$a_j = a/k$</span>, for a scalar <span>$a &gt; 0$</span> which can be controlled by the user through the keyword argument <code>a</code> (default <code>a=5.0</code>).</p><p>This approach to irregular histograms was pioneered by <a href="https://doi.org/10.48550/ARXIV.2505.22034">Simensen et al. (2025)</a>.</p><h4 id="pena"><a class="docs-heading-anchor" href="#pena">pena</a><a id="pena-1"></a><a class="docs-heading-anchor-permalink" href="#pena" title="Permalink"></a></h4><p>Consists of maximizing a penalized log-likelihood,</p><p class="math-container">\[    \sum_{j=1}^k N_j \log (N_j/|\mathcal{I}_j|) - \log \binom{k_n-1}{k-1} - k - 2\log(k) - \sqrt{2(k-1)\Big[\log \binom{k_n-1}{k-1}+ 2\log(k)\Big]}.\]</p><p>This approach was suggested by <a href="https://doi.org/10.1016/j.csda.2010.04.021">Rozenholc et al. (2010)</a>.</p><h4 id="penb"><a class="docs-heading-anchor" href="#penb">penb</a><a id="penb-1"></a><a class="docs-heading-anchor-permalink" href="#penb" title="Permalink"></a></h4><p>Consists of maximizing a penalized log-likelihood,</p><p class="math-container">\[    \sum_{j=1}^k N_j \log (N_j/|\mathcal{I}_j|) - \log \binom{k_n-1}{k-1} - k - \log^{2.5}(k).\]</p><p>This approach was suggested by <a href="https://doi.org/10.1016/j.csda.2010.04.021">Rozenholc et al. (2010)</a>.</p><h4 id="penr"><a class="docs-heading-anchor" href="#penr">penr</a><a id="penr-1"></a><a class="docs-heading-anchor-permalink" href="#penr" title="Permalink"></a></h4><p>Consists of maximizing a penalized log-likelihood,</p><p class="math-container">\[    \sum_{j=1}^k N_j \log (N_j/|\mathcal{I}_j|) - \frac{1}{2n}\sum_{j=1}^k \frac{N_j}{|\mathcal{I}_j|} - \log \binom{k_n-1}{k-1} - \log^{2.5}(k).\]</p><p>This criterion was also suggested by <a href="https://doi.org/10.1016/j.csda.2010.04.021">Rozenholc et al. (2010)</a>.</p><h4 id="l2cv-(irregular)"><a class="docs-heading-anchor" href="#l2cv-(irregular)">l2cv (irregular)</a><a id="l2cv-(irregular)-1"></a><a class="docs-heading-anchor-permalink" href="#l2cv-(irregular)" title="Permalink"></a></h4><p>Consists of maximization of an L2 leave-one-out cross-validation criterion,</p><p class="math-container">\[    \frac{n+1}{n}\sum_{j=1}^k \frac{N_j^2}{|\mathcal{I}_j|} - 2\sum_{j=1}^k \frac{N_j}{|\mathcal{I}_j|}.\]</p><p>This approach dates back to <a href="https://www.jstor.org/stable/4615859">Rudemo (1982)</a>.</p><h4 id="klcv-(irregular)"><a class="docs-heading-anchor" href="#klcv-(irregular)">klcv (irregular)</a><a id="klcv-(irregular)-1"></a><a class="docs-heading-anchor-permalink" href="#klcv-(irregular)" title="Permalink"></a></h4><p>Consists of maximization of a Kullback-Leibler cross-validation criterion,</p><p class="math-container">\[    \sum_{j=1}^k N_j\log(N_j-1) - \sum_{j=1}^k N_j\log |I_j|,\]</p><p>where the maximmization is over all partitions with <span>$N_j \geq 2$</span> for all <span>$j$</span>. This approach was, to our knowledge, first pursued by <a href="https://doi.org/10.48550/ARXIV.2505.22034">Simensen et al. (2025)</a>.</p><h4 id="nml-(irregular)"><a class="docs-heading-anchor" href="#nml-(irregular)">nml (irregular)</a><a id="nml-(irregular)-1"></a><a class="docs-heading-anchor-permalink" href="#nml-(irregular)" title="Permalink"></a></h4><p>Consists of maximization of a penalized likelihood,</p><p class="math-container">\[\begin{aligned}
    &amp;\sum_{j=1}^k N_j\log \frac{N_j}{|\mathcal{I}_j|} - \frac{k-1}{2}\log(n/2) - \log\frac{\sqrt{\pi}}{\Gamma(k/2)} - n^{-1/2}\frac{\sqrt{2}k\Gamma(k/2)}{3\Gamma(k/2-1/2)} \\
    &amp;- n^{-1}\left(\frac{3+k(k-2)(2k+1)}{36} - \frac{\Gamma(k/2)^2 k^2}{9\Gamma(k/2-1/2)^2} \right)  - \log \binom{k_n-1}{k-1}
\end{aligned}\]</p><p>This a variant of this criterion first suggested by <a href="https://proceedings.mlr.press/v2/kontkanen07a.html">Kontkanen and Myllymäki (2007)</a>. The above criterion uses an asymptotic expansion of their proposed penalty term, as their proposed penalty can be quite expensive to evaluate.</p><h2 id="Regular-histograms"><a class="docs-heading-anchor" href="#Regular-histograms">Regular histograms</a><a id="Regular-histograms-1"></a><a class="docs-heading-anchor-permalink" href="#Regular-histograms" title="Permalink"></a></h2><p>The following section details how each value of the <code>rule</code> keyword supported by the <code>histogram_regular</code> function selects the number <span>$k$</span> of bins to draw a histogram automatically based on a random sample. In the following, <span>$\mathcal{I} = (\mathcal{I}_1, \mathcal{I}_2, \ldots, \mathcal{I}_k)$</span> is the corresponding partition of <span>$[0,1]$</span> consisting of <span>$k$</span> equal-length bins. In cases where the value of the number of bins is computed by maximizing an expression, we look for the best regular partition among all regular partitions consisting of no more than <span>$k_n$</span> bins.</p><h4 id="bayes,-knuth"><a class="docs-heading-anchor" href="#bayes,-knuth">bayes, knuth</a><a id="bayes,-knuth-1"></a><a class="docs-heading-anchor-permalink" href="#bayes,-knuth" title="Permalink"></a></h4><p>Consists of maximizing the log-marginal likelihood for given <span>$k$</span>,</p><p class="math-container">\[   n\log (k) + \sum_{j=1}^k \big\{\log \Gamma(a_j + N_j) - \log \Gamma(a_j)\big\} + \log p_n(k).\]</p><p>Here <span>$p_n(k)$</span> is the prior distribution on the number <span>$k$</span> of bins, which can be controlled by supplying a function to the <code>logprior</code> keyword argument. The default value is <span>$p_n(k) \propto 1$</span>. Here, <span>$a_j = a/k$</span>, for a scalar <span>$a &gt; 0$</span>, possibly depending on <span>$k$</span>. The value of <span>$a$</span> can be set by supplying a fixed, positive scalar or a function <span>$a(k)$</span> to the keyword argument <code>a</code>. For <code>rule=:bayes</code>, the default value is <code>a=5.0</code>.</p><p>The default regular procedure is <code>rule=:knuth</code>, which corresponds to the particular choices <span>$a_j = 0.5$</span> and <span>$p_n(k)\propto 1$</span> which is the suggestion of <a href="https://doi.org/10.1016/j.dsp.2019.102581">Knuth (2019)</a>. Note that if <code>a</code> and <code>logprior</code> will both be overridden in this case.</p><h4 id="aic"><a class="docs-heading-anchor" href="#aic">aic</a><a id="aic-1"></a><a class="docs-heading-anchor-permalink" href="#aic" title="Permalink"></a></h4><p>Consists of maximizing a penalized log-likelihood,</p><p class="math-container">\[    n\log (k) + \sum_{j=1}^k N_j \log (N_j/n) - k.\]</p><p>The aic criterion was proposed by <a href="https://doi.org/10.1093/biomet/74.3.636">Taylor (1987)</a> for histograms.</p><h4 id="bic"><a class="docs-heading-anchor" href="#bic">bic</a><a id="bic-1"></a><a class="docs-heading-anchor-permalink" href="#bic" title="Permalink"></a></h4><p>Consists of maximizing a penalized log-likelihood,</p><p class="math-container">\[    n\log (k) + \sum_{j=1}^k N_j \log (N_j/n) - \frac{k}{2}\log(n).\]</p><h4 id="br"><a class="docs-heading-anchor" href="#br">br</a><a id="br-1"></a><a class="docs-heading-anchor-permalink" href="#br" title="Permalink"></a></h4><p>Consists of maximizing a penalized log-likelihood,</p><p class="math-container">\[    n\log (k) + \sum_{j=1}^k N_j \log (N_j/n) - k - \log^{2.5}(k).\]</p><p>This criterion was proposed by <a href="https://doi.org/10.1051/ps:2006001">Birgé and Rozenholc (2006)</a>.</p><h4 id="l2cv-(regular)"><a class="docs-heading-anchor" href="#l2cv-(regular)">l2cv (regular)</a><a id="l2cv-(regular)-1"></a><a class="docs-heading-anchor-permalink" href="#l2cv-(regular)" title="Permalink"></a></h4><p>Consists of maximizing a L2 leave-one-out cross-validation criterion,</p><p class="math-container">\[    -2k + k\frac{n+1}{n^2}\sum_{j=1}^k N_j^2.\]</p><p>This approach to histogram density estimation was first considered by <a href="https://www.jstor.org/stable/4615859 ">Rudemo (1982)</a>.</p><h4 id="klcv-(regular)"><a class="docs-heading-anchor" href="#klcv-(regular)">klcv (regular)</a><a id="klcv-(regular)-1"></a><a class="docs-heading-anchor-permalink" href="#klcv-(regular)" title="Permalink"></a></h4><p>Consists of maximizing a Kullback-Leibler leave-one-out cross-validation criterion,</p><p class="math-container">\[    n\log(k) + \sum_{j=1}^k N_j\log (N_j-1),\]</p><p>where the maximmization is over all regular partitions with <span>$N_j \geq 2$</span> for all <span>$j$</span>. This approach was first studied by <a href="https://doi.org/10.1007/BF01203164">Hall (1990)</a>.</p><h4 id="mdl"><a class="docs-heading-anchor" href="#mdl">mdl</a><a id="mdl-1"></a><a class="docs-heading-anchor-permalink" href="#mdl" title="Permalink"></a></h4><p>Consists of finding the model providing the shortest encoding of the data, which is equivalent to maximization of</p><p class="math-container">\[    n\log(k) + \sum_{j=1}^k \big(N_j-\frac{1}{2}\big)\log\big(N_j-\frac{1}{2}\big) - \big(n-\frac{k}{2}\big)\log\big(n-\frac{k}{2}\big) - \frac{k}{2}\log(n),\]</p><p>where the maximmization is over all regular partitions with <span>$N_j \geq 1$</span> for all <span>$j$</span>. The minimum description length principle was first applied to histogram estimation by <a href="https://doi.org/10.1093/biomet/75.4.705">Hall and Hannan (1988)</a>.</p><h4 id="nml-(regular)"><a class="docs-heading-anchor" href="#nml-(regular)">nml (regular)</a><a id="nml-(regular)-1"></a><a class="docs-heading-anchor-permalink" href="#nml-(regular)" title="Permalink"></a></h4><p>Consists of maximization of a penalized likelihood,</p><p class="math-container">\[\begin{aligned}
    &amp;\sum_{j=1}^k N_j\log \frac{N_j}{|\mathcal{I}_j|} - \frac{k-1}{2}\log(n/2) - \log\frac{\sqrt{\pi}}{\Gamma(k/2)} - n^{-1/2}\frac{\sqrt{2}k\Gamma(k/2)}{3\Gamma(k/2-1/2)} \\
    &amp;- n^{-1}\left(\frac{3+k(k-2)(2k+1)}{36} - \frac{\Gamma(k/2)^2 k^2}{9\Gamma(k/2-1/2)^2} \right).
\end{aligned}\]</p><p>This is a regular variant of the normalized maximum likelihood criterion considered by <a href="https://proceedings.mlr.press/v2/kontkanen07a.html">Kontkanen and Myllymäki (2007)</a>.</p><h4 id="Sturges&#39;-rule"><a class="docs-heading-anchor" href="#Sturges&#39;-rule">Sturges&#39; rule</a><a id="Sturges&#39;-rule-1"></a><a class="docs-heading-anchor-permalink" href="#Sturges&#39;-rule" title="Permalink"></a></h4><p>The number <span>$k$</span> of bins is computed according to the formula</p><p class="math-container">\[    k = \lceil \log_2(n) \rceil + 1.\]</p><p>This classical rule, due to <a href="https://doi.org/10.1080/01621459.1926.10502161">Sturges (1926)</a>, is the default for determining the number of bins in R.</p><h4 id="Freedman-and-Diaconis&#39;-rule"><a class="docs-heading-anchor" href="#Freedman-and-Diaconis&#39;-rule">Freedman and Diaconis&#39; rule</a><a id="Freedman-and-Diaconis&#39;-rule-1"></a><a class="docs-heading-anchor-permalink" href="#Freedman-and-Diaconis&#39;-rule" title="Permalink"></a></h4><p>The number <span>$k$</span> of bins is computed according to the formula</p><p class="math-container">\[    k = \big\lceil\frac{n^{1/3}}{2\mathrm{IQR}(\boldsymbol{x})}\big\rceil,\]</p><p>where <span>$\mathrm{IQR}(\boldsymbol{x})$</span> is the sample interquartile range. This rule dates back to <a href="https://doi.org/10.1007/BF01025868">Freedman and Diaconis (1982)</a> and is the default bin selection rule used by the <code>histogram()</code> function from Plots.jl.</p><h4 id="Scott&#39;s-rule"><a class="docs-heading-anchor" href="#Scott&#39;s-rule">Scott&#39;s rule</a><a id="Scott&#39;s-rule-1"></a><a class="docs-heading-anchor-permalink" href="#Scott&#39;s-rule" title="Permalink"></a></h4><p>The number <span>$k$</span> of bins is computed according to the formula</p><p class="math-container">\[    k = \big\lceil \hat{\sigma}^{-1}(24\sqrt{\pi})^{-1/3}n^{1/3}\big\rceil,\]</p><p>where <span>$\hat{\sigma}$</span> is the sample standard deviation. Scott&#39;s normal reference rule was first proposed by <a href="https://doi.org/10.1093/biomet/66.3.605">Scott (1979)</a>.</p><h4 id="Wand&#39;s-rule"><a class="docs-heading-anchor" href="#Wand&#39;s-rule">Wand&#39;s rule</a><a id="Wand&#39;s-rule-1"></a><a class="docs-heading-anchor-permalink" href="#Wand&#39;s-rule" title="Permalink"></a></h4><p>A more sophisticated version of Scott&#39;s rule, Wand&#39;s rule proceeds by determining the bin width <span>$h$</span> as</p><p class="math-container">\[    h = \Big(\frac{6}{\hat{C}(f_0) n}\Big)^{1/3},\]</p><p>where <span>$\hat{C}(f_0)$</span> is an estimate of the functional <span>$C(f_0) = \int \{f_0&#39;(x)\}^2\, \text{d}x$</span>. The corresponding number of bins is <span>$k = \lceil h^{-1}\rceil$</span>. The full details on this method are given in <a href="https://doi.org/10.2307/2684697">Wand (1997)</a>. The density estimate is computed based on a scale estimate, which can be controlled through the <code>scale</code> keyword argument. Possible choices are <code>:stdev</code>, <code>:iqr</code> which uses an estimate based on the sample standard deviation or the sample interquartile range as a scale estimate. The default choice <code>:minim</code> uses the minimum of the above estimates. The <code>level</code> keyword controls the number of stages of functional estimation used to compute <span>$\hat{C}$</span>, and can take values <code>0, 1, 2, 3, 4, 5</code>, with the default value being <code>level=2</code>. The choice <code>level=0</code> corresponds to Scott&#39;s rule under the chosen scale estimate.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Introdution</a><a class="docs-footer-nextpage" href="../examples/density_estimation/">Density estimation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.11.4 on <span class="colophon-date" title="Wednesday 16 July 2025 20:29">Wednesday 16 July 2025</span>. Using Julia version 1.10.10.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
