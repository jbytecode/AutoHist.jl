var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#The-AutomaticHistogram-type","page":"API","title":"The AutomaticHistogram type","text":"","category":"section"},{"location":"api/#AutoHist.AutomaticHistogram","page":"API","title":"AutoHist.AutomaticHistogram","text":"AutomaticHistogram\n\nA type for representing a histogram where the histogram partition has been chosen automatically based on the sample. Can be fitted to data using the fit, histogram_irregular or histogram_regular methods.\n\n...\n\nFields\n\nbreaks: AbstractVector consisting of the cut points in the chosen partition.\ndensity: Estimated density in each bin.\ncounts: The bin counts for the partition corresponding to breaks.\ntype: Symbol indicating whether the histogram was fit using an irregular procedure (type==:irregular) or a regular one (type==:regular).\nclosed: Symbol indicating whether the drawn intervals should be right-inclusive or not. Possible values are :right (default) and :left.\na: Value of the Dirichlet concentration parameter corresponding to the chosen partition. Only of relevance if a Bayesian method was used to fit the histogram, and is otherwise set to NaN.\n\nExamples\n\njulia> x = [0.037, 0.208, 0.189, 0.656, 0.45, 0.846, 0.986, 0.751, 0.249, 0.447]\njulia> h1 = fit(AutomaticHistogram, x)      # fits an irregular histogram\njulia> h2 = fit(AutomaticHistogram, x; rule=:wand, scalest=:stdev, level=4)\njulia> h3 = histogram_irregular(x; grid=:quantile, support=(0.0, 1.0), logprior=k->-log(k), a=sqrt(10))\n\n\n\n\n\n","category":"type"},{"location":"api/#Fitting-an-automatic-histogram-to-data","page":"API","title":"Fitting an automatic histogram to data","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"An automatic histogram based on regular or irregular partitions can be fitted to the data by calling the fit method.","category":"page"},{"location":"api/#StatsAPI.fit-Tuple{Type{AutomaticHistogram}, AbstractVector{<:Real}}","page":"API","title":"StatsAPI.fit","text":"fit(AutomaticHistogram, x::AbstractVector{x<:Real}; rule=:bayes, type=:irregular, kwargs...)\n\nFit a histogram to a one-dimensional vector x with an automatic and data-based selection of the histogram partition.\n\n...\n\nArguments\n\nx: 1D vector of data for which a histogram is to be constructed.\n\nKeyword arguments\n\nrule: The criterion used to determine the optimal number of bins. Defaults to the method Bayesian method of Simensen et al. (2025).\ntype: Symbol indicating whether the fitted method is a regular and irregular one. The rules :bayes, :l2cv, :klcv and :nml are implemented for both regular and irregular histograms, and this keyword specifies whether the regular or irregular version should be used. For other rules, this function infers the type automatically from the rule keyword, and misspecifying the rule in this case has not effect. Possible values are :irregular (default) and regular.\nkwargs: Additional keyword arguments passed to histogram_regular or histogram_irregular depending on the specified or inferred type.\n\nReturns\n\nh: An object of type AutomaticHistogram, corresponding to the fitted histogram.\n\njulia> x = randn(10^3)\njulia> h1 = fit(AutomaticHistogram, x)                                      # fits an irregular histogram\njulia> h2 = fit(AutomaticHistogram, x; rule=:wand, scalest=:stdev, level=4) # fits a regular histogram\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API","title":"API","text":"Alternatively, an automatic histogram can be fitted to the data by the following methods:","category":"page"},{"location":"api/#AutoHist.histogram_irregular","page":"API","title":"AutoHist.histogram_irregular","text":"histogram_irregular(x::AbstractVector{<:Real}; rule::Symbol=:bayes, grid::Symbol=:regular, closed::Symbol=:right, greedy::Bool=true, maxbins::Int=-1, support::Tuple{Real,Real}=(-Inf,Inf), use_min_length::Bool=false, logprior::Function=k->0.0, a::Real=1.0)\n\nCreate an irregular histogram based on optimization of a criterion based on Bayesian probability, penalized likelihood or LOOCV. Returns an AutomaticHistogram object with the optimal partition corresponding to the supplied rule.\n\nArguments\n\nx: 1D vector of data for which a histogram is to be constructed.\n\nKeyword arguments\n\nrule: The criterion used to determine the optimal number of bins. Defaults to the Bayesian method of Simensen et al. (2025).\ngrid: Symbol indicating how the finest possible mesh should be constructed. Options are :data, which uses each unique data point as a grid point, :regular (default) which constructs a fine regular grid, and :quantile which constructs the grid based on the sample quantiles.\nclosed: Symbol indicating whether the drawn intervals should be right-inclusive or not. Possible values are :right (default) and :left.\ngreedy: Boolean indicating whether or not the greedy binning strategy of Rozenholc et al. (2006) should be used prior to running the dynamical programming algorithm. Defaults to true. The algorithm can be quite slow for large datasets when this keyword is set to false.\nmaxbins: The maximal number of bins to be considered by the optimization criterion, only used if grid is set to \"regular\" or \"quantile\". Defaults to maxbins=min(4*n/log(n)^2, 1000). If the specified argument is not a positive integer, the default value is used.\nsupport: Tuple specifying the the support of the histogram estimate. If the first element is -Inf, then minimum(x) is taken as the leftmost cutpoint. Likewise, if the second elemen is Inf, then the rightmost cutpoint is maximum(x). Default value is (-Inf, Inf), which estimates the support of the data.\nuse_min_length: Boolean indicating whether or not to impose a restriction on the minimum bin length of the histogram. If set to true, the smallest allowed bin length is set to (maximum(x)-minimum(x))/n*log(n)^(1.5).\nlogprior: Unnormalized logprior distribution for the number k of bins. Defaults to a uniform prior. Only used in when rule keyword is set to :bayes.\na: Dirichlet concentration parameter in the Bayesian irregular histogram model. Set to the default value (5.0) if the supplied value is not a positive real number. Only used when rule is set to :bayes.\n\nReturns\n\nh: AutomaticHistogram object with weights corresponding to densities, e.g. :isdensity is set to true.\n\nExamples\n\njulia> x = [0.037, 0.208, 0.189, 0.656, 0.45, 0.846, 0.986, 0.751, 0.249, 0.447]\njulia> h1 = histogram_irregular(x)\njulia> h2 = histogram_irregular(x; grid=:quantile, support=(0.0, 1.0), logprior=k->-log(k), a=sqrt(10))\n\n...\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoHist.histogram_regular","page":"API","title":"AutoHist.histogram_regular","text":"histogram_regular(x::AbstractVector{<:Real}; rule::Symbol=:bayes, closed::Symbol=:right, maxbins::Int=1000, support::Tuple{Real,Real}=(-Inf,Inf), logprior::Function=k->0.0, a::Union{Real,Function}=1.0, level::Int=2, scalest::Symbol=:minim)\n\nCreate a regular histogram based on an asymptotic risk estimate, or optimization criteria from Bayesian probability, penalized likelihood or LOOCV. Returns a AutomaticHistogram object with regular bins, with the optimal bin number corresponding to the supplied criterion.\n\n...\n\nArguments\n\nx: 1D vector of data for which a histogram is to be constructed.\n\nKeyword arguments\n\nrule: The criterion used to determine the optimal number of bins. Defaults to the method Bayesian method of Simensen et al. (2025)\nclosed: Symbol indicating whether the drawn intervals should be right-inclusive or not. Possible values are :right (default) and :left.\nmaxbins: The maximal number of bins to be considered by the optimization criterion. Ignored if the specified argument is not a positive integer. Defaults to maxbins=1000\nsupport: Tuple specifying the the support of the histogram estimate. If the first element is -Inf, then minimum(x) is taken as the leftmost cutpoint. Likewise, if the second element is Inf, then the rightmost cutpoint is maximum(x). Default value is (-Inf, Inf), which estimates the support of the data.\nlogprior: Unnormalized logprior distribution of the number k of bins. Only used in the case where the supplied rule is :bayes. Defaults to a uniform prior.\na: Specifies Dirichlet concentration parameter in the Bayesian histogram model. Can either be a fixed positive number or a function computing aâ‚– for different values of k. Defaults to 1.0 if not supplied. Uses default if suppled value is negative.\nscalest: Estimate of scale parameter used in computing Wands' rule. Only used if rule is set to :wand. Possible values are :minim :stdved and :iqr. Default value is scalest=:minim.\nlevel: Specifies the level used for the Kernel functional estimate in Wands' rule. Only used if rule==:wand. Possible values are 0,1,2,3,4 and 5. Default value is level=2.\n\nReturns\n\nh: AutomaticHistogram object with weights corresponding to densities, e.g. isdensity is set to true.\n\nExamples\n\njulia> x = [0.037, 0.208, 0.189, 0.656, 0.45, 0.846, 0.986, 0.751, 0.249, 0.447]\njulia> h1 = histogram_regular(x)\njulia> h2 = histogram_regular(x; logprior=k->-log(k), a=k->0.5*k)\n\n...\n\n\n\n\n\n","category":"function"},{"location":"api/#Additional-methods-for-AutomaticHist","page":"API","title":"Additional methods for AutomaticHist","text":"","category":"section"},{"location":"api/#StatsBase.modes-Tuple{AutomaticHistogram}","page":"API","title":"StatsBase.modes","text":"modes(h::AutomaticHistogram)\n\nReturn the location of the modes of h as a Vector, sorted in increasing order.\n\nFormally, the modes of the histogram h are defined as the midpoints of an interval I, where the density of h is constant on I, and the density of h is strictly smaller than this value in the histogram bins adjacent to I. Note that according this definition, I is in general a nonempty union of intervals in the histogram partition.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.minimum-Tuple{AutomaticHistogram}","page":"API","title":"Base.minimum","text":"minimum(h::AutomaticHistogram)\n\nReturn the minimum of the support of h.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.maximum-Tuple{AutomaticHistogram}","page":"API","title":"Base.maximum","text":"maximum(h::AutomaticHistogram)\n\nReturn the maximum of the support of h.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.extrema-Tuple{AutomaticHistogram}","page":"API","title":"Base.extrema","text":"extrema(h::AutomaticHistogram)\n\nReturn the minimum and the maximum of the support of h as a 2-tuple.\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsAPI.loglikelihood-Tuple{AutomaticHistogram}","page":"API","title":"StatsAPI.loglikelihood","text":"loglikelihood(h::AutomaticHistogram)\n\nCompute the log-likelihood (up to proportionality) of an AutomaticHistogram.\n\nThe value of the log-likelihood is     âˆ‘â±¼ Nâ±¼ log (dâ±¼), where Nâ±¼, dâ±¼ are the bin counts and estimated densities for bin j.\n\n\n\n\n\n","category":"method"},{"location":"api/#AutoHist.logmarginallikelihood","page":"API","title":"AutoHist.logmarginallikelihood","text":"logmarginallikelihood(h::AutomaticHistogram, a::Real)\nlogmarginallikelihood(h::AutomaticHistogram)\n\nCompute the log-marginal likelihood (up to proportionality) of an AutomaticHistogram when the value of the Dirichlet concentration parameter equals a. This can be automatically inferred if the histogram was fitted with rule=:bayes, and does not have to be explicitly passed as an argument in this case.\n\nAssumes that the Dirichlet prior is centered on the uniform distribution, so that aâ±¼ = a/k for a scalar a>0 and all j. The value of the log-marginal likelihood is     âˆ‘â±¼ {log Î“(Nâ±¼+aâ±¼) - log Î“(aâ±¼) - Nâ±¼log(dâ±¼)} - log Î“(a+n) + log Î“(a), where where Nâ±¼ is the bin count for bin j.\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.convert-Tuple{Type{Histogram}, AutomaticHistogram}","page":"API","title":"Base.convert","text":"convert(Histogram, h::AutomaticHistogram)\n\nConvert an AutomaticHistogram to a StatsBase.Histogram, normalized to be a density.\n\n\n\n\n\n","category":"method"},{"location":"methods/#Supported-Methods","page":"Supported Methods","title":"Supported Methods","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This page provides background on each histogram method supported through the rule argument. Our presentation is intended to be rather brief, and we do as such not cover the theoretical underpinnings of each method in great detail. For some further background on automatic histogram procedures and the theory behind them, we recommend the excellent reviews contained in the articles of BirgÃ© and Rozenholc (2006) and Davies et al. (2009).","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"For ease of exposition, we present all methods covered here in the context of estimating the density of a sample boldsymbolx = (x_1 x_2 ldots x_n) on the unit interval, but note that extending the procedures presented here to other compact intervals is possible through a suitable affine transformation. In particular, if a density estimate with support ab is desired, we can scale the data to the unit interval through z_i = (x_i - a)(b-a), and apply the methods on this transformed sample and rescale the resulting density estimate to ab. In cases where the support of the density is unknown, a natural choice is a = x_(1) and b = x_(n). Cases where only the lower or upper bound is known can be handled similarly. The transformation used to construct the histogram can be controlled through the support keyword, where the default argument support=(-Inf, Inf) uses the order statistics-based approach described above.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Before we describe the methods included here in more detail, we introduce some notation. We let mathcalI = (mathcalI_1 mathcalI_2 ldots mathcalI_k) denote a partition of 01 into k intervals and write mathcalI_j for the length of interval mathcalI_j. The intervals in the partition mathcalI can be either right- or left-closed. Whether a left- or right-closed partition is used to draw the histogram is controlled by the keyword argument closed, with options :left and :right (default). This choice is somewhat arbitrary, but is unlikely to matter much in practical applications.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Based on a partition mathcalI, we can write down the corresponding histogram density estimate by","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"widehatf(x) = sum_j=1^k fracwidehattheta_jmathcalI_jmathbf1_mathcalI_j(x) quad xin 01","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where mathbf1_mathcalI_j is the indicator function, widehattheta_j geq 0 for all j and sum_j=1^k widehattheta_j = 1. ","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"For most of the methods considered here, the estimated bin probabilities are the maximum likelihood estimates widehattheta_j = N_jn, where N_j = sum_i=1^n mathbb1_mathcalI_j(x_i) is number of observations landing in interval mathcalI_j . The exception to this rule is are the two Bayesian approaches, which uses the Bayes estimator widehattheta_j = (a_j + N_j)(a+n) for (a_1 ldots a_k) in (0infty)^k and a = sum_j=1^k a_j instead.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The goal of an automatic histogram procedure is to find a partition mathcalI based on the sample alone which produces a reasonable density estimate. Regular histogram procedures only consider regular partitions, where all intervals in the partition are of equal length, so that one only needs to determine the number k of bins. Irregular histograms allow for partitions with intervals of unequal length, and try to determine both the number of bins and the locations of the cutpoints between the intervals. In all the irregular procedures covered here, we attempt to find best partition according to a criterion among all partitions with endpoints belonging to a given discrete mesh.","category":"page"},{"location":"methods/#Irregular-histograms","page":"Supported Methods","title":"Irregular histograms","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The following section describes how each value of the rule keyword supported by the histogram_irregular function selects the optimal histogram partition. In each case, the best partition is selected among the subset of interval partitions of the unit interval that have cut points belonging to a discrete set of cardinality k_n-1. The construction of the candidate cut point set can be controlled through the grid keyword argument, with options :regular (default), data and quantile.","category":"page"},{"location":"methods/#bayes:","page":"Supported Methods","title":"bayes:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing the log-marginal likelihood conditional on the partition mathcalI = (mathcalI_1 ldots mathcalI_k),","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    sum_j=1^k biglog Gamma(a_j + N_j) - log Gamma(a_j) - N_jlogmathcalI_jbig + log p_n(k) - log binomk_n-1k-1","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Here p_n(k) is the prior distribution on the number k of bins, which can be controlled by supplying a function to the logprior keyword argument. The default value is p_n(k) propto 1. Here, a_j = ak, for a scalar a  0 which can be controlled by the user through the keyword argument a (default a=5.0).","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This approach to irregular histograms was pioneered by Simensen et al. (2025).","category":"page"},{"location":"methods/#penb:","page":"Supported Methods","title":"penb:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a penalized log-likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    sum_j=1^k N_j log (N_jmathcalI_j) - log binomk_n-1k-1 - k - log^25(k)","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This approach was suggested by Rozenholc et al. (2010).","category":"page"},{"location":"methods/#penr:","page":"Supported Methods","title":"penr:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a penalized log-likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    sum_j=1^k N_j log (N_jmathcalI_j) - frac12nsum_j=1^k fracN_jmathcalI_j - log binomk_n-1k-1 - log^25(k)","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This criterion was also suggested by Rozenholc et al. (2010).","category":"page"},{"location":"methods/#l2cv:","page":"Supported Methods","title":"l2cv:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximization of an L2 leave-one-out cross-validation criterion,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    fracn+1nsum_j=1^k fracN_j^2mathcalI_j - 2sum_j=1^k fracN_jmathcalI_j","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This approach dates back to Rudemo (1982).","category":"page"},{"location":"methods/#klcv:","page":"Supported Methods","title":"klcv:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximization of a Kullback-Leibler cross-validation criterion,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    sum_j=1^k N_jlog(N_j-1) - sum_j=1^k N_jlog I_j","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where the maximmization is over all partitions with N_j geq 2 for all j. This approach was, to our knowledge, first pursued by Simensen et al. (2025).","category":"page"},{"location":"methods/#nml:","page":"Supported Methods","title":"nml:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximization of a penalized likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"beginaligned\n    sum_j=1^k N_jlog fracN_jmathcalI_j - frack-12log(n2) - logfracsqrtpiGamma(k2) - n^-12fracsqrt2kGamma(k2)3Gamma(k2-12) \n    - n^-1left(frac3+k(k-2)(2k+1)36 - fracGamma(k2)^2 k^29Gamma(k2-12)^2 right)  - log binomk_n-1k-1\nendaligned","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This a variant of this criterion first suggested by Kontkanen and MyllymÃ¤ki (2007). The above criterion uses an asymptotic expansion of their proposed penalty term, as their proposed penalty can be quite expensive to evaluate.","category":"page"},{"location":"methods/#Regular-histograms","page":"Supported Methods","title":"Regular histograms","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The following section details how each value of the rule keyword supported by the histogram_regular function selects the number k of bins to draw a histogram automatically based on a random sample. In the following, mathcalI = (mathcalI_1 mathcalI_2 ldots mathcalI_k) is the corresponding partition of 01 consisting of k equal-length bins. In cases where the value of the number of bins is computed by maximizing an expression, we look for the best regular partition among all regular partitions consisting of no more than k_n bins.","category":"page"},{"location":"methods/#bayes:-2","page":"Supported Methods","title":"bayes:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing the log-marginal likelihood for given k,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"   nlog (k) + sum_j=1^k biglog Gamma(a_j + N_j) - log Gamma(a_j)big + log p_n(k)","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Here p_n(k) is the prior distribution on the number k of bins, which can be controlled by supplying a function to the logprior keyword argument. The default value is p_n(k) propto 1. Here, a_j = ak, for a scalar a  0, possibly depending on k. The value of a can be set by supplying a fixed, positive scalar or a function a(k) to the keyword argument a.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The particular choices a_j = 05 and p_n(k)propto 1 were suggested by Knuth (2019).","category":"page"},{"location":"methods/#aic:","page":"Supported Methods","title":"aic:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a penalized log-likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    nlog (k) + sum_j=1^k N_j log (N_jn) - k","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The aic criterion was proposed by Taylor (1987) for histograms.","category":"page"},{"location":"methods/#bic:","page":"Supported Methods","title":"bic:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a penalized log-likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    nlog (k) + sum_j=1^k N_j log (N_jn) - frack2log(n)","category":"page"},{"location":"methods/#br:","page":"Supported Methods","title":"br:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a penalized log-likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    nlog (k) + sum_j=1^k N_j log (N_jn) - k - log^25(k)","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This criterion was proposed by BirgÃ© and Rozenholc (2006).","category":"page"},{"location":"methods/#l2cv:-2","page":"Supported Methods","title":"l2cv:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a L2 leave-one-out cross-validation criterion,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    -2k + kfracn+1n^2sum_j=1^k N_j^2","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This approach to histogram density estimation was first considered by Rudemo (1982).","category":"page"},{"location":"methods/#klcv:-2","page":"Supported Methods","title":"klcv:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a Kullback-Leibler leave-one-out cross-validation criterion,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    nlog(k) + sum_j=1^k N_jlog (N_j-1)","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where the maximmization is over all regular partitions with N_j geq 2 for all j. This approach was first studied by Hall (1990).","category":"page"},{"location":"methods/#mdl:","page":"Supported Methods","title":"mdl:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of finding the model providing the shortest encoding of the data, which is equivalent to maximization of","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    nlog(k) + sum_j=1^k big(N_j-frac12big)logbig(N_j-frac12big) - big(n-frack2big)logbig(n-frack2big) - frack2log(n)","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where the maximmization is over all regular partitions with N_j geq 1 for all j. The minimum description length principle was first applied to histogram estimation by Hall and Hannan (1988).","category":"page"},{"location":"methods/#nml:-2","page":"Supported Methods","title":"nml:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximization of a penalized likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"beginaligned\n    sum_j=1^k N_jlog fracN_jmathcalI_j - frack-12log(n2) - logfracsqrtpiGamma(k2) - n^-12fracsqrt2kGamma(k2)3Gamma(k2-12) \n    - n^-1left(frac3+k(k-2)(2k+1)36 - fracGamma(k2)^2 k^29Gamma(k2-12)^2 right)\nendaligned","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This is a regular variant of the normalized maximum likelihood criterion considered by Kontkanen and MyllymÃ¤ki (2007).","category":"page"},{"location":"methods/#Sturges'-rule:","page":"Supported Methods","title":"Sturges' rule:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The number k of bins is computed according to the formula","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    k = lceil log_2(n) rceil + 1","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This classical rule, due to Sturges (1926), is the default for determining the number of bins in R.","category":"page"},{"location":"methods/#Freedman-and-Diaconis'-rule:","page":"Supported Methods","title":"Freedman and Diaconis' rule:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The number k of bins is computed according to the formula","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    k = biglceilfracn^132mathrmIQR(boldsymbolx)bigrceil","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where mathrmIQR(boldsymbolx) is the sample interquartile range. This rule dates back to Freedman and Diaconis (1982) and is the default bin selection rule used by the histogram() function from Plots.jl.","category":"page"},{"location":"methods/#Scott's-rule:","page":"Supported Methods","title":"Scott's rule:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The number k of bins is computed according to the formula","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    k = biglceil hatsigma^-1(24sqrtpi)^-13n^13bigrceil","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where hatsigma is the sample standard deviation. Scott's normal reference rule was first proposed by Scott (1979).","category":"page"},{"location":"methods/#Wand's-rule","page":"Supported Methods","title":"Wand's rule","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"A more sophisticated version of Scott's rule, Wand's rule proceeds by determining the bin width h as","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    h = Big(frac6hatC(f_0) nBig)^13","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where hatC(f_0) is an estimate of the functional C(f_0) = int bigf_0(x)big^2mspace2mumathrmdx. The corresponding number of bins k = lceil h^-1rceil. The full details on this method are given in Wand (1997). The density estimate is computed based on a scale estimate, which can be controlled through the scale keyword argument. Possible choices are :stdev, :iqr which uses an estimate based on the sample standard deviation or the sample interquartile range as a scale estimate. The default choice :minim uses the minimum of the above estimates. The level keyword controls the number of stages of functional estimation used to compute hatC, and can take values 0, 1, 2, 3, 4, 5, with the default value being level=2. The choice level=0 corresponds to Scott's rule under the chosen scale estimate.","category":"page"},{"location":"methods/#References","page":"Supported Methods","title":"References","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Simensen, O. H., Christensen, D. & Hjort, N. L. (2025). Random Irregular Histograms. arXiv preprint. doi: 10.48550/ARXIV.2505.22034","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Taylor, C. C. (1987). Akaikeâ€™s information criterion and the histogram. Biometrika, 74, 636â€“639. doi: 10.1093/biomet/74.3.636","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Rozenholc, Y., Mildenberger, T., & Gather, U. (2010). Combining regular and irregular histograms by penalized likelihood. Computational Statistics & Data Analysis, 54, 3313â€“3323. doi: 10.1016/j.csda.2010.04.021","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"BirgÃ©, L., & Rozenholc, Y. (2006). How many bins should be put in a regular histogram. ESAIM: Probability and Statistics, 10, 24â€“45. doi: 10.1051/ps:2006001","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Rudemo, M. (1982). Empirical choice of histograms and kernel density estimators. Scandinavian Journal of Statistics, 9, 65-78","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Hall, P. (1990). Akaikeâ€™s information criterion and Kullbackâ€“Leibler loss for histogram density estimation. Probability Theory and Related Fields, 85, 449â€“467. doi: 10.1007/BF01203164","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Hall, P. and Hannan, E. J. (1988). On stochastic complexity and nonparametric density estimation. Biometrika, 75, 705â€“714. doi: 10.1093/biomet/75.4.705","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Knuth, K. H. (2019). Optimal data-based binning for histograms and histogram-based probability density models. Digital Signal Processing, 95, doi: 10.1016/j.dsp.2019.102581","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Kontkanen, P. and MyllymÃ¤ki, P. (2007). Mdl histogram density estimation. Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics, 2, 219â€“226","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Sturges, H. A. (1926). The choice of a class interval. Journal of the American Statistical Association, 21, 65â€“66. doi: 10.1080/01621459.1926.10502161.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Freedman, D. and Diaconis, P. (1981) On the histogram as a density estimator: L2 theory. Zeitschrift fÃ¼r Wahrscheinlichkeitstheorie und verwandte Gebiete, 57, 453â€“476. doi: 10.1007/BF01025868.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Scott, D. W. (1979). On optimal and data-based histograms. Biometrika, 66, 605â€“610, doi: 10.1093/biomet/66.3.605.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Wand, M. P. (1997). Data-based choice of histogram bin width. The American Statistician, 51, 59â€“64. doi: 10.2307/2684697","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Davies, P. L., Gather, U., Nordman, D., and Weinert, H. (2009). A comparison of automatic histogram constructions. ESAIM: Probability and Statistics, 13, 181â€“196. doi: 10.1051/ps:2008005.","category":"page"},{"location":"#AutoHist.jl-Documentation","page":"Introdution","title":"AutoHist.jl Documentation","text":"","category":"section"},{"location":"","page":"Introdution","title":"Introdution","text":"Fast automatic histogram construction. Supports a plethora of regular and irregular histogram procedures.","category":"page"},{"location":"#Quick-Start","page":"Introdution","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Introdution","title":"Introdution","text":"The two main functions exported by this package are histogram_irregular and histogram_regular, which constructs an irregular or regular histogram with automatic selection of the number of bins based on the sample. The following example shows how to compute and display a regular and an irregular histogram, with an automatic selection of the number of bins.","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"using AutoHist, Random, Distributions\nx = rand(Xoshiro(1812), Normal(), 10^6)     # simulate some data\nh_irr = histogram_irregular(x)              # compute an automatic irregular histogram\nh_reg = histogram_regular(x)                # compute an automatic regular histogram","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"Both histogram_irregular and histogram_regular return a AutoHist.AutomaticHistogram, with weights normalized so that the resulting histograms are probability densities. Alternatively, irregular and regular automatic histograms can be fitted to data using the fit method by controlling the type keyword argument.","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"h_irr = fit(AutomaticHistogram, x; type=:irregular)     # equivalent to h_irr = histogram_irregular(x)\nh_reg = fit(AutomaticHistogram, x; type=:regular)       # equivalent to h_reg = histogram_regular(x)","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"AutomaticHistogram objects are compatible with Plots.jl, which allows us to easily plot the two histograms resulting from the above code snippet:","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"using Plots; gr()\n# Plot the resulting histograms\np_irr = plot(h_irr, xlabel=\"x\", ylabel=\"Density\", label=\"Irregular\", alpha=0.4, color=\"red\")\np_reg = plot(h_reg, xlabel=\"x\", label=\"Regular\", alpha=0.4, color=\"blue\")\nplot(p_irr, p_reg, layout=(1, 2), size=(600, 300))","category":"page"},{"location":"#Supported-methods","page":"Introdution","title":"Supported methods","text":"","category":"section"},{"location":"","page":"Introdution","title":"Introdution","text":"Both the regular and the irregular procedure support a large number of criteria to select the histogram partition. The keyword argument rule controls the criterion used to choose the best partition, and includes the following criteria:","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"Regular Histograms:\nRegular random histogram, :bayes (default)\nL2 cross-validation, :l2cv\nKullback-Leibler cross-validation: :klcv\nAIC, :aic\nBIC, :bic\nBirgÃ© and Rozenholc's criterion, :br\nNormalized Maximum Likelihood, :nml\nMinimum Description Length, :mdl\nSturges' rule, :sturges\nFreedman and Diaconis' rule, :fd\nScott's rule, :scott\nWand's rule, :wand\nIrregular Histograms:\nIrregular random histogram, :bayes (default)\nL2 cross-validation, :l2cv\nKullback-Leibler cross-validation: :klcv\nRozenholc et al. penalty R: :penR\nRozenholc et al. penalty B: :penB\nNormalized Maximum Likelihood: :nml","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"A more detailed description along with references for each method can be found on the methods page.","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"Example usage with different rules:","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"histogram_irregular(x; rule=:penr)\nhistogram_regular(x; rule=:aic)","category":"page"},{"location":"#Features","page":"Introdution","title":"Features","text":"","category":"section"},{"location":"","page":"Introdution","title":"Introdution","text":"In addition to providing automatic histogram construction, this library will at a later point in time include several convenience functions for histograms. These include functions to determine the number and the location of the modes of a histogram, and functions to compute numerical estimation error made with piecewise continuous densities in mind.","category":"page"}]
}
