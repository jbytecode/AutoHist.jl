var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Automatic-histogram-construction","page":"API","title":"Automatic histogram construction","text":"","category":"section"},{"location":"api/#AutoHist.histogram_irregular","page":"API","title":"AutoHist.histogram_irregular","text":"histogram_irregular(x::AbstractVector{<:Real}; rule::Str=\"bayes\", grid::String=\"regular\", right::Bool=true, greedy::Bool=true, maxbins::Int=-1, support::Tuple{Real,Real}=(-Inf,Inf), use_min_length::Bool=false, logprior::Function=k->0.0, a::Real=1.0)\n\nCreate an irregular histogram based on optimization of a criterion based on Bayesian probability, penalized likelihood or LOOCV. Returns a StatsBase.Histogram object with the optimal partition corresponding to the supplied rule.\n\nArguments\n\nx: 1D vector of data for which a histogram is to be constructed.\n\nKeyword arguments\n\nrule: The criterion used to determine the optimal number of bins. Defaults to the Bayesian method of Simensen et al. (2025).\ngrid: String indicating how the finest possible mesh should be constructed. Options are \"data\", which uses each unique data point as a grid point, \"regular\" (default) which constructs a fine regular grid, and \"quantile\" which constructs the grid based on the sample quantiles.\nright: Boolean indicating whether the drawn intervals should be right-inclusive or not. Defaults to true.\ngreedy: Boolean indicating whether or not the greedy binning strategy of Rozenholc et al. (2006) should be used prior to running the dynamical programming algorithm. Defaults to true. The algorithm can be quite slow for large datasets when this keyword is set to false.\nmaxbins: The maximal number of bins to be considered by the optimization criterion, only used if grid is set to \"regular\" or \"quantile\". Defaults to maxbins=min(4*n/log(n)^2, 1000). If the specified argument is not a positive integer, the default value is used.\nsupport: Tuple specifying the the support of the histogram estimate. If the first element is -Inf, then minimum(x) is taken as the leftmost cutpoint. Likewise, if the second elemen is Inf, then the rightmost cutpoint is maximum(x). Default value is (-Inf, Inf), which estimates the support of the data.\nuse_min_length: Boolean indicating whether or not to impose a restriction on the minimum bin length of the histogram. If set to true, the smallest allowed bin length is set to (maximum(x)-minimum(x))/n*log(n)^(1.5).\nlogprior: Unnormalized logprior distribution for the number k of bins. Defaults to a uniform prior. Only used in when rule=\"bayes\".\na: Dirichlet concentration parameter in the Bayesian irregular histogram model. Set to the default value (5.0) if the supplied value is not a positive real number. Only used when rule=\"bayes\".\n\nReturns\n\nH: StatsBase.Histogram object with weights corresponding to densities, e.g. :isdensity is set to true.\n\nExamples\n\njulia> x = [0.037, 0.208, 0.189, 0.656, 0.45, 0.846, 0.986, 0.751, 0.249, 0.447]\njulia> H1 = histogram_irregular(x)\njulia> H2 = histogram_irregular(x; grid=\"quantile\", support=(0.0, 1.0), logprior=k->-log(k), a=sqrt(10))\n\n...\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoHist.histogram_regular","page":"API","title":"AutoHist.histogram_regular","text":"histogram_regular(x::AbstractVector{<:Real}; rule::Str=\"bayes\", right::Bool=true, maxbins::Int=1000, support::Tuple{Real,Real}=(-Inf,Inf), logprior::Function=k->0.0, a::Union{Real,Function}=1.0, level::Int=2, scalest::Symbol=:minim)\n\nCreate a regular histogram based on an asymptotic risk estimate, or optimization criteria from Bayesian probability, penalized likelihood or LOOCV. Returns a StatsBase.Histogram object with regular bins, with the optimal bin number corresponding to the supplied criterion.\n\n...\n\nArguments\n\nx: 1D vector of data for which a histogram is to be constructed.\n\nKeyword arguments\n\nrule: The criterion used to determine the optimal number of bins. Defaults to the method Bayesian method of Simensen et al. (2025)\nright: Boolean indicating whether the drawn intervals should be right-inclusive or not. Defaults to true.\nmaxbins: The maximal number of bins to be considered by the optimization criterion. Ignored if the specified argument is not a positive integer. Defaults to maxbins=1000\nsupport: Tuple specifying the the support of the histogram estimate. If the first element is -Inf, then minimum(x) is taken as the leftmost cutpoint. Likewise, if the second element is Inf, then the rightmost cutpoint is maximum(x). Default value is (-Inf, Inf), which estimates the support of the data.\nlogprior: Unnormalized logprior distribution of the number k of bins. Only used in the case where the supplied rule is \"bayes\". Defaults to a uniform prior.\na: Specifies Dirichlet concentration parameter in the Bayesian histogram model. Can either be a fixed positive number or a function computing aₖ for different values of k. Defaults to 1.0 if not supplied. Uses default if suppled value is negative.\nlevel: Specifies the level used for the Kernel functional estimate in Wands' rule. Only used if rule=\"wand\". Possible values are 0,1,2,3,4 and 5. Default value is level=2.\nscalest: Estimate of scale parameter used in computing Wands' rule. Only used if rule=\"wand\". Possible values are :minim :stdved and :iqr. Default value is scalest=:minim.\n\nReturns\n\nH: StatsBase.Histogram object with weights corresponding to densities, e.g. :isdensity is set to true.\n\nExamples\n\njulia> x = [0.037, 0.208, 0.189, 0.656, 0.45, 0.846, 0.986, 0.751, 0.249, 0.447]\njulia> H1 = histogram_regular(x)\njulia> H2 = histogram_regular(x; logprior=k->-log(k), a=k->0.5*k)\n\n...\n\n\n\n\n\n","category":"function"},{"location":"methods/#Supported-Methods","page":"Supported Methods","title":"Supported Methods","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This page provides background on each histogram method supported through the rule argument. Our presentation is intended to be rather brief, and we do as such not cover the theoretical underpinnings of each method in any detail. For some further background on automatic histogram procedures and the theory behind them, we recommend the excellent reviews contained in the articles of Birgé and Rozenholc (2006) and Davies et al. (2009).","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"For ease of exposition, we present all methods covered here in the context of estimating the density of a sample boldsymbolx = (x_1 x_2 ldots x_n) on the unit interval, but note that extending the procedures presented here to other compact intervals is possible through a suitable affine transformation. In particular, if a density estimate with support ab is desired, we can scale the data to the unit interval through z_i = (x_i - a)(b-a), and apply the methods on this transformed sample and rescale the resulting density estimate to [a,b]. In cases where the support of the density is unknown, a natural choice is a = x_(1) and b = x_(2). Cases where only the lower or upper bound is known can be handled similarly. The transformation used to construct the histogram can be controlled through the support keyword, where the default argument support=(-Inf, Inf) uses the order statistics-based approach described above.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Before we describe the methods included here in more detail, we introduce some notation. We let mathcalI = (mathcalI_1 mathcalI_2 ldots mathcalI_k) denote a partition of 01 into k intervals and write mathcalI_j for the length of interval mathcalI_j. We can then write a histogram density estimate by","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"widehatf(x) = sum_j=1^k fracwidehattheta_jmathcalI_jmathbf1_mathcalI_j(x) quad xin 01","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where mathbf1_mathcalI_j is the indicator function, widehattheta_j geq 0 for all j and sum_j=1^k widehattheta_j = 1.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"For most of the methods considered here, the estimated bin probabilities are the maximum likelihood estimates widehattheta_j = N_jn, where N_j = sum_i=1^n mathbb1_mathcalI_j(x_i) is number of observations landing in interval mathcalI_j . The exception to this rule is the Bayesian approach of Simensen et al. (2025), which uses the Bayes estimator widehattheta_j = (a_j + N_j)(a+n) for (a_1 ldots a_k) in (0infty) and a = sum_j=1^k a_j instead.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The goal of an automatic histogram procedure is to find a partition mathcalI based on the sample alone which produces a reasonable density estimate. Regular histogram procedures only consider regular partitions, where all intervals in the partition are of equal length, so that one only needs to determine the number k of bins. Irregular histograms allow for partitions with intervals of unequal length, and try to determine both the number of bins and the locations of the cutpoints between the intervals. In all the irregular procedures covered here, we attempt to find best partition according to a criterion among all partitions with endpoints belonging to a given discrete mesh.","category":"page"},{"location":"methods/#Irregular-histograms","page":"Supported Methods","title":"Irregular histograms","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The following section describes how each value of the rule keyword supported by the histogram_irregular function selects the optimal histogram partition. In each case, the best partition is selected among the subset of partitions which have cut points belonging to a discrete set of k_n possible cut points.","category":"page"},{"location":"methods/#bayes:","page":"Supported Methods","title":"bayes:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing the log-marginal likelihood conditional on the partition mathcalI = (mathcalI_1 ldots mathcalI_k),","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    sum_j=1^k biglog Gamma(a_j + N_j) - log Gamma(a_j) - N_jlogmathcalI_jbig + log p_n(k) - log binomk_n-1k-1","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Here p_n(k) is the prior distribution on the number k of bins, which can be controlled by supplying a function to the logprior keyword argument. The default value is p_n(k) propto 1. Here, a_j = ak, for a scalar a  0 which can be controlled by the user through the keyword argument a.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This approach to irregular histograms was pioneered by Simensen et al. (2025).","category":"page"},{"location":"methods/#penb:","page":"Supported Methods","title":"penb:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a penalized log-likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    sum_j=1^k N_j log (N_jmathcalI_j) - log binomk_n-1k-1 - k - log^25(k)","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This approach was suggested by Rozenholc et al. (2010).","category":"page"},{"location":"methods/#penr:","page":"Supported Methods","title":"penr:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a penalized log-likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    sum_j=1^k N_j log (N_jmathcalI_j) - frac12nsum_j=1^k fracN_jmathcalI_j - log binomk_n-1k-1 - log^25(k)","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This criterion was also suggested by Rozenholc et al. (2010).","category":"page"},{"location":"methods/#l2cv:","page":"Supported Methods","title":"l2cv:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximization of an L2 leave-one-out cross-validation criterion,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    fracn+1nsum_j=1^k fracN_j^2mathcalI_j - 2sum_j=1^k fracN_jmathcalI_j","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This approach dates back to Rudemo (1982).","category":"page"},{"location":"methods/#klcv:","page":"Supported Methods","title":"klcv:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximization of a Kullback-Leibler cross-validation criterion,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    sum_j=1^k N_jlog(N_j-1) - sum_j=1^k N_jlog I_j","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where the maximmization is over all partitions with N_j geq 2 for all j. This approach was, to our knowledge, first pursued by Simensen et al. (2025).","category":"page"},{"location":"methods/#nml:","page":"Supported Methods","title":"nml:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximization of a penalized likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"beginaligned\n    sum_j=1^k N_jlog fracN_jmathcalI_j - frack-12log(n2) - logfracsqrtpiGamma(k2) - n^-12fracsqrt2kGamma(k2)3Gamma(k2-12) \n    - n^-1left(frac3+k(k-2)(2k+1)36 - fracGamma(k2)^2 k^29Gamma(k2-12)^2 right)  - log binomk_n-1k-1\nendaligned","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This a variant of this criterion first suggested by Kontkanen and Myllymäki (2007). The above criterion uses an asymptotic expansion of their proposed penalty term, as their proposed penalty can be quite expensive to evaluate.","category":"page"},{"location":"methods/#Regular-histograms","page":"Supported Methods","title":"Regular histograms","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The following section details how each value of the rule keyword supported by the histogram_regular function selects the number k of bins to draw a histogram automatically based on a random sample. In the following, mathcalI = (mathcalI_1 mathcalI_2 ldots mathcalI_k) is the corresponding partition of 01 consisting of k equal-length bins.","category":"page"},{"location":"methods/#bayes:-2","page":"Supported Methods","title":"bayes:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing the log-marginal likelihood for given k,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"   nlog (k) + sum_j=1^k biglog Gamma(a_j + N_j) - log Gamma(a_j)big + log p_n(k)","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Here p_n(k) is the prior distribution on the number k of bins, which can be controlled by supplying a function to the logprior keyword argument. The default value is p_n(k) propto 1. Here, a_j = ak, for a scalar a  0, possibly depending on k. The value of a can be set by supplying a fixed, positive scalar or a function a(k) to the keyword argument a.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The particular choices a_j = 05 and p_n(k)propto 1 were suggested by Knuth (2019).","category":"page"},{"location":"methods/#aic:","page":"Supported Methods","title":"aic:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a penalized log-likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    nlog (k) + sum_j=1^k N_j log (N_jn) - k","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The aic criterion was proposed by Taylor (1987) for histograms.","category":"page"},{"location":"methods/#bic:","page":"Supported Methods","title":"bic:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a penalized log-likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    nlog (k) + sum_j=1^k N_j log (N_jn) - frack2log(n)","category":"page"},{"location":"methods/#br:","page":"Supported Methods","title":"br:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a penalized log-likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    nlog (k) + sum_j=1^k N_j log (N_jn) - k - log^25(k)","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This criterion was proposed by Birgé and Rozenholc (2006).","category":"page"},{"location":"methods/#l2cv:-2","page":"Supported Methods","title":"l2cv:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a L2 leave-one-out cross-validation criterion,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    -2k + kfracn+1n^2sum_j=1^k N_j^2","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This approach to histogram density estimation was first considered by Rudemo (1982).","category":"page"},{"location":"methods/#klcv:-2","page":"Supported Methods","title":"klcv:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a Kullback-Leibler leave-one-out cross-validation criterion,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    nlog(k) + sum_j=1^k N_jlog (N_j-1)","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where the maximmization is over all partitions with N_j geq 2 for all j. This approach was first studied by Hall (1990).","category":"page"},{"location":"methods/#mdl:","page":"Supported Methods","title":"mdl:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of finding the model providing the shortest encoding of the data, which is equivalent to maximization of","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    nlog(k) + sum_j=1^k big(N_j-frac12big)logbig(N_j-frac12big) - big(n-frack2big)logbig(n-frack2big) - frack2log(n)","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where the maximmization is over all partitions with N_j geq 1 for all j. The minimum description length principle was first applied to histogram estimation by Hall and Hannan (1988).","category":"page"},{"location":"methods/#nml:-2","page":"Supported Methods","title":"nml:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximization of a penalized likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"beginaligned\n    sum_j=1^k N_jlog fracN_jmathcalI_j - frack-12log(n2) - logfracsqrtpiGamma(k2) - n^-12fracsqrt2kGamma(k2)3Gamma(k2-12) \n    - n^-1left(frac3+k(k-2)(2k+1)36 - fracGamma(k2)^2 k^29Gamma(k2-12)^2 right)\nendaligned","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This is a regular variant of the normalized maximum likelihood criterion considered by Kontkanen and Myllymäki (2007).","category":"page"},{"location":"methods/#Sturges'-rule:","page":"Supported Methods","title":"Sturges' rule:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The number k of bins is computed according to the formula","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    k = lceil log_2(n) rceil + 1","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This classical rule, due to Sturges (1926), is the default for determining the number of bins in R.","category":"page"},{"location":"methods/#Freedman-and-Diaconis'-rule:","page":"Supported Methods","title":"Freedman and Diaconis' rule:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The number k of bins is computed according to the formula","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    k = biglceilfracn^132mathrmIQR(boldsymbolx)bigrceil","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where mathrmIQR(boldsymbolx) is the sample interquartile range. This rule dates back to Freedman and Diaconis (1982) and is the default bin selection rule used by the histogram() function from Plots.jl.","category":"page"},{"location":"methods/#Scott's-rule:","page":"Supported Methods","title":"Scott's rule:","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The number k of bins is computed according to the formula","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    k = biglceil hatsigma^-1(24sqrtpi)^-13n^13bigrceil","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where hatsigma is the sample standard deviation. Scott's normal reference rule was first proposed by Scott (1979).","category":"page"},{"location":"methods/#Wand's-rule","page":"Supported Methods","title":"Wand's rule","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"A more sophisticated version of Scott's rule, Wand's rule proceeds by determining the bin width h as","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    h = big(frac6hatC(f_0) nbig)^13","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where hatC(f_0) is an estimate of a functional C(f_0). The corresponding number of bins k = lceil h^-1rceil. The full details on this method are given in Wand (1997). The density estimate is computed based on a scale estimate, which can be controlled through the scale keyword argument. Possible choices are :stdev, :iqr which uses an estimate based on the sample standard deviation or the sample interquartile range as a scale estimate. The default choice :minim uses the minimum of the above estimates. The level keyword controls the number of stages of functional estimation used to compute hatC, and can take values 0, 1, 2, 3, 4, 5, with the default value being level=2. The choice level=0 corresponds to Scott's rule under the chosen scale estimate.","category":"page"},{"location":"methods/#References","page":"Supported Methods","title":"References","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Simensen, O. H., Christensen, D. & Hjort, N. L. (2025). Random Irregular Histograms. arXiv preprint. doi: 10.48550/ARXIV.2505.22034","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Taylor, C. C. (1987). Akaike’s information criterion and the histogram. Biometrika, 74, 636–639. doi: 10.1093/biomet/74.3.636","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Rozenholc, Y., Mildenberger, T., & Gather, U. (2010). Combining regular and irregular histograms by penalized likelihood. Computational Statistics & Data Analysis, 54, 3313–3323. doi: 10.1016/j.csda.2010.04.021","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Birgé, L., & Rozenholc, Y. (2006). How many bins should be put in a regular histogram. ESAIM: Probability and Statistics, 10, 24–45. doi: 10.1051/ps:2006001","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Rudemo, M. (1982). Empirical choice of histograms and kernel density estimators. Scandinavian Journal of Statistics, 9, 65-78","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Hall, P. (1990). Akaike’s information criterion and Kullback–Leibler loss for histogram density estimation. Probability Theory and Related Fields, 85, 449–467. doi: 10.1007/BF01203164","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Hall, P. and Hannan, E. J. (1988). On stochastic complexity and nonparametric density estimation. Biometrika, 75, 705–714. doi: 10.1093/biomet/75.4.705","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Knuth, K. H. (2019). Optimal data-based binning for histograms and histogram-based probability density models. Digital Signal Processing, 95, doi: 10.1016/j.dsp.2019.102581","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Kontkanen, P. and Myllymäki, P. (2007). Mdl histogram density estimation. Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics, 2, 219–226","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Sturges, H. A. (1926). The choice of a class interval. Journal of the American Statistical Association, 21, 65–66. doi: 10.1080/01621459.1926.10502161.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Freedman, D. and Diaconis, P. (1981) On the histogram as a density estimator: L2 theory. Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete, 57, 453–476. doi: 10.1007/BF01025868.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Scott, D. W. (1979). On optimal and data-based histograms. Biometrika, 66, 605–610, doi: 10.1093/biomet/66.3.605.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Wand, M. P. (1997). Data-based choice of histogram bin width. The American Statistician, 51, 59–64. doi: 10.2307/2684697","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Davies, P. L., Gather, U., Nordman, D., and Weinert, H. (2009). A comparison of automatic histogram constructions. ESAIM: Probability and Statistics, 13, 181–196. doi: 10.1051/ps:2008005.","category":"page"},{"location":"#AutoHist.jl-Documentation","page":"Introdution","title":"AutoHist.jl Documentation","text":"","category":"section"},{"location":"","page":"Introdution","title":"Introdution","text":"Fast automatic histogram construction. Supports a plethora of regular and irregular histogram procedures.","category":"page"},{"location":"#Quick-Start","page":"Introdution","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Introdution","title":"Introdution","text":"The two main functions exported by this package are histogram_irregular and histogram_regular, which constructs an irregular or regular histogram with automatic selection of the number of bins based on the sample. The following example shows how to compute and display a regular and an irregular histogram, with an automatic selection of the number of bins.","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"using AutoHist, Random, Distributions\nx = rand(Xoshiro(1812), Normal(), 10^6)     # simulate some data\nh_irr = histogram_irregular(x)              # compute an automatic irregular histogram\nh_reg = histogram_regular(x)                # compute an automatic regular histogram","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"Both histogram_irregular and histogram_regular return a StatsBase.Histogram, with weights normalized so that the resulting histograms are probability densities. This allows us to easily plot the two histograms resulting from the above code snippet:","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"using Plots; gr()\n# Plot the resulting histograms\np1 = plot(h_irr, xlabel=\"x\", ylabel=\"Density\", label=\"Irregular\", alpha=0.4, color=\"red\")\np2 = plot(h_reg, xlabel=\"x\", label=\"Regular\", alpha=0.4, color=\"blue\")\nplot(p1, p2, layout=(1, 2), size=(600, 300))","category":"page"},{"location":"#Supported-methods","page":"Introdution","title":"Supported methods","text":"","category":"section"},{"location":"","page":"Introdution","title":"Introdution","text":"Both the regular and the irregular procedure support a large number of criteria to select the histogram partition. The keyword argument rule controls the criterion used to choose the best partition, and includes the following criteria:","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"Regular Histograms:\nRegular random histogram, \"bayes\" (default)\nL2 cross-validation, \"l2cv\"\nKullback-Leibler cross-validation: \"klcv\"\nAIC, \"aic\"\nBIC, \"bic\"\nBirgé and Rozenholc's criterion, \"br\"\nNormalized Maximum Likelihood, \"nml\"\nMinimum Description Length, \"mdl\"\nSturges' rule, \"sturges\"\nFreedman and Diaconis' rule, \"fd\"\nScott's rule, \"scott\"\nWand's rule, \"wand\"\nIrregular Histograms:\nIrregular random histogram, \"bayes\" (default)\nL2 cross-validation, \"l2cv\"\nKullback-Leibler cross-validation: \"klcv\"\nRozenholc et al. penalty R: \"penR\"\nRozenholc et al. penalty B: \"penB\"\nNormalized Maximum Likelihood: \"nml\"","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"A more detailed description along with references for each method can be found on the methods page.","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"Example usage with different rules:","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"histogram_irregular(x; rule=\"penr\")\nhistogram_regular(x; rule=\"aic\")","category":"page"},{"location":"#Features","page":"Introdution","title":"Features","text":"","category":"section"},{"location":"","page":"Introdution","title":"Introdution","text":"In addition to providing automatic histogram construction, this library will at a later point in time include several convenience functions for histograms. These include functions to determine the number and the location of the modes of a histogram, and functions to compute numerical estimation error made with piecewise continuous densities in mind.","category":"page"}]
}
