var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#The-AutomaticHistogram-type","page":"API","title":"The AutomaticHistogram type","text":"","category":"section"},{"location":"api/#AutoHist.AutomaticHistogram","page":"API","title":"AutoHist.AutomaticHistogram","text":"AutomaticHistogram\n\nA type for representing a histogram where the histogram partition has been chosen automatically based on the sample. Can be fitted to data using the fit, histogram_irregular or histogram_regular methods.\n\nFields\n\nbreaks: AbstractVector consisting of the cut points in the chosen partition.\ndensity: Estimated density in each bin.\ncounts: The bin counts for the partition corresponding to breaks.\ntype: Symbol indicating whether the histogram was fit using an irregular procedure (type==:irregular) or a regular one (type==:regular).\nclosed: Symbol indicating whether the drawn intervals should be right-inclusive or not. Possible values are :right (default) and :left.\na: Value of the Dirichlet concentration parameter corresponding to the chosen partition. Only of relevance if a Bayesian method was used to fit the histogram, and is otherwise set to NaN.\n\nExamples\n\njulia> using AutoHist\n\njulia> x = LinRange(eps(), 1.0-eps(), 5000) .^(1.0/4.0);\n\njulia> h = fit(AutomaticHistogram, x)\nAutomaticHistogram\nbreaks: [0.00012207031249977798, 0.17763663029325183, 0.29718725232110504, 0.4022468898607337, 0.4928155429121377, 0.5797614498414855, 0.6667073567708333, 0.7572760098222373, 0.8405991706295289, 0.9202995853147645, 1.0000000000000002]\ndensity: [0.00662683597412854, 0.057821970706400425, 0.17596277991076312, 0.36279353706969375, 0.6214544825215076, 0.9730458529384184, 1.4481767793920146, 2.0440057561776532, 2.733509595364622, 3.545742066060367]\ncounts: [5, 34, 92, 164, 270, 423, 656, 852, 1090, 1414]\ntype: irregular\nclosed: right\na: 5.0\n\n\n\n\n\n","category":"type"},{"location":"api/#Fitting-an-automatic-histogram-to-data","page":"API","title":"Fitting an automatic histogram to data","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"An automatic histogram based on regular or irregular partitions can be fitted to the data by calling the fit method.","category":"page"},{"location":"api/#StatsAPI.fit-Tuple{Type{AutomaticHistogram}, AbstractVector{<:Real}}","page":"API","title":"StatsAPI.fit","text":"fit(AutomaticHistogram, x::AbstractVector{x<:Real}; rule=:default, type=:irregular, kwargs...)\n\nFit a histogram to a one-dimensional vector x with an automatic and data-based selection of the histogram partition.\n\nArguments\n\nx: 1D vector of data for which a histogram is to be constructed.\n\nKeyword arguments\n\nrule: The criterion used to determine the optimal number of bins. Default value is rule=:default which uses the default rule for the regular or irregular histogram procedure depending on the value of type.\ntype: Symbol indicating whether the fitted method is a regular and irregular one. The rules :bayes, :l2cv, :klcv and :nml are implemented for both regular and irregular histograms, and this keyword specifies whether the regular or irregular version should be used. For other rules, this function automatically infers the type from the rule keyword, and misspecifying the rule in this case has no effect. Possible values are :irregular (default) and :regular.\nkwargs: Additional keyword arguments passed to histogram_regular or histogram_irregular depending on the specified or inferred type.\n\nReturns\n\nh: An object of type AutomaticHistogram, corresponding to the fitted histogram.\n\nExamples\n\njulia> x = randn(10^3)\njulia> h1 = fit(AutomaticHistogram, x)                                      # fits an irregular histogram\njulia> h2 = fit(AutomaticHistogram, x; rule=:wand, scalest=:stdev, level=4) # fits a regular histogram\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API","title":"API","text":"Alternatively, an automatic histogram can be fitted to the data by the following methods:","category":"page"},{"location":"api/#AutoHist.histogram_irregular","page":"API","title":"AutoHist.histogram_irregular","text":"histogram_irregular(x::AbstractVector{<:Real}; rule::Symbol=:bayes, grid::Symbol=:regular, closed::Symbol=:right, alg::AbstractAlgorithm=:DP, maxbins::Int=-1, support::Tuple{Real,Real}=(-Inf,Inf), use_min_length::Bool=false, logprior::Function=k->0.0, a::Real=1.0)\n\nCreate an irregular histogram based on optimization of a criterion based on Bayesian probability, penalized likelihood or LOOCV. Returns an AutomaticHistogram object with the optimal partition corresponding to the supplied rule.\n\nArguments\n\nx: 1D vector of data for which a histogram is to be constructed.\n\nKeyword arguments\n\nrule: The criterion used to determine the optimal number of bins. Defaults to the Bayesian method of Simensen et al. (2025).\ngrid: Symbol indicating how the finest possible mesh should be constructed. Options are :data, which uses each unique data point as a grid point, :regular (default) which constructs a fine regular grid, and :quantile which constructs the grid based on the sample quantiles.\nclosed: Symbol indicating whether the drawn intervals should be right-inclusive or not. Possible values are :right (default) and :left.\nalg: Algorithm used to fit the model. Currently, only DP() is supported. See DP for further details.\nmaxbins: The maximal number of bins to be considered by the optimization criterion. Only used if grid is set to :regular or :quantile. Defaults to maxbins=min(4*n/log(n)^2, 1000). If the specified argument is not a positive integer, the default value is used.\nsupport: Tuple specifying the the support of the histogram estimate. If the first element is -Inf, then minimum(x) is taken as the leftmost cutpoint. Likewise, if the second elemen is Inf, then the rightmost cutpoint is maximum(x). Default value is (-Inf, Inf), which estimates the support of the data.\nuse_min_length: Boolean indicating whether or not to impose a restriction on the minimum bin length of the histogram. If set to true, the smallest allowed bin length is set to (maximum(x)-minimum(x))/n*log(n)^(1.5).\nlogprior: Unnormalized logprior distribution for the number k of bins. Defaults to a uniform prior. Only used when rule keyword is set to :bayes.\na: Dirichlet concentration parameter in the Bayesian irregular histogram model. Only used when rule is set to :bayes.\n\nReturns\n\nh: The fitted histogram as an AutomaticHistogram object.\n\nExamples\n\njulia> x = [0.037, 0.208, 0.189, 0.656, 0.45, 0.846, 0.986, 0.751, 0.249, 0.447]\njulia> h1 = histogram_irregular(x)\njulia> h2 = histogram_irregular(x; grid=:quantile, support=(0.0, 1.0), logprior=k->-log(k), a=sqrt(10))\n\n\n\n\n\n","category":"function"},{"location":"api/#AutoHist.histogram_regular","page":"API","title":"AutoHist.histogram_regular","text":"histogram_regular(x::AbstractVector{<:Real}; rule::Symbol=:bayes, closed::Symbol=:right, maxbins::Int=1000, support::Tuple{Real,Real}=(-Inf,Inf), logprior::Function=k->0.0, a::Union{Real,Function}=1.0, level::Int=2, scalest::Symbol=:minim)\n\nCreate a regular histogram based on an asymptotic risk estimate, or optimization criteria from Bayesian probability, penalized likelihood or LOOCV. Returns a AutomaticHistogram object with regular bins, with the optimal bin number corresponding to the supplied criterion.\n\nArguments\n\nx: 1D vector of data for which a histogram is to be constructed.\n\nKeyword arguments\n\nrule: The criterion used to determine the optimal number of bins. Defaults to the method Bayesian method of Simensen et al. (2025)\nclosed: Symbol indicating whether the drawn intervals should be right-inclusive or not. Possible values are :right (default) and :left.\nmaxbins: The maximal number of bins to be considered by the optimization criterion. Ignored if the specified argument is not a positive integer. Defaults to maxbins=1000\nsupport: Tuple specifying the the support of the histogram estimate. If the first element is -Inf, then minimum(x) is taken as the leftmost cutpoint. Likewise, if the second element is Inf, then the rightmost cutpoint is maximum(x). Default value is (-Inf, Inf), which estimates the support of the data.\nlogprior: Unnormalized logprior distribution of the number k of bins. Only used in the case where the supplied rule is :bayes. Defaults to a uniform prior.\na: Specifies Dirichlet concentration parameter in the Bayesian histogram model. Can either be a fixed positive number or a function computing aâ‚– for different values of k. Defaults to 1.0 if not supplied. Uses default if suppled value is negative.\nscalest: Estimate of scale parameter used in computing Wands' rule. Only used if rule is set to :wand. Possible values are :minim :stdved and :iqr. Default value is scalest=:minim.\nlevel: Specifies the level used for the Kernel functional estimate in Wands' rule. Only used if rule==:wand. Possible values are 0,1,2,3,4 and 5. Default value is level=2.\n\nReturns\n\nh: The fitted histogram as an AutomaticHistogram object.\n\nExamples\n\njulia> x = [0.037, 0.208, 0.189, 0.656, 0.45, 0.846, 0.986, 0.751, 0.249, 0.447]\njulia> h1 = histogram_regular(x)\njulia> h2 = histogram_regular(x; logprior=k->-log(k), a=k->0.5*k)\n\n\n\n\n\n","category":"function"},{"location":"api/#Additional-methods-for-AutomaticHist","page":"API","title":"Additional methods for AutomaticHist","text":"","category":"section"},{"location":"api/#AutoHist.peaks-Tuple{AutomaticHistogram}","page":"API","title":"AutoHist.peaks","text":"peaks(h::AutomaticHistogram)\n\nReturn the location of the modes/peaks of h as a Vector, sorted in increasing order.\n\nFormally, the modes/peaks of the histogram h are defined as the midpoints of an interval mathcalJ, where the density of h is constant on mathcalJ, and the density of h is strictly smaller than this value in the histogram bins adjacent to mathcalJ. Note that according this definition, mathcalJ is in general a nonempty union of intervals in the histogram partition.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.minimum-Tuple{AutomaticHistogram}","page":"API","title":"Base.minimum","text":"minimum(h::AutomaticHistogram)\n\nReturn the minimum of the support of h.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.maximum-Tuple{AutomaticHistogram}","page":"API","title":"Base.maximum","text":"maximum(h::AutomaticHistogram)\n\nReturn the maximum of the support of h.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.extrema-Tuple{AutomaticHistogram}","page":"API","title":"Base.extrema","text":"extrema(h::AutomaticHistogram)\n\nReturn the minimum and the maximum of the support of h as a 2-tuple.\n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.insupport-Tuple{AutomaticHistogram, Real}","page":"API","title":"Distributions.insupport","text":"insupport(h::AutomaticHistogram, x::Real)\n\nReturn true if x is in the support of h, and false otherwise.\n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.pdf-Tuple{AutomaticHistogram, Real}","page":"API","title":"Distributions.pdf","text":"pdf(h::AutomaticHistogram, x::Real)\n\nEvaluate the probability density function of h at x.\n\n\n\n\n\n","category":"method"},{"location":"api/#AutoHist.cdf-Tuple{AutomaticHistogram, Real}","page":"API","title":"AutoHist.cdf","text":"cdf(h::AutomaticHistogram, x::Real)\n\nEvaluate the cumulative distribution function of h at x.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.length-Tuple{AutomaticHistogram}","page":"API","title":"Base.length","text":"length(h::AutomaticHistogram)\n\nReturns the number of bins of h.\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsAPI.loglikelihood-Tuple{AutomaticHistogram}","page":"API","title":"StatsAPI.loglikelihood","text":"loglikelihood(h::AutomaticHistogram)\n\nCompute the log-likelihood (up to proportionality) of an h.\n\nThe value of the log-likelihood is     âˆ‘â±¼ Nâ±¼ log (dâ±¼), where Nâ±¼, dâ±¼ are the bin counts and estimated densities for bin j.\n\n\n\n\n\n","category":"method"},{"location":"api/#AutoHist.logmarginallikelihood","page":"API","title":"AutoHist.logmarginallikelihood","text":"logmarginallikelihood(h::AutomaticHistogram, a::Real)\nlogmarginallikelihood(h::AutomaticHistogram)\n\nCompute the log-marginal likelihood (up to proportionality) of h when the value of the Dirichlet concentration parameter equals a. This can be automatically inferred if the histogram was fitted with rule=:bayes, and does not have to be explicitly passed as an argument in this case.\n\nAssumes that the Dirichlet prior is centered on the uniform distribution, so that aâ±¼ = a/k for a scalar a>0 and all j. The value of the log-marginal likelihood is     âˆ‘â±¼ {log Î“(Nâ±¼+aâ±¼) - log Î“(aâ±¼) - Nâ±¼log(dâ±¼)} - log Î“(a+n) + log Î“(a), where where Nâ±¼ is the bin count for bin j.\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.convert-Tuple{Type{Histogram}, AutomaticHistogram}","page":"API","title":"Base.convert","text":"convert(Histogram, h::AutomaticHistogram)\n\nConvert an h to a StatsBase.Histogram, normalized to be a probability density.\n\n\n\n\n\n","category":"method"},{"location":"api/#AutoHist.distance","page":"API","title":"AutoHist.distance","text":"distance(h1::AutomaticHistogram, h2::AutomaticHistogram, dist::Symbol=:iae; p::Real=1.0)\n\nCompute a statistical distance between two histogram probability densities.\n\nArguments\n\nh1, h2: The two histograms for which the distance should be computed\ndist: The name of the distance to compute. Valid options are :iae (default), :ise, :hellinger, :sup, :kl, :lp. For the l_p-metric, a given power p can be specified as a keyword argument. \n\nKeyword arguments\n\np: Power of the l_p-metric, which should be a number in the interval 1 infty. Ignored if dist != :lp. Defaults to p=1.0.\n\n\n\n\n\n","category":"function"},{"location":"api/#Index","page":"API","title":"Index","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Pages = [\"api.md\"]","category":"page"},{"location":"examples/algorithm_choice/#Choice-of-algorithm","page":"Choice of algorithm","title":"Choice of algorithm","text":"","category":"section"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"In this section, we empirically assess the efficiency of the dynamic programming algorithm provided for irregular histograms, and show how heuristics can be used to speed up the computations.[1]","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"[1]: Note: The benchmarks presented here were performed on a Windows machine with a 12th Gen IntelÂ® Coreâ„¢ i7-1255U CPU. Results may vary on systems with different hardware configurations.","category":"page"},{"location":"examples/algorithm_choice/#Dynamic-programming","page":"Choice of algorithm","title":"Dynamic programming","text":"","category":"section"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"As a toy problem, we consider standard normal random samples of using a data-based grid. In this case, the number of candidate cutpoints are k_n = n+1, where n is the sample size. For smaller samples, we can just compute the exact solution using the default dynamic programming algorithm, available as DP. The code snippet below illustrates how this algorithm can be explicitly specified when calling fit:","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"using AutoHist, Distributions\nn = 500\nfit(\n    AutomaticHistogram, \n    rand(Normal(), n);\n    grid = :data,\n    alg = DP(greedy=false)\n)","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"Benchmarking the above code snippet yields a mean runtime of around 60 textms on my machine. Since dynamic programming is quick for samples of this size, the greedy algorithm will only be used if the number of candidate cutpoints exceeds 501. Thus, changing greedy to true in the above code snippet would produce the same histogram, as there are 501 possible cutpoints.","category":"page"},{"location":"examples/algorithm_choice/#Speeding-up-computations-via-heuristics","page":"Choice of algorithm","title":"Speeding up computations via heuristics","text":"","category":"section"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"The mathcalO(k_n^3) runtime of dynamic programming means that computing the optimal solution quickly becomes computationally prohibitive, even for moderate samples. As an example, when doubling the number of samples in the above code snippet to n = 1000, the mean runtime increases to 450 textms, a rougly 8-fold increase. To ensure that the code retains good performance even for larger samples, we have implemented a greedy search heuristic which selects a subset of the candidate cutpoints, and the dynamic programming algorithm is subsequently run on this smaller set. Adopting the heuristic approach can improve performance considerably, but comes at the cost of no longer being guaranteed to find the optimal solution. To showcase the computational advantages of the heuristic approach, we run a benchmark on a normal sample of size n = 10^6.","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"n = 10^6\nfit(\n    AutomaticHistogram, \n    rand(Normal(), n);\n    grid = :data,\n    alg = DP(greedy=true) # NB! greedy=true is the default option\n)","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"Benchmarking the above, I find a mean runtime of approximately 720 textms which is not much slower than the time it took to compute the exact solution for random samples of size n = 10^3.","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"The number candidate cutpoints constructed by the greedy search heuristic can be controlled through the gr_maxbins keyword argument, which equals the number of selected gridpoints plus one. By default, the greedy algorithm will produce a subset consisting of max500 n^13+1 cutpoints by default (including the edges). For gr_maxbins1 < gr_maxbins2, the cutpoint subset formed by the greedy algorithm for gr_maxbins1 is a subset of that selected with gr_maxbins2 bins. Thus, increasing the number of candidate cutpoints added this grid will never lead to a worse solution of the original optimization problem. If additional precision is desired in the above example, we can increase gr_maxbins to 1000:","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"n = 10^6\nfit(\n    AutomaticHistogram, \n    rand(Normal(), n);\n    grid = :data,\n    alg = DP(greedy=true, gr_maxbins=10^3)\n)","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"The above code snippet averages a runtime of about 11 texts on my machine.","category":"page"},{"location":"examples/algorithm_choice/#A-note-on-cross-validation-based-criteria","page":"Choice of algorithm","title":"A note on cross-validation based criteria","text":"","category":"section"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"For the L2CV and KLCV criteria, it becomes possible to compute the exact solution via a quadratic-time dynamic programming algorithm instead of the cubic-time algorithm used for the other problems. In practice, this means that computing the exact solution is often feasible even as the number of candidate cutpoints becomes quite large. For instance, benchmarking the following code snippet results in a mean runtime of 42 textms, roughly 2 orders of magnitude below the runtime from computing the exact solution with the cubic time algorithm.","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"n = 10^3\nfit(\n    AutomaticHistogram, \n    rand(Normal(), n);\n    rule = :l2cv,\n    grid = :data,\n    alg = DP(greedy=false)\n)\nnothing # hide","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"Due to the superior runtime complexity of the exact algorithm for cross-validation criteria, the exact solution is by default computed when the number of total candidate cutpoints is less than 3001, and the greedy search heuristic is used thereafter to build a smaller candidate set as previously. By default gr_maxbins is set to max3000 sqrtn, but this can be replaced with a user-defined value if better performance or additional accuracy is desired.","category":"page"},{"location":"methods/#Supported-Methods","page":"Supported Methods","title":"Supported Methods","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This page provides background on each histogram method supported through the rule argument. Our presentation is intended to be rather brief, and we do as such not cover the theoretical underpinnings of each method in great detail. For some further background on automatic histogram procedures and the theory behind them, we recommend the excellent reviews contained in the articles of BirgÃ© and Rozenholc (2006) and Davies et al. (2009).","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"For ease of exposition, we present all methods covered here in the context of estimating the density of a sample boldsymbolx = (x_1 x_2 ldots x_n) on the unit interval, but note that extending the procedures presented here to other compact intervals is possible through a suitable affine transformation. In particular, if a density estimate with support ab is desired, we can scale the data to the unit interval through z_i = (x_i - a)(b-a), and apply the methods on this transformed sample and rescale the resulting density estimate to ab. In cases where the support of the density is unknown, a natural choice is a = x_(1) and b = x_(n). Cases where only the lower or upper bound is known can be handled similarly. The transformation used to construct the histogram can be controlled through the support keyword, where the default argument support=(-Inf, Inf) uses the order statistics-based approach described above.","category":"page"},{"location":"methods/#Notation","page":"Supported Methods","title":"Notation","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Before we describe the methods included here in more detail, we introduce some notation. We let mathcalI = (mathcalI_1 mathcalI_2 ldots mathcalI_k) denote a partition of 01 into k intervals and write mathcalI_j for the length of interval mathcalI_j. The intervals in the partition mathcalI can be either right- or left-closed. Whether a left- or right-closed partition is used to draw the histogram is controlled by the keyword argument closed, with options :left and :right (default). This choice is somewhat arbitrary, but is unlikely to matter much in practical applications.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Based on a partition mathcalI, we can write down the corresponding histogram density estimate by","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"widehatf(x) = sum_j=1^k fracwidehattheta_jmathcalI_jmathbf1_mathcalI_j(x) quad xin 01","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where mathbf1_mathcalI_j is the indicator function, widehattheta_j geq 0 for all j and sum_j=1^k widehattheta_j = 1. ","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"For most of the methods considered here, the estimated bin probabilities are the maximum likelihood estimates widehattheta_j = N_jn, where N_j = sum_i=1^n mathbb1_mathcalI_j(x_i) is number of observations landing in interval mathcalI_j . The exception to this rule is are the two Bayesian approaches, which uses the Bayes estimator widehattheta_j = (a_j + N_j)(a+n) for (a_1 ldots a_k) in (0infty)^k and a = sum_j=1^k a_j instead.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The goal of an automatic histogram procedure is to find a partition mathcalI based on the sample alone which produces a reasonable density estimate. Regular histogram procedures only consider regular partitions, where all intervals in the partition are of equal length, so that one only needs to determine the number k of bins. Irregular histograms allow for partitions with intervals of unequal length, and try to determine both the number of bins and the locations of the cutpoints between the intervals. In all the irregular procedures covered here, we attempt to find best partition according to a criterion among all partitions with endpoints belonging to a given discrete mesh.","category":"page"},{"location":"methods/#Irregular-histograms","page":"Supported Methods","title":"Irregular histograms","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The following section describes how each value of the rule keyword supported by the histogram_irregular function selects the optimal histogram partition. In each case, the best partition is selected among the subset of interval partitions of the unit interval that have cut points belonging to a discrete set of cardinality k_n-1. The construction of the candidate cut point set can be controlled through the grid keyword argument, with options :regular (default), :data and :quantile.","category":"page"},{"location":"methods/#bayes","page":"Supported Methods","title":"bayes","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing the log-marginal likelihood conditional on the partition mathcalI = (mathcalI_1 ldots mathcalI_k),","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    sum_j=1^k biglog Gamma(a_j + N_j) - log Gamma(a_j) - N_jlogmathcalI_jbig + log p_n(k) - log binomk_n-1k-1","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Here p_n(k) is the prior distribution on the number k of bins, which can be controlled by supplying a function to the logprior keyword argument. The default value is p_n(k) propto 1. Here, a_j = ak, for a scalar a  0 which can be controlled by the user through the keyword argument a (default a=5.0).","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This approach to irregular histograms was pioneered by Simensen et al. (2025).","category":"page"},{"location":"methods/#pena","page":"Supported Methods","title":"pena","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a penalized log-likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    sum_j=1^k N_j log (N_jmathcalI_j) - log binomk_n-1k-1 - k - 2log(k) - sqrt2(k-1)Biglog binomk_n-1k-1+ 2log(k)Big","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This approach was suggested by Rozenholc et al. (2010).","category":"page"},{"location":"methods/#penb","page":"Supported Methods","title":"penb","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a penalized log-likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    sum_j=1^k N_j log (N_jmathcalI_j) - log binomk_n-1k-1 - k - log^25(k)","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This approach was suggested by Rozenholc et al. (2010).","category":"page"},{"location":"methods/#penr","page":"Supported Methods","title":"penr","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a penalized log-likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    sum_j=1^k N_j log (N_jmathcalI_j) - frac12nsum_j=1^k fracN_jmathcalI_j - log binomk_n-1k-1 - log^25(k)","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This criterion was also suggested by Rozenholc et al. (2010).","category":"page"},{"location":"methods/#l2cv-(irregular)","page":"Supported Methods","title":"l2cv (irregular)","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximization of an L2 leave-one-out cross-validation criterion,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    fracn+1nsum_j=1^k fracN_j^2mathcalI_j - 2sum_j=1^k fracN_jmathcalI_j","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This approach dates back to Rudemo (1982).","category":"page"},{"location":"methods/#klcv-(irregular)","page":"Supported Methods","title":"klcv (irregular)","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximization of a Kullback-Leibler cross-validation criterion,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    sum_j=1^k N_jlog(N_j-1) - sum_j=1^k N_jlog I_j","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where the maximmization is over all partitions with N_j geq 2 for all j. This approach was, to our knowledge, first pursued by Simensen et al. (2025).","category":"page"},{"location":"methods/#nml-(irregular)","page":"Supported Methods","title":"nml (irregular)","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximization of a penalized likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"beginaligned\n    sum_j=1^k N_jlog fracN_jmathcalI_j - frack-12log(n2) - logfracsqrtpiGamma(k2) - n^-12fracsqrt2kGamma(k2)3Gamma(k2-12) \n    - n^-1left(frac3+k(k-2)(2k+1)36 - fracGamma(k2)^2 k^29Gamma(k2-12)^2 right)  - log binomk_n-1k-1\nendaligned","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This a variant of this criterion first suggested by Kontkanen and MyllymÃ¤ki (2007). The above criterion uses an asymptotic expansion of their proposed penalty term, as their proposed penalty can be quite expensive to evaluate.","category":"page"},{"location":"methods/#Regular-histograms","page":"Supported Methods","title":"Regular histograms","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The following section details how each value of the rule keyword supported by the histogram_regular function selects the number k of bins to draw a histogram automatically based on a random sample. In the following, mathcalI = (mathcalI_1 mathcalI_2 ldots mathcalI_k) is the corresponding partition of 01 consisting of k equal-length bins. In cases where the value of the number of bins is computed by maximizing an expression, we look for the best regular partition among all regular partitions consisting of no more than k_n bins.","category":"page"},{"location":"methods/#bayes,-knuth","page":"Supported Methods","title":"bayes, knuth","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing the log-marginal likelihood for given k,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"   nlog (k) + sum_j=1^k biglog Gamma(a_j + N_j) - log Gamma(a_j)big + log p_n(k)","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Here p_n(k) is the prior distribution on the number k of bins, which can be controlled by supplying a function to the logprior keyword argument. The default value is p_n(k) propto 1. Here, a_j = ak, for a scalar a  0, possibly depending on k. The value of a can be set by supplying a fixed, positive scalar or a function a(k) to the keyword argument a. For rule=:bayes, the default value is a=5.0.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The default regular procedure is rule=:knuth, which corresponds to the particular choices a_j = 05 and p_n(k)propto 1 which is the suggestion of Knuth (2019). Note that if a and logprior will both be overridden in this case.","category":"page"},{"location":"methods/#aic","page":"Supported Methods","title":"aic","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a penalized log-likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    nlog (k) + sum_j=1^k N_j log (N_jn) - k","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The aic criterion was proposed by Taylor (1987) for histograms.","category":"page"},{"location":"methods/#bic","page":"Supported Methods","title":"bic","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a penalized log-likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    nlog (k) + sum_j=1^k N_j log (N_jn) - frack2log(n)","category":"page"},{"location":"methods/#br","page":"Supported Methods","title":"br","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a penalized log-likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    nlog (k) + sum_j=1^k N_j log (N_jn) - k - log^25(k)","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This criterion was proposed by BirgÃ© and Rozenholc (2006).","category":"page"},{"location":"methods/#l2cv-(regular)","page":"Supported Methods","title":"l2cv (regular)","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a L2 leave-one-out cross-validation criterion,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    -2k + kfracn+1n^2sum_j=1^k N_j^2","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This approach to histogram density estimation was first considered by Rudemo (1982).","category":"page"},{"location":"methods/#klcv-(regular)","page":"Supported Methods","title":"klcv (regular)","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximizing a Kullback-Leibler leave-one-out cross-validation criterion,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    nlog(k) + sum_j=1^k N_jlog (N_j-1)","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where the maximmization is over all regular partitions with N_j geq 2 for all j. This approach was first studied by Hall (1990).","category":"page"},{"location":"methods/#mdl","page":"Supported Methods","title":"mdl","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of finding the model providing the shortest encoding of the data, which is equivalent to maximization of","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    nlog(k) + sum_j=1^k big(N_j-frac12big)logbig(N_j-frac12big) - big(n-frack2big)logbig(n-frack2big) - frack2log(n)","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where the maximmization is over all regular partitions with N_j geq 1 for all j. The minimum description length principle was first applied to histogram estimation by Hall and Hannan (1988).","category":"page"},{"location":"methods/#nml-(regular)","page":"Supported Methods","title":"nml (regular)","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Consists of maximization of a penalized likelihood,","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"beginaligned\n    sum_j=1^k N_jlog fracN_jmathcalI_j - frack-12log(n2) - logfracsqrtpiGamma(k2) - n^-12fracsqrt2kGamma(k2)3Gamma(k2-12) \n    - n^-1left(frac3+k(k-2)(2k+1)36 - fracGamma(k2)^2 k^29Gamma(k2-12)^2 right)\nendaligned","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This is a regular variant of the normalized maximum likelihood criterion considered by Kontkanen and MyllymÃ¤ki (2007).","category":"page"},{"location":"methods/#Sturges'-rule","page":"Supported Methods","title":"Sturges' rule","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The number k of bins is computed according to the formula","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    k = lceil log_2(n) rceil + 1","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This classical rule, due to Sturges (1926), is the default for determining the number of bins in R.","category":"page"},{"location":"methods/#Freedman-and-Diaconis'-rule","page":"Supported Methods","title":"Freedman and Diaconis' rule","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The number k of bins is computed according to the formula","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    k = biglceilfracn^132mathrmIQR(boldsymbolx)bigrceil","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where mathrmIQR(boldsymbolx) is the sample interquartile range. This rule dates back to Freedman and Diaconis (1982) and is the default bin selection rule used by the histogram() function from Plots.jl.","category":"page"},{"location":"methods/#Scott's-rule","page":"Supported Methods","title":"Scott's rule","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The number k of bins is computed according to the formula","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    k = biglceil hatsigma^-1(24sqrtpi)^-13n^13bigrceil","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where hatsigma is the sample standard deviation. Scott's normal reference rule was first proposed by Scott (1979).","category":"page"},{"location":"methods/#Wand's-rule","page":"Supported Methods","title":"Wand's rule","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"A more sophisticated version of Scott's rule, Wand's rule proceeds by determining the bin width h as","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"    h = Big(frac6hatC(f_0) nBig)^13","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where hatC(f_0) is an estimate of the functional C(f_0) = int f_0(x)^2 textdx. The corresponding number of bins is k = lceil h^-1rceil. The full details on this method are given in Wand (1997). The density estimate is computed based on a scale estimate, which can be controlled through the scale keyword argument. Possible choices are :stdev, :iqr which uses an estimate based on the sample standard deviation or the sample interquartile range as a scale estimate. The default choice :minim uses the minimum of the above estimates. The level keyword controls the number of stages of functional estimation used to compute hatC, and can take values 0, 1, 2, 3, 4, 5, with the default value being level=2. The choice level=0 corresponds to Scott's rule under the chosen scale estimate.","category":"page"},{"location":"examples/density_estimation/#Density-estimation","page":"Density estimation","title":"Density estimation","text":"","category":"section"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"The following document illustrates the use of AutoHist.jl through examples from the world of density estimation. In particular, we showcase some of the relative advantages and disadvantages of regular and irregular histogram procedures.","category":"page"},{"location":"examples/density_estimation/#Estimating-the-LogNormal-probability-density","page":"Density estimation","title":"Estimating the LogNormal probability density","text":"","category":"section"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"We start by considering an example with some simulated data from the LogNormal-distribution. To start, we fit a regular histogram to the data, using the approach of BirgÃ© and Rozenholc (2006), which corresponds to rule=:br.","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"using AutoHist, Random, Distributions\nx = rand(Xoshiro(1812), LogNormal(), 10^4)\nh1 = fit(AutomaticHistogram, x; rule=:br)","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"Alternatively, since the standard LogNormal pdf has known support 0infty), we can incorporate this knowledge in our histogram estimate through the support keyword.","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"h2 = fit(AutomaticHistogram, x; rule=:br, support=(0.0, Inf))","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"To quantify the difference of using the correct, known support in this case, we compute the integrated absolute error between the two densities in the following code snippet, which is given by int f(x) - g(x)textdx.","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"distance(h1, h2, :iae)","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"The resulting l_1 distance of 0042 indicates that the new bin origin at 0 has a moderate effect on the resulting density estimate.","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"The standard LogNormal is quite challenging to estimate well using a regular histogram procedure due to its heavy tails. These two factors make irregular methods an appealing alternative in this case. Here, we use the penalized log-likelihood approach from Rozenholc et al. (2010) with penalty :penr and a data-based grid to construct the histogram.","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"h3 = fit(AutomaticHistogram, x; rule=:penr, grid=:data)","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"To compare the two approaches, we can plot the resulting histograms along with the true density:","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"using Distributions, Plots; gr() # hide\nt = LinRange(0.0, 8.5, 1000) # hide\np = plot(t, pdf(LogNormal(), t), xlabel=\"x\", label=\"True density\", color=\"blue\", lw=2.0, linestyle=:dash) # hide\np1 = plot(p, h2, ylabel=\"Density\", label=\"Regular\", alpha=0.4, color=\"red\") # hide\nxlims!(p1, -0.5, 8.5) # hide\np2 = plot(p, h3, label=\"Irregular\", alpha=0.4, color=\"black\") # hide\nxlims!(p2, -0.5, 8.5) # hide\nplot(p1, p2, layout=(1, 2), size=(670, 320)) # hide","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"The irregular procedure selects smaller bin widths near the origin, reflecting the fact that the LogNormal density is rapidly changing in this area. On the other hand, the bin widths are larger in the flatter region in the right tail of the density. Both histogram procedures provide quite reasonable estimates of the density, owing to the fairly large sample size.","category":"page"},{"location":"examples/density_estimation/#Mode-hunting","page":"Density estimation","title":"Mode hunting","text":"","category":"section"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"In an exploratory data analysis setting, identifying key features in a dataset such as modes is frequently of great interest to statisticians and practicioners alike. Unfortunately, most popular regular histogram have been designed with good performance in terms of statistical risk with respect to classical, integral-based loss functions, which typically results in a large amount of spurious histogram modes in regions where the true density is flat and in the tails of the density (Scott, 1992). In practice, this means that a data-analyst must use subjective judgement to infer whether a regular histogram estimate is indicative of a mode being present or not. If the presence of a mode is deemed likely, subjective visual smoothing is typically required to get a rough idea of its location. In constrast, some irregular histogram procedures have been shown empirically to perform quite well with regard to automatic mode detection in cases where the true density has a smallish amount of well-separated modes, see e.g. Davies et al. (2009); Li et al. (2020); Simensen et al. (2025).","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"To illustrate the advantage of irregular histograms when it comes to mode identification, we will consider an example where we attempt to locate the modes of the Harp density of Li et al. (2020), plotted below.","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"module TestDistributions # hide\n\nusing Distributions # hide\nusing Random # hide\nusing Plots # hide\n\nimport Distributions: pdf, cdf # hide\nimport Random: rand # hide\n\nexport pdf, cdf, rand, peaks, plot_test_distribution, Harp # hide\n\nstruct Harp <: ContinuousUnivariateDistribution end # hide\nfunction pdf(d::Harp, x::Real) # hide\n    means = [0.0, 5.0, 15.0, 30.0, 60.0] # hide\n    sds = [0.5, 1.0, 2.0, 4.0, 8.0] # hide\n    dens = 0.0 # hide\n    for j = 1:5 # hide\n        dens = dens + 0.2 * pdf(Normal(means[j], sds[j]), x) # hide\n    end # hide\n    return dens # hide\nend # hide\nfunction cdf(d::Harp, x::Real) # hide\n    means = [0.0, 5.0, 15.0, 30.0, 60.0] # hide\n    sds = [0.5, 1.0, 2.0, 4.0, 8.0] # hide\n    cum = 0.0 # hide\n    for j = 1:5 # hide\n        cum += 0.2 * cdf(Normal(means[j], sds[j]), x) # hide\n    end # hide\n    return cum # hide\nend # hide\nfunction rand(rng::AbstractRNG, d::Harp) # hide\n    means = [0.0, 5.0, 15.0, 30.0, 60.0] # hide\n    sds = [0.5, 1.0, 2.0, 4.0, 8.0] # hide\n    j = rand(rng, DiscreteUniform(1, 5)) # hide\n    return rand(rng, Normal(means[j], sds[j])) # hide\nend # hide\nfunction peaks(d::Harp) # hide\n    return Float64[0.0, 5.0, 15.001837158203125, 30.003319148997427, 60.000555419921874] # hide\nend # hide\nfunction plot_test_distribution(d::Harp) # hide\n    if d != Harp() # hide\n        throw(ArgumentError(\"Supplied distribution $d is not supported.\")) # hide\n    end # hide\n    dom = LinRange(-5.0, 80.0, 5000) # hide\n    x_lims = [-5.0, 80.0] # hide\n    p = plot() # hide\n    t = dom # hide\n    plot!(p, t, pdf.(d, t), label=\"\", ylims=(0.0, Inf), titlefontsize=10, color=\"blue\", linestyle=:dash) # hide\n    xlims!(p, x_lims...) # hide\n    return p # hide\nend # hide\nend # hide","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"using Plots; gr() # hide\nimport .TestDistributions as TD # hide\np = TD.plot_test_distribution(TD.Harp()) # hide\nplot!(p, title=\"\", xlab=\"x\", ylab=\"Density\") # hide\nplot(p, size=(650, 310)) # hide","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"The Harp density has 5 peaks with varying degrees of sharpness, with the location of each mode becoming gradually more difficult to locate in absolute terms as we move rightward on the x-axis. In the numerical experiment to follow, we generate a random sample of size n = 5000 from the Harp density, and fit both an irregular and a regular histogram to the data. Motivated by the results of the simulation studies in Davies et al. (2009) and Simensen et al. (2025), we have used the BIC criterion to draw the regular histogram, and the random irregular histogram method (rule=:bayes), as both of these have been shown to perform relatively well compared to their respective regular/irregular counterparts in automatic mode identification. To access the Harp distribution, we use the TestDistributions package, which can be found in the following github repository.","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"import .TestDistributions as TD\nusing AutoHist, Distributions, Random\nx = rand(Xoshiro(1812), TD.Harp(), 5000)\n\nh_reg = fit(AutomaticHistogram, x; rule=:bic)\nh_irr = fit(AutomaticHistogram, x; rule=:bayes)","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"We can now add the two fitted histograms along with the location of their modes to the plot of the Harp density above:","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"using Plots; gr() # hide\np = TD.plot_test_distribution(TD.Harp()) # hide\nplot!(p, title=\"\", xlab=\"x\", ylab=\"Density\") # hide\np1 = plot(p, h_reg, title=\"Regular\", color=\"red\", alpha=0.4, label=\"\", ylims=[-0.015, 0.17]) # hide\nscatter!(p1, peaks(h_reg), fill(-0.008, length(h_reg)), color=\"red\", label=\"\", ms=3) # hide\np2 = plot(p, h_irr, title=\"Irregular\", color=\"black\", alpha=0.4, label=\"\", ylims=[-0.015, 0.17]) # hide\nscatter!(p2, peaks(h_irr), fill(-0.008, length(h_irr)), color=\"black\", label=\"\", ms=3) # hide\nplot(p1, p2, layout=(2, 1), size=(670, 650)) # hide","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"The spatial inhomogeneity of the Harp density causes some trouble for the regular histogram, as the use of a global bin width leads to undersmoothing near the sharpest mode and oversmoothing near the flattest mode and in the tails. In particular, the regular estimate introduces many spurious modes near the rightmost mode of the true density, making it difficult to exactly infer the location and the number of modes in the region surrounding it. In contrast, the irregular histogram estimate is able to correctly infer the number of modes in the density automatically, with no subjective judgements required. The good mode-finding behavior is in this case enabled by the fact that the bin widths are able adapt to the local smoothness of the true density, resulting in more smoothing near modes. To inspect the accuracy of the inferred modes, we print the locations of the true modes and the peaks of the irregular histogram.","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"println(\"True modes: \", TD.Harp() |> TD.peaks |> x -> round.(x; digits=2))\nprintln(\"AutoHist modes: \", h_irr |> peaks |> x -> round.(x; digits=2))","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"We observe that all of the histogram peaks are quite close to a corresponding true mode, especially when taking the sharpness of each individual peak into account.","category":"page"},{"location":"#AutoHist.jl-Documentation","page":"Introdution","title":"AutoHist.jl Documentation","text":"","category":"section"},{"location":"","page":"Introdution","title":"Introdution","text":"Fast automatic histogram construction. Supports a plethora of regular and irregular histogram procedures.","category":"page"},{"location":"#Introduction","page":"Introdution","title":"Introduction","text":"","category":"section"},{"location":"","page":"Introdution","title":"Introdution","text":"Despite being the oldest nonparametric density estimator, the histogram remains widespread in use even to this day. Regrettably, the quality of a histogram density estimate is rather sensitive to the choice of partition used to draw the histogram, which has lead to the development of automatic histogram methods that select the partition based on the sample itself. Unfortunately, most default histogram plotting software only support a few regular automatic histogram procedures, where all the bins are of equal length, and use very simple plug-in rules by default to compute the the number of bins, frequently leading to poor density estimates for non-normal data. Moreover, fast and fully automatic irregular histogram methods are rarely supported by default plotting software, which has prevented their adaptation by practitioners.","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"The AutoHist.jl package makes it easy to construct both regular and irregular histograms automatically based on a given one-dimensional sample. It currently supports 7 different methods for irregular histograms and 12 criteria for regular histograms from the statistical literature. In addition, the package provides a number of convenience functions for automatic histograms, such as methods for evaluating the histogram probability density function or identifying the location of modes.","category":"page"},{"location":"#Quick-Start","page":"Introdution","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Introdution","title":"Introdution","text":"The three main functions exported by this package are fit, histogram_irregular and histogram_regular, which constructs an irregular or regular histogram with automatic selection of the number of bins based on the sample. The following example shows how to compute and display an irregular and a regular histogram, with an automatic selection of the number of bins.","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"using AutoHist, Random, Distributions\nx = rand(Xoshiro(1812), Normal(), 10^6)     # simulate some data\nh_irr = histogram_irregular(x)              # compute an automatic irregular histogram\nh_reg = histogram_regular(x)                # compute an automatic regular histogram","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"Alternatively, irregular and regular automatic histograms can be fitted to data using the fit method by controlling the type keyword argument.","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"h_irr = fit(AutomaticHistogram, x; type=:irregular)  # equivalent to h_irr = histogram_irregular(x)\nh_reg = fit(AutomaticHistogram, x; type=:regular)    # equivalent to h_reg = histogram_regular(x)","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"All of the above functions return an object of type AutomaticHistogram, with weights normalized so that the resulting histograms are probability densities. This type represents the histogram in a similar fashion to StatsBase.Histogram, but has more fields to enable the use of several convenience functions.","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"h_irr","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"AutomaticHistogram objects are compatible with Plots.jl, which allows us to easily plot the two histograms resulting from the above code snippet via e.g. Plots.plot(h_irr). To show both histograms side by side, we can create a plot as follows:","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"import Plots; Plots.gr()\np_irr = Plots.plot(h_irr, xlabel=\"x\", ylabel=\"Density\", title=\"Irregular\", alpha=0.4, color=\"black\", label=\"\")\np_reg = Plots.plot(h_reg, xlabel=\"x\", title=\"Regular\", alpha=0.4, color=\"red\", label=\"\")\nPlots.plot(p_irr, p_reg, layout=(1, 2), size=(670, 320))","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"Alternatively, Makie.jl can also be used to make graphical displays of the fitted histograms via e.g. Makie.plot(h_irr). To produce a plot similar to the above display, we may for instance do the following:","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"import CairoMakie, Makie # using the CairoMakie backend\nfig = Makie.Figure(size=(670, 320))\nax1 = Makie.Axis(fig[1, 1], title=\"Irregular\", xlabel=\"x\", ylabel=\"Density\")\nax2 = Makie.Axis(fig[1, 2], title=\"Regular\", xlabel=\"x\")\np_irr = Makie.plot!(ax1, h_irr, alpha=0.4, color=\"black\")\np_reg = Makie.plot!(ax2, h_reg, alpha=0.4, color=\"red\")\nfig","category":"page"},{"location":"#Supported-methods","page":"Introdution","title":"Supported methods","text":"","category":"section"},{"location":"","page":"Introdution","title":"Introdution","text":"Both the regular and the irregular procedure support a large number of criteria to select the histogram partition. The keyword argument rule controls the criterion used to choose the best partition, and includes the following options:","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"Regular Histograms:\nKnuth's rule, :knuth (default)\nRandom regular histogram, :bayes\nL2 cross-validation, :l2cv\nKullback-Leibler cross-validation: :klcv\nAIC, :aic\nBIC, :bic\nBirgÃ© and Rozenholc's criterion, :br\nNormalized Maximum Likelihood, :nml\nMinimum Description Length, :mdl\nSturges' rule, :sturges\nFreedman and Diaconis' rule, :fd\nScott's rule, :scott\nWand's rule, :wand\nIrregular Histograms:\nRandom irregular histogram, :bayes (default)\nL2 cross-validation, :l2cv\nKullback-Leibler cross-validation: :klcv\nRozenholc et al. penalty R: :penr\nRozenholc et al. penalty B: :penb\nRozenholc et al. penalty A: :pena\nNormalized Maximum Likelihood: :nml","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"A more detailed description along with references for each method can be found on the methods page.","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"Example usage with different rules:","category":"page"},{"location":"","page":"Introdution","title":"Introdution","text":"histogram_irregular(x; rule=:penr)\nhistogram_regular(x; rule=:aic)","category":"page"},{"location":"algorithms/#Algorithms-for-constructing-irregular-histograms","page":"Algorithms","title":"Algorithms for constructing irregular histograms","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Constructing data-adaptive irregular histograms is in general a difficult problem from a computational perspective, and as a result computing the exact optimal partition is often impractical for larger sample sizes. This package solves the problem of irregular histogram construction via heuristics that combine a greedy search procedure with dynamic programming techniques to quickly compute a nearly optimal partition. Examples showcasing the use of the provided algorithms in toy problems can be found here. Note that the default option for the alg keyword offers a reasonable tradeoff between accuracy and computational efficiency, and in cases where one simply wants to draw an irregular histogram quickly for a given dataset, these default choices will typically yield a reasonable density estimate within a reasonable amount of time. However, if better performance or additional accuracy is desired, then a more fine-tuned approach to selecting the algorithm used and its hyperparameters is needed.","category":"page"},{"location":"algorithms/#Problem-description","page":"Algorithms","title":"Problem description","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"All the irregular histogram methods supported by this library are the product of solving an optimization problem of the form","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"    max_1leq kleq k_nmax_boldsymbolt_0k Bigsum_j=1^k Phi(tau_n t_j-1 tau_n t_j + Psi(k)Big","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"where the inner maximum is over integer vectors boldsymbolt_0k satisfying 0 = t_0  t_1  cdots  t_k-1  t_k = k_n and tau_njcolon 0leq j leq k_n are the candidate cutpoints between the partition intervals. While it is generally desirable to use a moderate number of possible cutpoints relative to the number of samples, this comes at a heavy computational cost if an exact solution of the above optimization is desired as the runtime complexity of the algorithm is cubic in the number of candidates. We note that for the special case where Psi(k) = 0 for all k, a more efficient algorithm is available, allowing for a quadratic-time solution. In both cases, computing the exact solution becomes unfeasible for larger sample sizes.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"To ease the computational burden we adopt a greedy search heuristic to construct a subset of possible cutpoints when the number of candidates is large, and then run the optimization algorithm of choice on this smaller set. To find the optimal partition for a given set of candidate cutpoints, this package includes two solvers; an exact dynamic programming algorithm and a heuristic greedy pruned dynamic programming algorithm. The algorithm used can be controlled via the alg keyword passed to fit or histogram_irregular.","category":"page"},{"location":"algorithms/#Dynamic-programming","page":"Algorithms","title":"Dynamic programming","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"The choice alg = DP() results in the use of the exact dynamic programming algorithm of Kanazawa (1988) or, if applicable, the exact optimal partitioning algorithm of Jackson et al. (2005) to find the optimal histogram partition. Runtime complexity is cubic in the number of candidate cutpoints for the former and quadratic for the latter.","category":"page"},{"location":"algorithms/#AutoHist.DP","page":"Algorithms","title":"AutoHist.DP","text":"DP(; greedy::Bool=true, gr_maxbins::Union{Int, Symbol}=:default)\n\nDynamic programming algorithm for constructing an irregular histogram.\n\nKeyword arguments\n\ngreedy: Boolean indicating whether or not the greedy cutpoint selection strategy of Rozenholc et al. (2010) should be used to select a smaller number of candidate cutpoints prior to running the dynamic programming algorithm. Defaults to true.\ngr_maxbins: Number of candidate cutpoints chosen by the greedy algorithm. Supplying gr_maxbins=:default results in the selection of at most max(500, n^(1/3))+1 candidate cutpoints (including edges).\n\nnote: Note\nThis algorithm can be quite slow for large datasets when the greedy keyword is set to false.\n\nExamples\n\njulia> using AutoHist\n\njulia> x = LinRange(eps(), 1.0-eps(), 5000) .^(1.0/4.0);\n\njulia> h = fit(AutomaticHistogram, x; alg = DP(greedy=true, gr_maxbins=200))\nAutomaticHistogram\nbreaks: [0.00012207031249977798, 0.17763663029325183, 0.29718725232110504, 0.4022468898607337, 0.4928155429121377, 0.5797614498414855, 0.6667073567708333, 0.7572760098222373, 0.8405991706295289, 0.9239223314368207, 1.0000000000000002]\ndensity: [0.00662683597412854, 0.057821970706400425, 0.17596277991076312, 0.36279353706969375, 0.6214544825215076, 0.9730458529384184, 1.4481767793920146, 2.0440057561776532, 2.7513848134529346, 3.564842182949155]\ncounts: [5, 34, 92, 164, 270, 423, 656, 852, 1147, 1357]\ntype: irregular\nclosed: right\na: 5.0\n\njulia> h = fit(AutomaticHistogram, x; alg = DP(greedy=false))\nAutomaticHistogram\nbreaks: [0.00012207031249977798, 0.17763663029325183, 0.29718725232110504, 0.4022468898607337, 0.4928155429121377, 0.5797614498414855, 0.6667073567708333, 0.7572760098222373, 0.8405991706295289, 0.9202995853147645, 1.0000000000000002]\ndensity: [0.00662683597412854, 0.057821970706400425, 0.17596277991076312, 0.36279353706969375, 0.6214544825215076, 0.9730458529384184, 1.4481767793920146, 2.0440057561776532, 2.733509595364622, 3.545742066060367]\ncounts: [5, 34, 92, 164, 270, 423, 656, 852, 1090, 1414]\ntype: irregular\nclosed: right\na: 5.0\n\n\n\n\n\n","category":"type"}]
}
