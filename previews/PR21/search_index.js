var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#The-AutomaticHistogram-type","page":"API","title":"The AutomaticHistogram type","text":"","category":"section"},{"location":"api/#AutoHist.AutomaticHistogram","page":"API","title":"AutoHist.AutomaticHistogram","text":"AutomaticHistogram\n\nA type for representing a histogram where the histogram partition has been chosen automatically based on the sample. Can be fitted to data using the fit method.\n\nFields\n\nbreaks: AbstractVector consisting of the cut points in the chosen partition.\ndensity: Estimated density in each bin.\ncounts: The bin counts for the partition corresponding to breaks.\ntype: Symbol indicating whether the histogram was fit using an irregular procedure (type==:irregular) or a regular one (type==:regular).\nclosed: Symbol indicating whether the drawn intervals should be right-inclusive or not. Possible values are :right (default) and :left.\na: Value of the Dirichlet concentration parameter corresponding to the chosen partition. Only of relevance if a Bayesian method was used to fit the histogram, and is otherwise set to NaN.\n\nExamples\n\njulia> x = LinRange(eps(), 1.0-eps(), 5000) .^(1.0/4.0);\n\njulia> h = fit(AutomaticHistogram, x)\nAutomaticHistogram{Vector{Float64}, Vector{Float64}, Vector{Int64}}\nbreaks: [0.0001220703125, 0.17763663029325183, 0.29718725232110504, 0.4022468898607337, 0.4928155429121377, 0.5797614498414855, 0.6667073567708333, 0.7572760098222373, 0.8405991706295289, 0.9202995853147645, 1.0]\ndensity: [0.006626835974128547, 0.057821970706400425, 0.17596277991076312, 0.36279353706969375, 0.6214544825215076, 0.9730458529384184, 1.4481767793920146, 2.0440057561776532, 2.733509595364622, 3.545742066060377]\ncounts: [5, 34, 92, 164, 270, 423, 656, 852, 1090, 1414]\ntype: irregular\nclosed: right\na: 5.0\n\n\n\n\n\n","category":"type"},{"location":"api/#Fitting-an-automatic-histogram-to-data","page":"API","title":"Fitting an automatic histogram to data","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"An automatic histogram based on regular or irregular partitions can be fitted to the data by calling the fit method.","category":"page"},{"location":"api/#StatsAPI.fit-Tuple{Type{AutomaticHistogram}, AbstractVector{<:Real}, AutoHist.AbstractRule}","page":"API","title":"StatsAPI.fit","text":"fit(\n    AutomaticHistogram,\n    x::AbstractVector{<:Real},\n    rule::AbstractRule=RIH();\n    support::Tuple{Real,Real} = (-Inf,Inf),\n    closed::Symbol            = :right\n)\n\nFit a histogram to a one-dimensional vector x with an automatic and data-based selection of the histogram partition.\n\nArguments\n\nx: 1D vector of data for which a histogram is to be constructed.\nrule: The criterion used to determine the optimal number of bins. Default value is rule=RIH(), the random irregular histogram.\n\nKeyword arguments\n\nclosed: Symbol indicating whether the drawn intervals should be right-inclusive or not. Possible values are :right (default) and :left.\nsupport: Tuple specifying the the support of the histogram estimate. If the first element is -Inf, then minimum(x) is taken as the leftmost cutpoint. Likewise, if the second element is Inf, then the rightmost cutpoint is maximum(x). Default value is (-Inf, Inf), which estimates the support of the data.\n\nReturns\n\nh: An object of type AutomaticHistogram, corresponding to the fitted histogram.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);\n\njulia> fit(AutomaticHistogram, x) == fit(AutomaticHistogram, x, RIH())\ntrue\n\njulia> h = fit(AutomaticHistogram, x, Wand(scalest=:stdev, level=4))\nAutomaticHistogram{LinRange{Float64, Int64}, Vector{Float64}, Vector{Int64}}\nbreaks: LinRange{Float64}(0.0, 1.0, 27)\ndensity: [0.0052, 0.0312, 0.0884, 0.1612, 0.2652, 0.4004, 0.5408, 0.7176, 0.8944, 1.0868  …  2.0072, 1.9656, 1.8616, 1.69, 1.4508, 1.1596, 0.8372, 0.5044, 0.2184, 0.0364]\ncounts: [1, 6, 17, 31, 51, 77, 104, 138, 172, 209  …  386, 378, 358, 325, 279, 223, 161, 97, 42, 7]\ntype: regular\nclosed: right\na: NaN\n\n\n\n\n\n","category":"method"},{"location":"api/#AutoHist.autohist-Tuple{AbstractVector{<:Real}, AutoHist.AbstractRule}","page":"API","title":"AutoHist.autohist","text":"autohist(\n    x::AbstractVector{<:Real},\n    rule::AbstractRule=RIH();\n    kwargs...\n)\n\nFit an automatic histogram to data based on the supplied rule. This is an alias for fit(AutomaticHistogram, x, rule; kwargs...). See fit for further details.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);\n\njulia> autohist(x, Sturges()) == fit(AutomaticHistogram, x, Sturges())\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#Additional-methods-for-AutomaticHist","page":"API","title":"Additional methods for AutomaticHist","text":"","category":"section"},{"location":"api/#AutoHist.peaks-Tuple{AutomaticHistogram}","page":"API","title":"AutoHist.peaks","text":"peaks(h::AutomaticHistogram)\n\nReturn the location of the modes/peaks of h as a Vector, sorted in increasing order.\n\nFormally, the modes/peaks of the histogram h are defined as the midpoints of an interval mathcalJ, where the density of h is constant on mathcalJ, and the density of h is strictly smaller than this value in the histogram bins adjacent to mathcalJ. Note that according this definition, mathcalJ is in general a nonempty union of intervals in the histogram partition.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.minimum-Tuple{AutomaticHistogram}","page":"API","title":"Base.minimum","text":"minimum(h::AutomaticHistogram)\n\nReturn the minimum of the support of h.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.maximum-Tuple{AutomaticHistogram}","page":"API","title":"Base.maximum","text":"maximum(h::AutomaticHistogram)\n\nReturn the maximum of the support of h.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.extrema-Tuple{AutomaticHistogram}","page":"API","title":"Base.extrema","text":"extrema(h::AutomaticHistogram)\n\nReturn the minimum and the maximum of the support of h as a 2-tuple.\n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.insupport-Tuple{AutomaticHistogram, Real}","page":"API","title":"Distributions.insupport","text":"insupport(h::AutomaticHistogram, x::Real)\n\nReturn true if x is in the support of h, and false otherwise.\n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.pdf-Tuple{AutomaticHistogram, Real}","page":"API","title":"Distributions.pdf","text":"pdf(h::AutomaticHistogram, x::Real)\n\nEvaluate the probability density function of h at x.\n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.cdf-Tuple{AutomaticHistogram, Real}","page":"API","title":"Distributions.cdf","text":"cdf(h::AutomaticHistogram, x::Real)\n\nEvaluate the cumulative distribution function of h at x.\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.quantile-Tuple{AutomaticHistogram, Real}","page":"API","title":"Statistics.quantile","text":"quantile(h::AutomaticHistogram, q::Real)\n\nEvaluate the quantile function of h at q in 0 1.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.length-Tuple{AutomaticHistogram}","page":"API","title":"Base.length","text":"length(h::AutomaticHistogram)\n\nReturns the number of bins of h.\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsAPI.loglikelihood-Tuple{AutomaticHistogram}","page":"API","title":"StatsAPI.loglikelihood","text":"loglikelihood(h::AutomaticHistogram)\n\nCompute the log-likelihood (up to proportionality) of an h.\n\nThe value of the log-likelihood is     sum_j N_j log (d_j) where N_j, d_j are the bin counts and estimated densities for bin j.\n\n\n\n\n\n","category":"method"},{"location":"api/#AutoHist.logmarginallikelihood","page":"API","title":"AutoHist.logmarginallikelihood","text":"logmarginallikelihood(h::AutomaticHistogram, a::Real)\nlogmarginallikelihood(h::AutomaticHistogram)\n\nCompute the log-marginal likelihood (up to proportionality) of h when the value of the Dirichlet concentration parameter equals a. This can be automatically inferred if the histogram was fitted with the rule argument set to RIH or RRH, and does not have to be explicitly passed as an argument in this case.\n\nAssumes that the Dirichlet prior is centered on the uniform distribution, so that a_j = ak for a scalar a0 and all j. The value of the log-marginal likelihood is sum_j  log Gamma (a_j + N_j) - log Gamma (a_j) - N_jlog mathcalI_j  - log Gamma (a+n) + log Gamma (a) , where N_j is the bin count for bin j .\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.convert-Tuple{Type{Histogram}, AutomaticHistogram}","page":"API","title":"Base.convert","text":"convert(Histogram, h::AutomaticHistogram)\n\nConvert an h to a StatsBase.Histogram, normalized to be a probability density.\n\n\n\n\n\n","category":"method"},{"location":"api/#AutoHist.distance","page":"API","title":"AutoHist.distance","text":"distance(h1::AutomaticHistogram, h2::AutomaticHistogram, dist::Symbol=:iae; p::Real=1.0)\n\nCompute a statistical distance between two histogram probability densities.\n\nArguments\n\nh1, h2: The two histograms for which the distance should be computed\ndist: The name of the distance to compute. Valid options are :iae (default), :ise, :hellinger, :sup, :kl, :lp. For the L_p-metric, a given power p can be specified as a keyword argument. \n\nKeyword arguments\n\np: Power of the L_p-metric, which should be a number in the interval 1 infty. Ignored if dist != :lp. Defaults to p=1.0.\n\n\n\n\n\n","category":"function"},{"location":"api/#Index","page":"API","title":"Index","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Pages = [\"api.md\"]","category":"page"},{"location":"examples/plotting/#Plotting-tutorial","page":"Plotting tutorial","title":"Plotting tutorial","text":"","category":"section"},{"location":"examples/plotting/","page":"Plotting tutorial","title":"Plotting tutorial","text":"The following markdown page gives a brief, example-driven introduction to plotting histograms with AutoHist.jl. All the plotting functionality is provided through extensions for Makie.jl and Plots.jl. Both extensions provide recipes for plotting AutomaticHistogram objects, and additionally lets the user utilize the automatic bin selection rules from AutoHist when calling the respective built-in histogram plotting functions from the two libraries.","category":"page"},{"location":"examples/plotting/#Plots.jl","page":"Plotting tutorial","title":"Plots.jl","text":"","category":"section"},{"location":"examples/plotting/","page":"Plotting tutorial","title":"Plotting tutorial","text":"In order to use the AutoHist.jl plotting functionality with Plots.jl, we will need to load both libraries first.","category":"page"},{"location":"examples/plotting/","page":"Plotting tutorial","title":"Plotting tutorial","text":"using AutoHist; import Plots","category":"page"},{"location":"examples/plotting/","page":"Plotting tutorial","title":"Plotting tutorial","text":"We start by showing how to plot the result from calling fit or autohist. The Plots extension provides a type recipe for objects of the AutomaticHistogram type, so that Plots.plot(h) will draw the histogram h, as illustrated in the code snippet below.","category":"page"},{"location":"examples/plotting/","page":"Plotting tutorial","title":"Plotting tutorial","text":"import Random\nx = Random.randexp(Random.Xoshiro(1812), 10^5)\n\nh1 = autohist(x, AIC())\nh2 = autohist(x, BIC())\n\n# Set axis limits\nxlims = [-0.5, 6.0]\nylims = [-0.05, 1.0]\n\n# Plot a filled histogram\np1 = Plots.plot(h1, color=:red, alpha=0.4, label=\"\", title=\"AIC\", xlims=xlims, ylims=ylims)\n\n# Plot a stephist\np2 = Plots.stephist(h2, color=:green, label=\"\", title=\"BIC\", xlims=xlims, ylims=ylims)\n\n# Display the two plots side-by-side\nPlots.plot(p1, p2, layout=(1, 2), size=(670, 320))","category":"page"},{"location":"examples/plotting/","page":"Plotting tutorial","title":"Plotting tutorial","text":"As illustrated here, the call to Plots.plot also accepts additional keyword arguments which allow us to customize the look of the histogram. AutomaticHistogram objects are compatible with most histogram-like seriestypes, so the above call to Plots.plot is equivalent to calling Plots.histogram with the same input. By default, the plotted histogram is a filled bar chart. If one desires, a non-filled histogram can be drawn by using the stephist seriestype, as we did above.","category":"page"},{"location":"examples/plotting/","page":"Plotting tutorial","title":"Plotting tutorial","text":"note: Note\nSome of the arguments normally supported by the histogram and stephist seriestypes are deprecated when used together with AutomaticHistogram. In particular, the weights keyword is not supported, as autohist currently only handles equally weighted samples. The bins keyword is ignored, as the histogram partition has already been chosen by autohist. Additionally, the normalize kwarg is deprecated, so that e.g. Plots.histogram(h, normalize=norm) will always draw a histogram on the density scale no matter the value of norm.","category":"page"},{"location":"examples/plotting/","page":"Plotting tutorial","title":"Plotting tutorial","text":"Automatic histograms can also be plotted by accessing Plots.jl functions directly. The general syntax for fitting a histogram in this manner is Plots.plot(x, rule), where rule is any of the bin selection rules provided by the AutoHist.jl package, see the methods page. Note that the above syntax supports any histogram-like seriestype, so the above inline code snippet is equivalent to Plots.histogram(x, rule). When using Plots.jl functions directly, one can pass additional keyword arguments directly to the plotting function. In the code snippet below, we show how to directly plot a right-closed histogram with support constrained to be positive.","category":"page"},{"location":"examples/plotting/","page":"Plotting tutorial","title":"Plotting tutorial","text":"# Plot a filled histogram\np3 = Plots.plot(x, BayesBlocks(), color=:black, alpha=0.4, label=\"\",\n                title=\"BayesBlocks\", xlims=xlims, ylims=ylims)\n\n# Plot a stephist, supported on [0, maximum(x)] with left-closed intervals\np4 = Plots.stephist(x, RRH(), support=(0.0, Inf), closed=:left, color=:blue,\n                    label=\"\", title=\"RRH\", xlims=xlims, ylims=ylims)\n\n# Display the two plots\nPlots.plot(p3, p4, layout=(1, 2), size=(670, 320))","category":"page"},{"location":"examples/plotting/#Makie.jl","page":"Plotting tutorial","title":"Makie.jl","text":"","category":"section"},{"location":"examples/plotting/","page":"Plotting tutorial","title":"Plotting tutorial","text":"To showcase the Makie.jl extension, we start by loading the required packages:","category":"page"},{"location":"examples/plotting/","page":"Plotting tutorial","title":"Plotting tutorial","text":"using AutoHist; import Makie, CairoMakie;","category":"page"},{"location":"examples/plotting/","page":"Plotting tutorial","title":"Plotting tutorial","text":"Quickly plotting a pre-fitted AutomaticHistogram object can be done easily via Makie.plot(h). AutomaticHistogram objects also supports other histogram-like recipes such as hist, barplot and stephist, so e.g. Makie.hist(h) can also be used to draw a fitted histogram. Below, we show a slightly more involved example where we plot two histograms fitted using different rules side-by-side, using a stephist for the last panel.","category":"page"},{"location":"examples/plotting/","page":"Plotting tutorial","title":"Plotting tutorial","text":"import Random\nx = Random.randexp(Random.Xoshiro(1812), 10^5)\n\nh1 = autohist(x, AIC())\nh2 = autohist(x, BIC())\n\n# Specify axis lims, ticks\nlimits = ((-0.5, 6.0), (-0.05, 1.0))\nyticks = 0.0:0.25:1.0\n\n# Make a figure with a 2x1 Layout\nfig = Makie.Figure(size=(670, 320))\nax1 = Makie.Axis(fig[1, 1], title=\"AIC\", xlabel=\"x\",\n                 ylabel=\"Density\", limits=limits, yticks = yticks)\nax2 = Makie.Axis(fig[1, 2], title=\"BIC\", xlabel=\"x\", limits=limits, yticks=yticks)\n\n# Draw a filled AIC histogram in the left panel\nMakie.plot!(ax1, h1, alpha=0.4, color=\"red\")\n\n# Draw a BIC stephist in the right panel\nMakie.stephist!(ax2, h2, color=\"green\")\n\nfig # Display figure","category":"page"},{"location":"examples/plotting/","page":"Plotting tutorial","title":"Plotting tutorial","text":"As shown in the above example, we can pass additional plot attributes to the Makie plotting functions in order to customize our histogram plots. We refer the interested reader to the Makie documentation for further details on the supported attributes.","category":"page"},{"location":"examples/plotting/","page":"Plotting tutorial","title":"Plotting tutorial","text":"note: Note\nSome of the arguments normally supported by the barplot, hist and stephist seriestypes are deprecated when used together with AutomaticHistogram. In particular, the weights keyword is not supported, as autohist currently only handles equally weigthed samples. The bins keyword is ignored, as the histogram partition has already been chosen by autohist. Additionally, the normalization kwarg is deprecated, so that e.g. Makie.hist(h, normalization=norm) will always draw a histogram on the density scale no matter the value of norm.","category":"page"},{"location":"examples/plotting/","page":"Plotting tutorial","title":"Plotting tutorial","text":"We can also plot automatically selected histograms directly without needing to construct an AutomaticHistogram object directly first. This is achieved by calling e.g. Makie.plot(x, rule), where rule is any of the bin selection rules provided as part of AutoHist.jl, see the methods page. Alternatively, we can use another histogram-like series type like stephist instead. In the following code snippet, we illustrate the use of these methods on the simulated exponential dataset considered previously:","category":"page"},{"location":"examples/plotting/","page":"Plotting tutorial","title":"Plotting tutorial","text":"# Make a figure with a 2x1 Layout\nfig = Makie.Figure(size=(670, 320))\nax1 = Makie.Axis(fig[1, 1], title=\"BayesBlocks\", xlabel=\"x\",\n                 ylabel=\"Density\", limits=limits, yticks=yticks)\nax2 = Makie.Axis(fig[1, 2], title=\"RRH\", xlabel=\"x\", limits=limits, yticks=yticks)\n\n# Draw a filled BayesBlocks histogram in the left panel\nMakie.plot!(ax1, x, BayesBlocks(), alpha=0.4, color=\"black\")\n\n# Draw a RRH stephist in the right panel\nMakie.stephist!(ax2, x, RRH(), color=\"blue\")\n\nfig # Display figure","category":"page"},{"location":"examples/plotting/","page":"Plotting tutorial","title":"Plotting tutorial","text":"note: Note\nCurrently, the Makie.jl plotting functions do not support passing additional keyword arguments to fit/autohist. In order to draw an automatic histogram using Makie with non-default values for the support and closed kwargs, it is therefore necessary to manually fit an AutomaticHistogram object first, and then plot this using the syntax outlined previously.","category":"page"},{"location":"examples/algorithm_choice/#Choice-of-algorithm","page":"Choice of algorithm","title":"Choice of algorithm","text":"","category":"section"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"In this section, we empirically assess the efficiency of the dynamic programming algorithm provided for irregular histograms, and show how heuristics can be used to speed up the computations with a few benchmarks.[1]","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"[1]: Note: The benchmarks presented here were performed on a Windows machine with an Intel® Core™ Ultra 5 125U CPU. Results may vary on systems with different hardware configurations.","category":"page"},{"location":"examples/algorithm_choice/#The-cubic-time-dynamic-programming-algorithm","page":"Choice of algorithm","title":"The cubic-time dynamic programming algorithm","text":"","category":"section"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"As a toy problem, we consider standard normal random samples of using a data-based grid. In this case, the number of candidate cutpoints are k_n+1 = n+1 (including the edges), where n is the sample size. For smaller samples, we can just compute the exact solution using the default dynamic programming algorithm, available as SegNeig. The code snippet below illustrates how this algorithm can be explicitly specified when calling fit:","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"using AutoHist, Distributions, BenchmarkTools\nn = 500\n@benchmark fit(\n    AutomaticHistogram, \n    $rand(Normal(), n),\n    RIH(\n        grid = :data,\n        alg = SegNeig(greedy=false)\n    )\n)\n\n# Output\nBenchmarkTools.Trial: 55 samples with 1 evaluation per sample.\n Range (min … max):  51.734 ms … 235.103 ms  ┊ GC (min … max): 10.63% … 76.02%\n Time  (median):     57.087 ms               ┊ GC (median):    10.08%\n Time  (mean ± σ):   91.326 ms ±  66.481 ms  ┊ GC (mean ± σ):  43.70% ± 26.34%","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"Benchmarking the above code snippet yields a median runtime of around 57 textms on my machine. Since dynamic programming is quick for samples of this size, the greedy algorithm will only be used if the number of candidate cutpoints exceeds 501. Thus, changing greedy to true in the above code snippet would produce the same histogram, as there are 501 possible cutpoints.","category":"page"},{"location":"examples/algorithm_choice/#Speeding-up-computations-via-heuristics","page":"Choice of algorithm","title":"Speeding up computations via heuristics","text":"","category":"section"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"The mathcalO(k_n^3) runtime of dynamic programming means that computing the optimal solution quickly becomes computationally prohibitive, even for moderate samples. As an example, when doubling the number of samples in the above code snippet to n = 1000, the median runtime increases to 442 textms, a roughly 8-fold increase. To ensure that the code retains good performance even for larger samples, we have implemented a greedy search heuristic which selects a subset of the candidate cutpoints, and the dynamic programming algorithm is subsequently run on this smaller set. Adopting the heuristic approach can improve performance considerably, but comes at the cost of no longer being guaranteed to find the optimal solution. To showcase the computational advantages of the heuristic approach, we run a benchmark on a normal sample of size n = 10^6.","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"n = 10^6\n@benchmark fit(\n    AutomaticHistogram, \n    $rand(Normal(), n),\n    RIH(\n        grid = :data,\n        alg = SegNeig(greedy=true) # NB! greedy=true is the default option\n    )\n)\n\n# Output\nBenchmarkTools.Trial: 7 samples with 1 evaluation per sample.\n Range (min … max):  683.418 ms … 826.615 ms  ┊ GC (min … max):  1.71% … 19.55%\n Time  (median):     777.266 ms               ┊ GC (median):    11.38%\n Time  (mean ± σ):   765.973 ms ±  48.456 ms  ┊ GC (mean ± σ):  10.04% ±  6.39%","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"As we can see, the median runtime is less than 2 times slower than the mean time it took to compute the exact solution for random samples of size n = 10^3.","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"The number of candidate cutpoints constructed by the greedy search heuristic can be controlled through the gr_maxbins keyword argument, which equals the number of selected gridpoints plus one. By default, the greedy algorithm will produce a subset consisting of max500 n^13+1 cutpoints (including the edges). For gr_maxbins1 < gr_maxbins2, the cutpoint subset formed by the greedy algorithm for gr_maxbins1 is a subset of that selected with gr_maxbins2 bins. Thus, increasing the number of candidate cutpoints added to this grid will never lead to a worse solution of the original optimization problem. If additional precision is desired in the above example, we can increase gr_maxbins to 1000:","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"n = 10^6\n@benchmark fit(\n    AutomaticHistogram, \n    $rand(Normal(), n),\n    RIH(\n        grid = :data,\n        alg = SegNeig(greedy=true, gr_maxbins=10^3)\n    )\n)\n\n# Output\nBenchmarkTools.Trial: 5 samples with 1 evaluation per sample.\n Range (min … max):  1.114 s …   1.339 s  ┊ GC (min … max):  5.36% … 17.15%\n Time  (median):     1.270 s              ┊ GC (median):    15.61%\n Time  (mean ± σ):   1.232 s ± 99.117 ms  ┊ GC (mean ± σ):  12.38% ±  6.19%","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"The above code snippet has a median runtime of about 12 texts on my machine.","category":"page"},{"location":"examples/algorithm_choice/#The-quadratic-time-dynamic-programming-algorithm","page":"Choice of algorithm","title":"The quadratic-time dynamic programming algorithm","text":"","category":"section"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"For the L2CV_I, KLCV_I and BayesBlocks criteria, it becomes possible to compute the exact solution via the quadratic-time dynamic programming algorithm OptPart instead of the cubic-time SegNeig algorithm used for the other problems. In practice, this means that computing the exact solution is often feasible even as the number of candidate cutpoints becomes quite large. Both the OptPart and SegNeig algorithms can be used to fit L2CV_I, allowing us to get a direct comparison between the performance of the two algorithms.","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"n = 10^3\n@benchmark fit(\n    AutomaticHistogram, \n    $rand(Normal(), n),\n    L2CV_I(\n        grid = :data,\n        alg = OptPart(greedy=false)\n    )\n)\n\n@benchmark fit(\n    AutomaticHistogram, \n    $rand(Normal(), n),\n    L2CV_I(\n        grid = :data,\n        alg = SegNeig(greedy=false)\n    )\n)\n\n# Output OptPart\nBenchmarkTools.Trial: 1014 samples with 1 evaluation per sample.\n Range (min … max):  3.673 ms … 10.196 ms  ┊ GC (min … max):  0.00% … 33.51%\n Time  (median):     4.437 ms              ┊ GC (median):     0.00%\n Time  (mean ± σ):   4.921 ms ±  1.099 ms  ┊ GC (mean ± σ):  12.30% ± 15.90%\n\n# Output SegNeig\nBenchmarkTools.Trial: 11 samples with 1 evaluation per sample.\n Range (min … max):  408.000 ms … 570.588 ms  ┊ GC (min … max): 11.73% … 35.49%\n Time  (median):     421.996 ms               ┊ GC (median):    11.52%\n Time  (mean ± σ):   471.118 ms ±  68.395 ms  ┊ GC (mean ± σ):  22.00% ± 10.88%","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"The mean runtime is around 100 times faster for the quadratic-time algorithm! Although the speedup from the quadratic-time algorithm is considerable in this case, it is often too slow to be used in practice for larger sample sizes. To speed up the computation for these criteria, it is once again possible to use the greedy search heuristic used in the cubic-time case. Due to the superior runtime complexity of the exact algorithm for cross-validation criteria, the exact solution is by default computed when the number of total candidate cutpoints is less than 3001, and the greedy search heuristic is used thereafter to build a smaller candidate set as previously. By default gr_maxbins is set to max3000 sqrtn, but this can be replaced with a user-defined value if better performance or additional accuracy is desired.","category":"page"},{"location":"examples/algorithm_choice/","page":"Choice of algorithm","title":"Choice of algorithm","text":"note: Note\nIn practice one should of course never use the SegNeig algorithm for the L2CV_I, KLCV_I and BayesBlocks criteria, as it only leads to a worse speed/accuracy tradeoff compared to using OptPart, which is the default.","category":"page"},{"location":"methods/#Supported-Methods","page":"Supported Methods","title":"Supported Methods","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"This page provides background on each histogram method supported through the rule argument. Our presentation is intended to be rather brief, and we therefore do not cover the theoretical underpinnings of each method in great detail. For some further background on automatic histogram procedures and the theory behind them, we recommend the excellent reviews contained in the articles of Birgé and Rozenholc (2006) and Davies et al. (2009).","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"For ease of exposition, we present all methods covered here in the context of estimating the density of a sample boldsymbolx = (x_1 x_2 ldots x_n) on the unit interval, but note that extending the procedures presented here to other compact intervals is possible through a suitable affine transformation. In particular, if a density estimate with support ab is desired, we can scale the data to the unit interval through z_i = (x_i - a)(b-a), and apply the methods on this transformed sample and rescale the resulting density estimate to ab. In cases where the support of the density is unknown, a natural choice is a = x_(1) and b = x_(n). Cases where only the lower or upper bound is known can be handled similarly. The transformation used to construct the histogram can be controlled through the support keyword, where the default argument support=(-Inf, Inf) uses the order statistics-based approach described above.","category":"page"},{"location":"methods/#Notation","page":"Supported Methods","title":"Notation","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Before we describe the methods included here in more detail, we introduce some notation. We let mathcalI = (mathcalI_1 mathcalI_2 ldots mathcalI_k) denote a partition of 01 into k intervals and write mathcalI_j for the length of interval mathcalI_j. The intervals in the partition mathcalI can be either right- or left-closed. Whether a left- or right-closed partition is used to draw the histogram is controlled by the keyword argument closed, with options :left and :right (default). This choice is somewhat arbitrary, but is unlikely to matter much in practical applications.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Based on a partition mathcalI, we can write down the corresponding histogram density estimate by","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"widehatf(x) = sum_j=1^k fracwidehattheta_jmathcalI_jmathbf1_mathcalI_j(x) quad xin 01","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"where mathbf1_mathcalI_j is the indicator function, widehattheta_j geq 0 for all j and sum_j=1^k widehattheta_j = 1. ","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"For most of the methods considered here, the estimated bin probabilities are the maximum likelihood estimates widehattheta_j = N_jn, where N_j = sum_i=1^n mathbb1_mathcalI_j(x_i) is the number of observations landing in interval mathcalI_j . The exception to this rule are the two Bayesian approaches RIH and RRH, which uses the Bayes estimator widehattheta_j = (a_j + N_j)(a+n) for (a_1 ldots a_k) in (0infty)^k and a = sum_j=1^k a_j instead.","category":"page"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The goal of an automatic histogram procedure is to find a partition mathcalI based on the sample alone which produces a reasonable density estimate. Regular histogram procedures only consider regular partitions, where all intervals in the partition are of equal length, so that one only needs to determine the number k of bins. Irregular histograms allow for partitions with intervals of unequal length, and try to determine both the number of bins and the locations of the cutpoints between the intervals.","category":"page"},{"location":"methods/#A-short-note-on-using-different-rules","page":"Supported Methods","title":"A short note on using different rules","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"In order to fit a histogram using a specific rule, we call fit(AutomaticHistogram, x, rule), where x is the data vector. For many of the rules discussed below, the user can specify additional rule-specific keywords to rule, providing additional control over the supplied method when desired. We also provide a set of default values for these parameters, so that the user may for instance call fit(AutomaticHistogram, x, AIC()) to fit a regular histogram using the AIC criterion without having to worry about explicitly passing any keyword arguments.","category":"page"},{"location":"methods/#Irregular-histograms","page":"Supported Methods","title":"Irregular histograms","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The following section provides a description of all the irregular histogram rules that have been implemented in AutoHist.jl. In each case, the best partition is selected among the subset of interval partitions of the unit interval that have cut points belonging to a discrete set of cardinality k_n-1. In all the irregular procedures covered here, we attempt to find best partition according to a goodness-of-fit criterion among all partitions with endpoints belonging to a given discrete mesh tau_jcolon 0leq j leq k_n.","category":"page"},{"location":"methods/#Random-irregular-histogram","page":"Supported Methods","title":"Random irregular histogram","text":"","category":"section"},{"location":"methods/#AutoHist.RIH","page":"Supported Methods","title":"AutoHist.RIH","text":"RIH(;\n    a::Real                    = 5.0,\n    logprior::Function         = k -> 0.0,\n    grid::Symbol               = :regular,\n    maxbins::Union{Int,Symbol} = :default,\n    alg::AbstractAlgorithm     = SegNeig()\n)\n\nThe random irregular histogram criterion.\n\nConsists of maximizing the marginal log-posterior of the partition mathcalI = (mathcalI_1 ldots mathcalI_k),\n\n    sum_j=1^k biglog Gamma(a_j + N_j) - log Gamma(a_j) - N_jlogmathcalI_jbig + log p_n(k) - log binomk_n-1k-1\n\nHere p_n(k) is the prior distribution on the number k of bins, which can be controlled by supplying a function to the logprior keyword argument.  The default value is p_n(k) propto 1. Here, a_j = ak, for a scalar a  0, not depending on k.\n\nKeyword arguments\n\na: Specifies Dirichlet concentration parameter in the Bayesian histogram model. Must be a fixed positive number, and defaults to a=5.0.\nlogprior: Unnormalized logprior distribution on the number k of bins. Defaults to a uniform prior, e.g. logprior(k) = 0 for all k.\ngrid: Symbol indicating how the finest possible mesh should be constructed. Options are :data, which uses each unique data point as a grid point, :regular (default) which constructs a fine regular grid, and :quantile which constructs the grid based on the sample quantiles.\nmaxbins: Maximal number of bins for which the above criterion is evaluated. Defaults to maxbins=:default, which sets maxbins to the ceil of min(1000, 4n/log(n)^2) if grid is regular or quantile. Ignored if grid=:data.\nalg: Algorithm used to fit the model. Currently, only SegNeig is supported for this rule.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> rule = RIH(a = 5.0, logprior = k-> -log(k), grid = :data);\n\njulia> fit(AutomaticHistogram, x, rule)\nAutomaticHistogram{Vector{Float64}, Vector{Float64}, Vector{Int64}}\nbreaks: [0.0, 0.18220071105959446, 0.3587941358096334, 0.8722292888743843, 1.0]\ndensity: [0.11858322346056327, 0.6490600487586273, 1.6066011289666577, 0.30436411114439915]\ncounts: [10, 57, 414, 19]\ntype: irregular\nclosed: right\na: 5.0\n\nReferences\n\nThis approach to irregular histograms first appeared in Simensen et al. (2025).\n\n\n\n\n\n","category":"type"},{"location":"methods/#Rozenholc,-Mildenberger-and-Gather-penalty-A","page":"Supported Methods","title":"Rozenholc, Mildenberger & Gather penalty A","text":"","category":"section"},{"location":"methods/#AutoHist.RMG_penA","page":"Supported Methods","title":"AutoHist.RMG_penA","text":"RMG_penA(;\n    grid::Symbol               = :regular,\n    maxbins::Union{Int,Symbol} = :default,\n    alg::AbstractAlgorithm     = SegNeig()\n)\n\nConsists of finding the partition mathcalI that maximizes a penalized log-likelihood,\n\n    sum_j=1^k N_j log (N_jmathcalI_j) - log binomk_n-1k-1 - k - 2log(k) - sqrt2(k-1)Biglog binomk_n-1k-1+ 2log(k)Big\n\nKeyword arguments\n\ngrid: Symbol indicating how the finest possible mesh should be constructed. Options are :data, which uses each unique data point as a grid point, :regular (default) which constructs a fine regular grid, and :quantile which constructs the grid based on the sample quantiles.\nmaxbins: Maximal number of bins for which the above criterion is evaluated. Defaults to maxbins=:default, which sets maxbins to the ceil of min(1000, 4n/log(n)^2) if grid is regular or quantile. Ignored if grid=:data.\nalg: Algorithm used to fit the model. Currently, only SegNeig is supported for this rule.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> rule = RMG_penA(grid = :data);\n\njulia> fit(AutomaticHistogram, x, rule)\nAutomaticHistogram{Vector{Float64}, Vector{Float64}, Vector{Int64}}\nbreaks: [0.0, 0.18875598171056715, 0.3644223879547405, 0.8696799410193466, 1.0]\ndensity: [0.116552597701164, 0.6717277510418354, 1.6229346697072502, 0.30693663211078037]\ncounts: [11, 59, 410, 20]\ntype: irregular\nclosed: right\na: NaN\n\nReferences\n\nThis approach was suggested by Rozenholc et al. (2010).\n\n\n\n\n\n","category":"type"},{"location":"methods/#Rozenholc,-Mildenberger-and-Gather-penalty-B","page":"Supported Methods","title":"Rozenholc, Mildenberger & Gather penalty B","text":"","category":"section"},{"location":"methods/#AutoHist.RMG_penB","page":"Supported Methods","title":"AutoHist.RMG_penB","text":"RMG_penB(;\n    grid::Symbol               = :regular,\n    maxbins::Union{Int,Symbol} = :default,\n    alg::AbstractAlgorithm     = SegNeig()\n)\n\nConsists of finding the partition mathcalI that maximizes a penalized log-likelihood,\n\n    sum_j=1^k N_j log (N_jmathcalI_j) - log binomk_n-1k-1 - k - log^25(k)\n\nKeyword arguments\n\ngrid: Symbol indicating how the finest possible mesh should be constructed. Options are :data, which uses each unique data point as a grid point, :regular (default) which constructs a fine regular grid, and :quantile which constructs the grid based on the sample quantiles.\nmaxbins: Maximal number of bins for which the above criterion is evaluated. Defaults to maxbins=:default, which sets maxbins to the ceil of min(1000, 4n/log(n)^2) if grid is regular or quantile. Ignored if grid=:data.\nalg: Algorithm used to fit the model. Currently, only SegNeig is supported for this rule.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> rule = RMG_penB(grid = :data);\n\njulia> fit(AutomaticHistogram, x, rule)\nAutomaticHistogram{Vector{Float64}, Vector{Float64}, Vector{Int64}}\nbreaks: [0.0, 0.1948931612779725, 0.375258352661302, 0.8268306249022703, 0.9222490305512866, 1.0]\ndensity: [0.12314439276691318, 0.7096713008662634, 1.6962954704872724, 0.7545714006671028, 0.12861575966067232]\ncounts: [12, 64, 383, 36, 5]\ntype: irregular\nclosed: right\na: NaN\n\nReferences\n\nThis approach was suggested by Rozenholc et al. (2010).\n\n\n\n\n\n","category":"type"},{"location":"methods/#Rozenholc,-Mildenberger-and-Gather-penalty-R","page":"Supported Methods","title":"Rozenholc, Mildenberger & Gather penalty R","text":"","category":"section"},{"location":"methods/#AutoHist.RMG_penR","page":"Supported Methods","title":"AutoHist.RMG_penR","text":"RMG_penR(;\n    grid::Symbol               = :regular,\n    maxbins::Union{Int,Symbol} = :default,\n    alg::AbstractAlgorithm     = SegNeig()\n)\n\nConsists of finding the partition mathcalI that maximizes a penalized log-likelihood,\n\n    sum_j=1^k bigN_j log (N_jmathcalI_j) - fracN_j2nbig - log binomk_n-1k-1 - log^25(k)\n\nKeyword arguments\n\ngrid: Symbol indicating how the finest possible mesh should be constructed. Options are :data, which uses each unique data point as a grid point, :regular (default) which constructs a fine regular grid, and :quantile which constructs the grid based on the sample quantiles.\nmaxbins: Maximal number of bins for which the above criterion is evaluated. Defaults to maxbins=:default, which sets maxbins to the ceil of min(1000, 4n/log(n)^2) if grid is regular or quantile. Ignored if grid=:data.\nalg: Algorithm used to fit the model. Currently, only SegNeig is supported for this rule.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> rule = RMG_penR(grid = :data);\n\njulia> fit(AutomaticHistogram, x, rule)\nAutomaticHistogram{Vector{Float64}, Vector{Float64}, Vector{Int64}}\nbreaks: [0.0, 0.18875598171056715, 0.3699070396003733, 0.8285645195146814, 0.9222490305512866, 1.0]\ndensity: [0.116552597701164, 0.6845115973621804, 1.6875338000474953, 0.7471886144834441, 0.12861575966067232]\ncounts: [11, 62, 387, 35, 5]\ntype: irregular\nclosed: right\na: NaN\n\nReferences\n\nThis approach was suggested by Rozenholc et al. (2010).\n\n\n\n\n\n","category":"type"},{"location":"methods/#Irregular-L_2-leave-one-out-cross-validation-(L2CV_I)","page":"Supported Methods","title":"Irregular L_2 leave-one-out cross-validation (L2CV_I)","text":"","category":"section"},{"location":"methods/#AutoHist.L2CV_I","page":"Supported Methods","title":"AutoHist.L2CV_I","text":"L2CV_I(;\n    grid::Symbol               = :regular,\n    maxbins::Union{Int,Symbol} = :default,\n    alg::AbstractAlgorithm     = OptPart(),\n    use_min_length::Bool       = false\n)\n\nConsists of finding the partition mathcalI that maximizes a L2 leave-one-out cross-validation criterion,\n\n    fracn+1nsum_j=1^k fracN_j^2mathcalI_j - 2sum_j=1^k fracN_jmathcalI_j\n\nKeyword arguments\n\ngrid: Symbol indicating how the finest possible mesh should be constructed. Options are :data, which uses each unique data point as a grid point, :regular (default) which constructs a fine regular grid, and :quantile which constructs the grid based on the sample quantiles.\nmaxbins: Maximal number of bins for which the above criterion is evaluated. Defaults to maxbins=:default, which sets maxbins to the ceil of min(1000, 4n/log(n)^2) if grid is regular or quantile. Ignored if grid=:data.\nalg: Algorithm used to fit the model. Currently, OptPart and SegNeig are supported for this rule, with the former algorithm being the default.\nuse_min_length: Boolean indicating whether or not to impose a restriction on the minimum bin length of the histogram. If set to true, the smallest allowed bin length is set to (maximum(x)-minimum(x))/n*log(n)^(1.5).\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> rule = L2CV_I(grid = :data, use_min_length=true);\n\njulia> fit(AutomaticHistogram, x, rule)\nAutomaticHistogram{Vector{Float64}, Vector{Float64}, Vector{Int64}}\nbreaks: [0.0, 0.149647045210915, 0.2499005080461325, 0.3490626376697454, 0.4600140220788484, 0.7765683248449301, 0.8535131937737716, 0.9121099383916996, 0.9560732934980348, 1.0]\ndensity: [0.08018868653963065, 0.3590898407087615, 0.7664216197097746, 1.2798398213438569, 1.8448651460332468, 1.2476465466304287, 0.6826317786220794, 0.2729545998246794, 0.045530388214070294]\ncounts: [6, 18, 38, 71, 292, 48, 20, 6, 1]\ntype: irregular\nclosed: right\na: NaN\n\nReferences\n\nThis approach dates back to Rudemo (1982).\n\n\n\n\n\n","category":"type"},{"location":"methods/#Irregular-Kullback-Leibler-leave-one-out-cross-validation-(KLCV_I)","page":"Supported Methods","title":"Irregular Kullback-Leibler leave-one-out cross-validation (KLCV_I)","text":"","category":"section"},{"location":"methods/#AutoHist.KLCV_I","page":"Supported Methods","title":"AutoHist.KLCV_I","text":"KLCV_I(;\n    grid::Symbol               = :regular,\n    maxbins::Union{Int,Symbol} = :default,\n    alg::AbstractAlgorithm     = OptPart(),\n    use_min_length::Bool       = false\n)\n\nConsists of finding the partition mathcalI that maximizes a Kullback-Leibler leave-one-out cross-validation criterion,\n\n    sum_j=1^k N_jlog(N_j-1) - sum_j=1^k N_jlog mathcalI_j\n\nwhere the maximmization is over all partitions with N_j geq 2 for all j.\n\nKeyword arguments\n\ngrid: Symbol indicating how the finest possible mesh should be constructed. Options are :data, which uses each unique data point as a grid point, :regular (default) which constructs a fine regular grid, and :quantile which constructs the grid based on the sample quantiles.\nmaxbins: Maximal number of bins for which the above criterion is evaluated. Defaults to maxbins=:default, which sets maxbins to the ceil of min(1000, 4n/log(n)^2) if grid is regular or quantile. Ignored if grid=:data.\nalg: Algorithm used to fit the model. Currently, OptPart and SegNeig are supported for this rule, with the former algorithm being the default.\nuse_min_length: Boolean indicating whether or not to impose a restriction on the minimum bin length of the histogram. If set to true, the smallest allowed bin length is set to (maximum(x)-minimum(x))/n*log(n)^(1.5).\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> rule = KLCV_I(grid = :data, use_min_length=true);\n\njulia> fit(AutomaticHistogram, x, rule)\nAutomaticHistogram{Vector{Float64}, Vector{Float64}, Vector{Int64}}\nbreaks: [0.0, 0.13888886265725095, 0.23836051747480758, 0.33883651300547, 0.45084951551151237, 0.7900230337213711, 0.8722292888743843, 0.9352920770792058, 1.0]\ndensity: [0.07200001359848368, 0.321699684786505, 0.7165890680628054, 1.2319998295961743, 1.8220762141507212, 1.1191362485586687, 0.5074307830485909, 0.09272434856770642]\ncounts: [5, 16, 36, 69, 309, 46, 16, 3]\ntype: irregular\nclosed: right\na: NaN\n\nReferences\n\nThis approach to irregular histograms was, to the best of our knowledge, first considered in Simensen et al. (2025).\n\n\n\n\n\n","category":"type"},{"location":"methods/#Normalized-maximum-likelihood,-regular-(NML_R)","page":"Supported Methods","title":"Normalized maximum likelihood, regular (NML_R)","text":"","category":"section"},{"location":"methods/#AutoHist.NML_I","page":"Supported Methods","title":"AutoHist.NML_I","text":"NML_I(;\n    grid::Symbol               = :regular,\n    maxbins::Union{Int,Symbol} = :default,\n    alg::AbstractAlgorithm     = DP()\n)\n\nA quick-to-evalutate version of the normalized maximum likelihood criterion.\n\nConsists of finding the partition mathcalI that maximizes a penalized log-likelihood,\n\nbeginaligned\n    sum_j=1^k N_jlog fracN_jmathcalI_j - frack-12log(n2) - logfracsqrtpiGamma(k2) - n^-12fracsqrt2kGamma(k2)3Gamma(k2-12) \n    - n^-1left(frac3+k(k-2)(2k+1)36 - fracGamma(k2)^2 k^29Gamma(k2-12)^2 right)  - log binomk_n-1k-1\nendaligned\n\nKeyword arguments\n\ngrid: Symbol indicating how the finest possible mesh should be constructed. Options are :data, which uses each unique data point as a grid point, :regular (default) which constructs a fine regular grid, and :quantile which constructs the grid based on the sample quantiles.\nmaxbins: Maximal number of bins for which the above criterion is evaluated. Defaults to maxbins=:default, which sets maxbins to the ceil of min(1000, 4n/log(n)^2) if grid is regular or quantile. Ignored if grid=:data.\nalg: Algorithm used to fit the model. Currently, only SegNeig is supported for this rule.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> rule = NML_I(grid = :data);\n\njulia> fit(AutomaticHistogram, x, rule)\nAutomaticHistogram{Vector{Float64}, Vector{Float64}, Vector{Int64}}\nbreaks: [0.0, 0.18875598171056715, 0.3644223879547405, 0.8696799410193466, 1.0]\ndensity: [0.116552597701164, 0.6717277510418354, 1.6229346697072502, 0.30693663211078037]\ncounts: [11, 59, 410, 20]\ntype: irregular\nclosed: right\na: NaN\n\nReferences\n\nThis a variant of this criterion first suggested by Kontkanen and Myllymäki (2007).\n\n\n\n\n\n","category":"type"},{"location":"methods/#Bayesian-Blocks-(BayesBlocks)","page":"Supported Methods","title":"Bayesian Blocks (BayesBlocks)","text":"","category":"section"},{"location":"methods/#AutoHist.BayesBlocks","page":"Supported Methods","title":"AutoHist.BayesBlocks","text":"BayesBlocks(;\n    p0::Real                   = 0.05,\n    grid::Symbol               = :regular,\n    maxbins::Union{Int,Symbol} = :default,\n    alg::AbstractAlgorithm     = OptPart(),\n    use_min_length::Bool       = false\n)\n\nConsists of finding the partition mathcalI that maximizes a penalized Poisson likelihood criterion,\n\n    sum_j=1^k N_jlogbig(N_jmathcalI_jbig) + kbig(4 - log(7353 p_0 n^-0478)big)\n\nKeyword arguments\n\np0: Hyperparameter controlling the penalty for the number of bins. Must be a number in the open interval (01), with p0 = 0.05 serving as the default.\ngrid: Symbol indicating how the finest possible mesh should be constructed. Options are :data, which uses each unique data point as a grid point, :regular (default) which constructs a fine regular grid, and :quantile which constructs the grid based on the sample quantiles.\nmaxbins: Maximal number of bins for which the above criterion is evaluated. Defaults to maxbins=:default, which sets maxbins to the ceil of min(1000, 4n/log(n)^2) if grid is regular or quantile. Ignored if grid=:data.\nalg: Algorithm used to fit the model. Currently, OptPart and SegNeig are supported for this rule, with the former algorithm being the default.\nuse_min_length: Boolean indicating whether or not to impose a restriction on the minimum bin length of the histogram. If set to true, the smallest allowed bin length is set to (maximum(x)-minimum(x))/n*log(n)^(1.5).\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> rule = BayesBlocks(grid = :data, use_min_length=true);\n\njulia> fit(AutomaticHistogram, x, rule)\nAutomaticHistogram{Vector{Float64}, Vector{Float64}, Vector{Int64}}\nbreaks: [0.0, 0.1948931612779725, 0.375258352661302, 0.8268306249022703, 0.9222490305512866, 1.0]\ndensity: [0.12314439276691318, 0.7096713008662634, 1.6962954704872724, 0.7545714006671028, 0.12861575966067232]\ncounts: [12, 64, 383, 36, 5]\ntype: irregular\nclosed: right\na: NaN\n\nReferences\n\nThe Bayesian Blocks method was first introduced in Scargle et al. (2013).\n\n\n\n\n\n","category":"type"},{"location":"methods/#Regular-histograms","page":"Supported Methods","title":"Regular histograms","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"The following section details how each value of the rule argument selects the number k of bins to draw a regular histogram automatically based on a random sample. In the following, mathcalI = (mathcalI_1 mathcalI_2 ldots mathcalI_k) is the corresponding partition of 01 consisting of k equal-length bins. In cases where the value of the number of bins is computed by maximizing an expression, we look for the best regular partition among all regular partitions consisting of no more than k_n bins. For rules falling under this umbrella, k_n can be controlled through the maxbins keyword, as detailed below.","category":"page"},{"location":"methods/#Random-regular-histogram-(RRH),-Knuth","page":"Supported Methods","title":"Random regular histogram (RRH), Knuth","text":"","category":"section"},{"location":"methods/#AutoHist.RRH","page":"Supported Methods","title":"AutoHist.RRH","text":"RRH(;\n    a::Union{Real,Function}    = 5.0,\n    logprior::Function         = k->0.0,\n    maxbins::Union{Int,Symbol} = :default\n)\nKnuth(; maxbins::Union{Int,Symbol} = :default)\n\nThe random regular histogram criterion.\n\nThe number k of bins is chosen as the maximizer of the marginal log-posterior,\n\n   nlog (k) + sum_j=1^k biglog Gamma(a_j + N_j) - log Gamma(a_j)big + log p_n(k)\n\nHere p_n(k) is the prior distribution on the number k of bins, which can be controlled by supplying a function to the logprior keyword argument.  The default value is p_n(k) propto 1. Here, a_j = ak, for a scalar a  0, possibly depending on k. The value of a can be set by supplying a fixed, positive scalar or a function a(k) to the keyword argument a. The default value is a=5.0 for RRH().\n\nThe rule Knuth() is a special case of the RRH criterion, which corresponds to the particular choices a_j = 05 and p_n(k)propto 1.\n\nKeyword arguments\n\na: Specifies Dirichlet concentration parameter in the Bayesian histogram model. Can either be a fixed positive number or a function computing aₖ for different values of k. Defaults to 5.0 if not supplied.\nlogprior: Unnormalized logprior distribution on the number k of bins. Defaults to a uniform prior, e.g. logprior(k) = 0 for all k.\nmaxbins: Maximal number of bins for which the above criterion is evaluated. Defaults to maxbins=:default, which sets maxbins to the ceil of min(1000, 4n/log(n)^2).\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> rule = RRH(a = k->0.5*k, logprior = k->0.0);\n\njulia> h = fit(AutomaticHistogram, x, rule)\nAutomaticHistogram{LinRange{Float64, Int64}, Vector{Float64}, Vector{Int64}}\nbreaks: LinRange{Float64}(0.0, 1.0, 11)\ndensity: [0.04950495049504951, 0.2079207920792079, 0.5643564356435643, 1.0, 1.495049504950495, 1.8712871287128714, 1.9702970297029703, 1.6732673267326732, 0.9603960396039604, 0.2079207920792079]\ncounts: [2, 10, 28, 50, 75, 94, 99, 84, 48, 10]\ntype: regular\nclosed: right\na: 5.0\n\njulia> h == fit(AutomaticHistogram, x, Knuth())\ntrue\n\nReferences\n\nThe Knuth criterion for histograms was proposed by Knuth (2019). The random regular histogram criterion is a generalization.\n\n\n\n\n\n","category":"type"},{"location":"methods/#AIC","page":"Supported Methods","title":"AIC","text":"","category":"section"},{"location":"methods/#AutoHist.AIC","page":"Supported Methods","title":"AutoHist.AIC","text":"AIC(; maxbins::Union{Int,Symbol} = :default)\n\nAIC criterion for regular histograms.\n\nThe number k of bins is chosen as the maximizer of the penalized log-likelihood,\n\n    nlog (k) + sum_j=1^k N_j log (N_jn) - k\n\nwhere n is the sample size.\n\nKeyword arguments\n\nmaxbins: Maximal number of bins for which the above criterion is evaluated. Defaults to maxbins=:default, which sets maxbins to the ceil of min(1000, 4n/log(n)^2).\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> fit(AutomaticHistogram, x, AIC())\nAutomaticHistogram{LinRange{Float64, Int64}, Vector{Float64}, Vector{Int64}}\nbreaks: LinRange{Float64}(0.0, 1.0, 11)\ndensity: [0.04, 0.2, 0.56, 1.0, 1.5, 1.88, 1.98, 1.68, 0.96, 0.2]\ncounts: [2, 10, 28, 50, 75, 94, 99, 84, 48, 10]\ntype: regular\nclosed: right\na: NaN\n\nReferences\n\nThe aic criterion was proposed by Taylor (1987) for histograms.\n\n\n\n\n\n","category":"type"},{"location":"methods/#BIC","page":"Supported Methods","title":"BIC","text":"","category":"section"},{"location":"methods/#AutoHist.BIC","page":"Supported Methods","title":"AutoHist.BIC","text":"BIC(; maxbins::Union{Int,Symbol} = :default)\n\nBIC criterion for regular histograms.\n\nThe number k of bins is chosen as the maximizer of the penalized log-likelihood,\n\n    nlog (k) + sum_j=1^k N_j log (N_jn) - frack2log(n)\n\nwhere n is the sample size.\n\nKeyword arguments\n\nmaxbins: Maximal number of bins for which the above criterion is evaluated. Defaults to maxbins=:default, which sets maxbins to the ceil of min(1000, 4n/log(n)^2).\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> fit(AutomaticHistogram, x, BIC())\nAutomaticHistogram{LinRange{Float64, Int64}, Vector{Float64}, Vector{Int64}}\nbreaks: LinRange{Float64}(0.0, 1.0, 9)\ndensity: [0.048, 0.336, 0.816, 1.44, 1.904, 1.904, 1.264, 0.288]\ncounts: [3, 21, 51, 90, 119, 119, 79, 18]\ntype: regular\nclosed: right\na: NaN\n\n\n\n\n\n","category":"type"},{"location":"methods/#Birgé-Rozenholc-(BR)","page":"Supported Methods","title":"Birgé-Rozenholc (BR)","text":"","category":"section"},{"location":"methods/#AutoHist.BR","page":"Supported Methods","title":"AutoHist.BR","text":"BR(; maxbins::Union{Int,Symbol} = :default)\n\nBirgé-Rozenholc criterion for regular histograms.\n\nThe number k of bins is chosen as the maximizer of the penalized log-likelihood,\n\n    nlog (k) + sum_j=1^k N_j log (N_jn) - k - log^25 (k)\n\nwhere n is the sample size.\n\nKeyword arguments\n\nmaxbins: Maximal number of bins for which the above criterion is evaluated. Defaults to maxbins=:default, which sets maxbins to the ceil of min(1000, 4n/log(n)^2).\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> fit(AutomaticHistogram, x, BR())\nAutomaticHistogram{LinRange{Float64, Int64}, Vector{Float64}, Vector{Int64}}\nbreaks: LinRange{Float64}(0.0, 1.0, 11)\ndensity: [0.04, 0.2, 0.56, 1.0, 1.5, 1.88, 1.98, 1.68, 0.96, 0.2]\ncounts: [2, 10, 28, 50, 75, 94, 99, 84, 48, 10]\ntype: regular\nclosed: right\na: NaN\n\nReferences\n\nThis criterion was proposed by Birgé and Rozenholc (2006).\n\n\n\n\n\n","category":"type"},{"location":"methods/#Regular-L_2-leave-one-out-cross-validation-(L2CV_R)","page":"Supported Methods","title":"Regular L_2 leave-one-out cross-validation (L2CV_R)","text":"","category":"section"},{"location":"methods/#AutoHist.L2CV_R","page":"Supported Methods","title":"AutoHist.L2CV_R","text":"L2CV_R(; maxbins::Union{Int,Symbol} = :default)\n\nL2 cross-validation criterion for regular histograms.\n\nThe number k of bins is chosen by maximizing a leave-one-out L2 cross-validation criterion,\n\n    -2k + kfracn+1n^2sum_j=1^k N_j^2\n\nwhere n is the sample size.\n\nKeyword arguments\n\nmaxbins: Maximal number of bins for which the above criterion is evaluated. Defaults to maxbins=:default, which sets maxbins to the ceil of min(1000, 4n/log(n)^2).\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> fit(AutomaticHistogram, x, L2CV_R())\nAutomaticHistogram{LinRange{Float64, Int64}, Vector{Float64}, Vector{Int64}}\nbreaks: LinRange{Float64}(0.0, 1.0, 11)\ndensity: [0.04, 0.2, 0.56, 1.0, 1.5, 1.88, 1.98, 1.68, 0.96, 0.2]\ncounts: [2, 10, 28, 50, 75, 94, 99, 84, 48, 10]\ntype: regular\nclosed: right\na: NaN\n\nReferences\n\nThis approach to histogram density estimation was first considered by Rudemo (1982).\n\n\n\n\n\n","category":"type"},{"location":"methods/#Regular-Kullback-Leibler-leave-one-out-cross-validation-(KLCV_R)","page":"Supported Methods","title":"Regular Kullback-Leibler leave-one-out cross-validation (KLCV_R)","text":"","category":"section"},{"location":"methods/#AutoHist.KLCV_R","page":"Supported Methods","title":"AutoHist.KLCV_R","text":"KLCV_R(; maxbins::Union{Int,Symbol} = :default)\n\nKullback-Leibler cross-validation criterion for regular histograms.\n\nThe number k of bins is chosen by maximizing a leave-one-out Kullback-Leibler cross-validation criterion,\n\n    nlog(k) + sum_j=1^k N_jlog (N_j-1)\n\nwhere n is the sample size and the maximmization is over all regular partitions with N_j geq 2 for all j.\n\nKeyword arguments\n\nmaxbins: Maximal number of bins for which the above criterion is evaluated. Defaults to maxbins=:default, which sets maxbins to the ceil of min(1000, 4n/log(n)^2).\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> fit(AutomaticHistogram, x, KLCV_R())\nAutomaticHistogram{LinRange{Float64, Int64}, Vector{Float64}, Vector{Int64}}\nbreaks: LinRange{Float64}(0.0, 1.0, 11)\ndensity: [0.04, 0.2, 0.56, 1.0, 1.5, 1.88, 1.98, 1.68, 0.96, 0.2]\ncounts: [2, 10, 28, 50, 75, 94, 99, 84, 48, 10]\ntype: regular\nclosed: right\na: NaN\n\nReferences\n\nThis approach was first studied by Hall (1990).\n\n\n\n\n\n","category":"type"},{"location":"methods/#Minimum-description-length-(MDL)","page":"Supported Methods","title":"Minimum description length (MDL)","text":"","category":"section"},{"location":"methods/#AutoHist.MDL","page":"Supported Methods","title":"AutoHist.MDL","text":"MDL(; maxbins::Union{Int,Symbol} = :default)\n\nMDL criterion for regular histograms.\n\nThe number k of bins is chosen as the minimizer of an encoding length of the data, and is equivalent to the maximizer of\n\n    nlog(k) + sum_j=1^k big(N_j-frac12big)logbig(N_j-frac12big) - big(n-frack2big)logbig(n-frack2big) - frack2log(n)\n\nwhere n is the sample size and the maximmization is over all regular partitions with N_j geq 1 for all j.\n\nKeyword arguments\n\nmaxbins: Maximal number of bins for which the above criterion is evaluated. Defaults to maxbins=:default, which sets maxbins to the ceil of min(1000, 4n/log(n)^2).\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> fit(AutomaticHistogram, x, MDL())\nAutomaticHistogram{LinRange{Float64, Int64}, Vector{Float64}, Vector{Int64}}\nbreaks: LinRange{Float64}(0.0, 1.0, 11)\ndensity: [0.04, 0.2, 0.56, 1.0, 1.5, 1.88, 1.98, 1.68, 0.96, 0.2]\ncounts: [2, 10, 28, 50, 75, 94, 99, 84, 48, 10]\ntype: regular\nclosed: right\na: NaN\n\nReferences\n\nThe minimum description length principle was first applied to histogram estimation by Hall and Hannan (1988).\n\n\n\n\n\n","category":"type"},{"location":"methods/#Normalized-maximum-likelihood,-regular-(NML_R)-2","page":"Supported Methods","title":"Normalized maximum likelihood, regular (NML_R)","text":"","category":"section"},{"location":"methods/#AutoHist.NML_R","page":"Supported Methods","title":"AutoHist.NML_R","text":"NML_R(; maxbins::Union{Int,Symbol} = :default)\n\nNML_R criterion for regular histograms.\n\nThe number k of bins is chosen by maximizing a penalized likelihood,\n\nbeginaligned\n    sum_j=1^k N_jlog fracN_jmathcalI_j - frack-12log(n2) - logfracsqrtpiGamma(k2) - n^-12fracsqrt2kGamma(k2)3Gamma(k2-12) \n    - n^-1left(frac3+k(k-2)(2k+1)36 - fracGamma(k2)^2 k^29Gamma(k2-12)^2 right)\nendaligned\n\nwhere n is the sample size.\n\nKeyword arguments\n\nmaxbins: Maximal number of bins for which the above criterion is evaluated. Defaults to maxbins=:default, which sets maxbins to the ceil of min(1000, 4n/log(n)^2).\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> fit(AutomaticHistogram, x, NML_R())\nAutomaticHistogram{LinRange{Float64, Int64}, Vector{Float64}, Vector{Int64}}\nbreaks: LinRange{Float64}(0.0, 1.0, 24)\ndensity: [0.046, 0.0, 0.138, 0.184, 0.368, 0.506, 0.69, 0.874, 1.104, 1.334  …  1.978, 1.978, 1.978, 1.84, 1.61, 1.334, 0.966, 0.644, 0.276, 0.046]\ncounts: [1, 0, 3, 4, 8, 11, 15, 19, 24, 29  …  43, 43, 43, 40, 35, 29, 21, 14, 6, 1]\ntype: regular\nclosed: right\na: NaN\n\nReferences\n\nThis is a regular variant of the normalized maximum likelihood criterion considered by Kontkanen and Myllymäki (2007).\n\n\n\n\n\n","category":"type"},{"location":"methods/#Sturges'-rule","page":"Supported Methods","title":"Sturges' rule","text":"","category":"section"},{"location":"methods/#AutoHist.Sturges","page":"Supported Methods","title":"AutoHist.Sturges","text":"Sturges()\n\nSturges' rule for regular histograms.\n\nThe number k of bins is chosen as\n\n    k = lceil log_2(n) rceil + 1\n\nwhere n is the sample size.\n\nThis is the default procedure used by the hist() function in base R.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> fit(AutomaticHistogram, x, Sturges())\nAutomaticHistogram{LinRange{Float64, Int64}, Vector{Float64}, Vector{Int64}}\nbreaks: LinRange{Float64}(0.0, 1.0, 10)\ndensity: [0.054, 0.252, 0.666, 1.206, 1.71, 1.98, 1.782, 1.098, 0.252]\ncounts: [3, 14, 37, 67, 95, 110, 99, 61, 14]\ntype: regular\nclosed: right\na: NaN\n\nReferences\n\nThis classical rule is due to Sturges (1926).\n\n\n\n\n\n","category":"type"},{"location":"methods/#Freedman-and-Diaconis'-rule","page":"Supported Methods","title":"Freedman and Diaconis' rule","text":"","category":"section"},{"location":"methods/#AutoHist.FD","page":"Supported Methods","title":"AutoHist.FD","text":"FD()\n\nFreedman and Diaconis' rule for regular histograms.\n\nThe number k of bins is computed according to the formula\n\n    k = biglceilfracn^132textIQR(boldsymbolx)bigrceil\n\nwhere textIQR(boldsymbolx) is the sample interquartile range and n is the sample size.\n\nThis is the default procedure used by the histogram() function in Plots.jl.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> fit(AutomaticHistogram, x, FD())\nAutomaticHistogram{LinRange{Float64, Int64}, Vector{Float64}, Vector{Int64}}\nbreaks: LinRange{Float64}(0.0, 1.0, 16)\ndensity: [0.03, 0.09, 0.24, 0.48, 0.78, 1.08, 1.44, 1.71, 1.92, 2.01, 1.89, 1.59, 1.08, 0.54, 0.12]\ncounts: [1, 3, 8, 16, 26, 36, 48, 57, 64, 67, 63, 53, 36, 18, 4]\ntype: regular\nclosed: right\na: NaN\n\nReferences\n\nThis rule dates back to Freedman and Diaconis (1982).\n\n\n\n\n\n","category":"type"},{"location":"methods/#Scott's-rule","page":"Supported Methods","title":"Scott's rule","text":"","category":"section"},{"location":"methods/#AutoHist.Scott","page":"Supported Methods","title":"AutoHist.Scott","text":"Scott()\n\nScott's rule for regular histograms.\n\nThe number k of bins is computed according to the formula\n\n    k = biglceil hatsigma^-1(24sqrtpi)^-13n^13bigrceil\n\nwhere hatsigma is the sample standard deviation and n is the sample size.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> fit(AutomaticHistogram, x, Scott())\nAutomaticHistogram{LinRange{Float64, Int64}, Vector{Float64}, Vector{Int64}}\nbreaks: LinRange{Float64}(0.0, 1.0, 14)\ndensity: [0.026, 0.13, 0.338, 0.624, 0.988, 1.378, 1.716, 1.924, 2.002, 1.768, 1.3, 0.676, 0.13]\ncounts: [1, 5, 13, 24, 38, 53, 66, 74, 77, 68, 50, 26, 5]\ntype: regular\nclosed: right\na: NaN\n\nReferences\n\nThis classical rule is due to Scott (1979).\n\n\n\n\n\n","category":"type"},{"location":"methods/#Wand's-rule","page":"Supported Methods","title":"Wand's rule","text":"","category":"section"},{"location":"methods/#AutoHist.Wand","page":"Supported Methods","title":"AutoHist.Wand","text":"Wand(;\n    level::Int      = 2,\n    scalest::Symbol = :minim\n)\n\nWand's rule for regular histograms.\n\nA more sophisticated version of Scott's rule, Wand's rule proceeds by determining the bin width h as\n\n    h = Big(frac6hatC(f_0) nBig)^13\n\nwhere hatC(f_0) is an estimate of the functional C(f_0) = int f_0(x)^2 textdx. The corresponding number of bins is k = lceil h^-1rceil.\n\nKeyword arguments\n\nlevel: The level keyword controls the number of stages of functional estimation used to compute hatC, and can take values 0, 1, 2, 3, 4, 5, with the default value being level=2. The choice level=0 corresponds to a varation on Scott's rule, with a custom scale estimate. scalest: Estimate of scale parameter. Possible choices are :minim :stdev and :iqr. The latter two use sample standard deviation or the sample interquartile range, respectively, to estimate the scale. The default choice :minim uses the minimum of the above estimates.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 500)) .^(1/3)).^(1/3);\n\njulia> rule = Wand(scalest=:stdev, level=5);\n\njulia> fit(AutomaticHistogram, x, rule)\nAutomaticHistogram{LinRange{Float64, Int64}, Vector{Float64}, Vector{Int64}}\nbreaks: LinRange{Float64}(0.0, 1.0, 13)\ndensity: [0.024, 0.144, 0.408, 0.72, 1.128, 1.536, 1.872, 1.992, 1.848, 1.416, 0.744, 0.168]\ncounts: [1, 6, 17, 30, 47, 64, 78, 83, 77, 59, 31, 7]\ntype: regular\nclosed: right\na: NaN\n\nReferences\n\nThe full details on this method are given in Wand (1997).\n\n\n\n\n\n","category":"type"},{"location":"methods/#Index","page":"Supported Methods","title":"Index","text":"","category":"section"},{"location":"methods/","page":"Supported Methods","title":"Supported Methods","text":"Pages = [\"methods.md\"]","category":"page"},{"location":"examples/density_estimation/#Density-estimation","page":"Density estimation","title":"Density estimation","text":"","category":"section"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"The following document illustrates the use of AutoHist.jl through examples from the world of density estimation. In particular, we showcase some of the relative advantages and disadvantages of regular and irregular histogram procedures.","category":"page"},{"location":"examples/density_estimation/#Estimating-the-LogNormal-probability-density","page":"Density estimation","title":"Estimating the LogNormal probability density","text":"","category":"section"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"We start by considering an example with some simulated data from the LogNormal-distribution. To start, we fit a regular histogram to the data, using the approach of Birgé and Rozenholc (2006), which corresponds to rule=BR().","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"using AutoHist, Random, Distributions\nx = rand(Xoshiro(1812), LogNormal(), 10^4)\nh1 = fit(AutomaticHistogram, x, BR())","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"Alternatively, since the standard LogNormal pdf has known support 0infty), we can incorporate this knowledge in our histogram estimate through the support keyword.","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"h2 = fit(AutomaticHistogram, x, BR(); support=(0.0, Inf))","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"To quantify the difference of using the correct, known support in this case, we compute the integrated absolute error between the two densities in the following code snippet, which is given by int f(x) - g(x)textdx.","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"distance(h1, h2, :iae)","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"The resulting L_1 distance of 0042 indicates that the new bin origin at 0 has a moderate effect on the resulting density estimate.","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"The standard LogNormal is quite challenging to estimate well using a regular histogram procedure due to its heavy tails. These two factors make irregular methods an appealing alternative in this case. Here, we use the penalized log-likelihood approach from Rozenholc et al. (2010) with penalty R and a data-based grid to construct the histogram, (implemented in AutoHist.jl via rule = RMG_penR()).","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"h3 = fit(AutomaticHistogram, x, RMG_penR(grid=:data))","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"To compare the two approaches, we can plot the resulting histograms along with the true density:","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"using Distributions, Plots; gr() # hide\nt = LinRange(0.0, 8.5, 1000) # hide\np = plot(t, pdf(LogNormal(), t), xlabel=\"x\", label=\"True density\", color=\"blue\", lw=2.0, linestyle=:dash) # hide\np1 = plot(p, h2, ylabel=\"Density\", label=\"Regular\", alpha=0.4, color=\"red\") # hide\nxlims!(p1, -0.5, 8.5) # hide\np2 = plot(p, h3, label=\"Irregular\", alpha=0.4, color=\"black\") # hide\nxlims!(p2, -0.5, 8.5) # hide\nplot(p1, p2, layout=(1, 2), size=(670, 320)) # hide","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"The irregular procedure selects smaller bin widths near the origin, reflecting the fact that the LogNormal density is rapidly changing in this area. On the other hand, the bin widths are larger in the flatter region in the right tail of the density. Both histogram procedures provide quite reasonable estimates of the density, owing to the fairly large sample size.","category":"page"},{"location":"examples/density_estimation/#Mode-hunting","page":"Density estimation","title":"Mode hunting","text":"","category":"section"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"In an exploratory data analysis setting, identifying key features in a dataset such as modes is frequently of great interest to statisticians and practitioners alike. Unfortunately, most popular regular histogram have been designed with good performance in terms of statistical risk with respect to classical, integral-based loss functions, which typically results in a large amount of spurious histogram modes in regions where the true density is flat and in the tails of the density (Scott, 1992). In practice, this means that a data-analyst must use subjective judgement to infer whether a regular histogram estimate is indicative of a mode being present or not. If the presence of a mode is deemed likely, subjective visual smoothing is typically required to get a rough idea of its location. In contrast, some irregular histogram procedures have been shown empirically to perform quite well with regard to automatic mode detection in cases where the true density has a smallish amount of well-separated modes, see e.g. Davies et al. (2009); Li et al. (2020); Simensen et al. (2025).","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"To illustrate the advantage of irregular histograms when it comes to mode identification, we will consider an example where we attempt to locate the modes of the Harp density of Li et al. (2020), plotted below.","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"module TestDistributions # hide\n\nusing Distributions # hide\nusing Random # hide\nusing Plots # hide\n\nimport Distributions: pdf, cdf # hide\nimport Random: rand # hide\n\nexport pdf, cdf, rand, peaks, plot_test_distribution, Harp # hide\n\nstruct Harp <: ContinuousUnivariateDistribution end # hide\nfunction pdf(d::Harp, x::Real) # hide\n    means = [0.0, 5.0, 15.0, 30.0, 60.0] # hide\n    sds = [0.5, 1.0, 2.0, 4.0, 8.0] # hide\n    dens = 0.0 # hide\n    for j = 1:5 # hide\n        dens = dens + 0.2 * pdf(Normal(means[j], sds[j]), x) # hide\n    end # hide\n    return dens # hide\nend # hide\nfunction cdf(d::Harp, x::Real) # hide\n    means = [0.0, 5.0, 15.0, 30.0, 60.0] # hide\n    sds = [0.5, 1.0, 2.0, 4.0, 8.0] # hide\n    cum = 0.0 # hide\n    for j = 1:5 # hide\n        cum += 0.2 * cdf(Normal(means[j], sds[j]), x) # hide\n    end # hide\n    return cum # hide\nend # hide\nfunction rand(rng::AbstractRNG, d::Harp) # hide\n    means = [0.0, 5.0, 15.0, 30.0, 60.0] # hide\n    sds = [0.5, 1.0, 2.0, 4.0, 8.0] # hide\n    j = rand(rng, DiscreteUniform(1, 5)) # hide\n    return rand(rng, Normal(means[j], sds[j])) # hide\nend # hide\nfunction peaks(d::Harp) # hide\n    return Float64[0.0, 5.0, 15.001837158203125, 30.003319148997427, 60.000555419921874] # hide\nend # hide\nfunction plot_test_distribution(d::Harp) # hide\n    if d != Harp() # hide\n        throw(ArgumentError(\"Supplied distribution $d is not supported.\")) # hide\n    end # hide\n    dom = LinRange(-5.0, 80.0, 5000) # hide\n    x_lims = [-5.0, 80.0] # hide\n    p = plot() # hide\n    t = dom # hide\n    plot!(p, t, pdf.(d, t), label=\"\", ylims=(0.0, Inf), titlefontsize=10, color=\"blue\", linestyle=:dash) # hide\n    xlims!(p, x_lims...) # hide\n    return p # hide\nend # hide\nend # hide","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"using Plots; gr() # hide\nimport .TestDistributions as TD # hide\np = TD.plot_test_distribution(TD.Harp()) # hide\nplot!(p, title=\"\", xlab=\"x\", ylab=\"Density\") # hide\nplot(p, size=(650, 310)) # hide","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"The Harp density has 5 peaks with varying degrees of sharpness, with the location of each mode becoming gradually more difficult to locate in absolute terms as we move rightward on the x-axis. In the numerical experiment to follow, we generate a random sample of size n = 5000 from the Harp density, and fit both an irregular and a regular histogram to the data. Motivated by the results of the simulation studies in Davies et al. (2009) and Simensen et al. (2025), we have used the BIC criterion to draw the regular histogram, and the random irregular histogram method (rule=RIH()), as both of these have been shown to perform relatively well compared to their respective regular/irregular counterparts in automatic mode identification. To access the Harp distribution, we use the TestDistributions package, which can be found in the following github repository.","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"import .TestDistributions as TD # hide\nusing AutoHist, Distributions, Random # hide\nx = rand(Xoshiro(1812), TD.Harp(), 5000) # hide\n\nh_reg = fit(AutomaticHistogram, x, BIC()) # hide\nh_irr = fit(AutomaticHistogram, x, RIH()) # hide","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"import TestDistributions as TD\nusing AutoHist, Distributions, Random\nx = rand(Xoshiro(1812), TD.Harp(), 5000)\n\nh_reg = fit(AutomaticHistogram, x, BIC())\nh_irr = fit(AutomaticHistogram, x, RIH())","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"We can now add the two fitted histograms along with the location of their modes to the plot of the Harp density above:","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"using Plots; gr() # hide\np = TD.plot_test_distribution(TD.Harp()) # hide\nplot!(p, title=\"\", xlab=\"x\", ylab=\"Density\") # hide\np1 = plot(p, h_reg, title=\"Regular (BIC)\", color=\"red\", alpha=0.4, label=\"\", ylims=[-0.015, 0.17]) # hide\nscatter!(p1, peaks(h_reg), fill(-0.008, length(h_reg)), color=\"red\", label=\"\", ms=3) # hide\np2 = plot(p, h_irr, title=\"Irregular (RIH)\", color=\"black\", alpha=0.4, label=\"\", ylims=[-0.015, 0.17]) # hide\nscatter!(p2, peaks(h_irr), fill(-0.008, length(h_irr)), color=\"black\", label=\"\", ms=3) # hide\nplot(p1, p2, layout=(2, 1), size=(670, 650)) # hide","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"The spatial inhomogeneity of the Harp density causes some trouble for the regular histogram, as the use of a global bin width leads to undersmoothing near the sharpest mode and oversmoothing near the flattest mode and in the tails. In particular, the regular estimate introduces many spurious modes near the rightmost mode of the true density, making it difficult to exactly infer the location and the number of modes in the region surrounding it. In contrast, the irregular histogram estimate is able to correctly infer the number of modes in the density automatically, with no subjective judgments required. The good mode-finding behavior is in this case enabled by the fact that the bin widths are able adapt to the local smoothness of the true density, resulting in more smoothing near modes. To inspect the accuracy of the inferred modes, we print the locations of the true modes and the peaks of the irregular histogram.","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"println(\"True modes: \", TD.Harp() |> TD.peaks |> x -> round.(x; digits=2))\nprintln(\"AutoHist modes: \", h_irr |> peaks |> x -> round.(x; digits=2))","category":"page"},{"location":"examples/density_estimation/","page":"Density estimation","title":"Density estimation","text":"We observe that all of the histogram peaks are quite close to a corresponding true mode, especially when taking the sharpness of each individual peak into account.","category":"page"},{"location":"#AutoHist.jl-Documentation","page":"Introduction","title":"AutoHist.jl Documentation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Fast automatic histogram construction. Supports a plethora of regular and irregular histogram procedures.","category":"page"},{"location":"#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Despite being the oldest nonparametric density estimator, the histogram remains widespread in use even to this day. Regrettably, the quality of a histogram density estimate is rather sensitive to the choice of partition used to draw the histogram, which has led to the development of automatic histogram methods that select the partition based on the sample itself. Unfortunately, most default histogram plotting software only support a few regular automatic histogram procedures, where all the bins are of equal length, and use very simple plug-in rules by default to compute the number of bins, frequently leading to poor density estimates for non-normal data. Moreover, fast and fully automatic irregular histogram methods are rarely supported by default plotting software, which has prevented their adoption by practitioners.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"The AutoHist.jl package makes it easy to construct both regular and irregular histograms automatically based on a given one-dimensional sample. It currently supports 8 different methods for irregular histograms and 12 criteria for regular histograms from the statistical literature. In addition, the package provides a number of convenience functions for automatic histograms, such as methods for evaluating the histogram probability density function or identifying the location of modes.","category":"page"},{"location":"#Quick-Start","page":"Introduction","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"The main functions exported by this package are fit and autohist, which lets the user to fit a histogram to 1-dimensional data with an automatic and data-based choice of bins. The following short example shows how the fit method is used in practice","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"using AutoHist, Random, Distributions\nx = rand(Xoshiro(1812), Normal(), 10^6) # simulate some data\nh_irr = fit(AutomaticHistogram, x)      # compute an automatic irregular histogram (default method)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"The third argument to fit controls the rule used to select the histogram partition, with the default being the RIH method. To fit an automatic histogram with a specific rule, e.g. Knuth's rule, all we have to do is change the value of the rule argument.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"h_reg = fit(AutomaticHistogram, x, Knuth()) # compute an automatic regular histogram","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"The above calls to fit return an object of type AutomaticHistogram, with weights normalized so that the resulting histograms are probability densities. This type represents the histogram in a similar fashion to StatsBase.Histogram, but has more fields to enable the use of several convenience functions.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"h_irr","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Alternatively, an automatic histogram can be fitted to data through the autohist method, which serves as an alias for fit(AutomaticHistogram, x, rule; kwargs...).","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"h_reg = autohist(x, Knuth())     # equivalent to fit(AutomaticHistogram, x, Knuth())","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"AutomaticHistogram objects are compatible with Plots.jl, which lets us easily plot the two histograms resulting from the above code snippet via e.g. Plots.plot(h_irr). To show both histograms side by side, we can create a plot as follows:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"import Plots; Plots.gr()\np_irr = Plots.plot(h_irr, xlabel=\"x\", ylabel=\"Density\", title=\"Irregular\", alpha=0.4, color=\"black\", label=\"\")\np_reg = Plots.plot(h_reg, xlabel=\"x\", title=\"Regular\", alpha=0.4, color=\"red\", label=\"\")\nPlots.plot(p_irr, p_reg, layout=(1, 2), size=(670, 320))","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Alternatively, Makie.jl can also be used to make graphical displays of the fitted histograms via e.g. Makie.plot(h_irr). To produce a plot similar to the above display, we may for instance do the following:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"import CairoMakie, Makie # using the CairoMakie backend\nfig = Makie.Figure(size=(670, 320))\nax1 = Makie.Axis(fig[1, 1], title=\"Irregular\", xlabel=\"x\", ylabel=\"Density\")\nax2 = Makie.Axis(fig[1, 2], title=\"Regular\", xlabel=\"x\")\np_irr = Makie.plot!(ax1, h_irr, alpha=0.4, color=\"black\")\np_reg = Makie.plot!(ax2, h_reg, alpha=0.4, color=\"red\")\nfig","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"For a more detailed account of the plotting capabilities offered by AutoHist, please consult the plotting tutorial.","category":"page"},{"location":"#Supported-methods","page":"Introduction","title":"Supported methods","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Both the regular and the irregular procedure support a large number of criteria to select the histogram partition. The keyword argument rule controls the criterion used to choose the best partition, and includes the following options:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Regular Histograms:\nKnuth's rule, Random regular histogram, RRH\nL2 cross-validation, L2CV_R\nKullback-Leibler cross-validation, KLCV_R\nAkaike's information criterion, AIC\nThe Bayesian information criterion, BIC\nBirgé and Rozenholc's criterion, BR\nNormalized Maximum Likelihood, NML_R\nMinimum Description Length. MDL\nSturges' rule, Sturges\nFreedman and Diaconis' rule, FD\nScott's rule, Scott\nWand's rule, Wand\nIrregular Histograms:\nRandom irregular histogram, RIH\nL2 cross-validation, L2CV_I\nKullback-Leibler cross-validation, KLCV_I\nRozenholc et al. penalty A, RMG_penA\nRozenholc et al. penalty B, RMG_penB\nRozenholc et al. penalty R, RMG_penR\nNormalized Maximum Likelihood, NML_I\nBayesian Blocks BayesBlocks","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"A description of each method along with references for each method can be found on the methods page.","category":"page"},{"location":"algorithms/#Algorithms-for-constructing-irregular-histograms","page":"Algorithms","title":"Algorithms for constructing irregular histograms","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Constructing data-adaptive irregular histograms is in general a difficult problem from a computational perspective, and as a result computing the exact optimal partition is often impractical for larger sample sizes. This package solves the problem of irregular histogram construction via heuristics that combine a greedy search procedure with dynamic programming techniques to quickly compute a nearly optimal partition. Examples showcasing the use of the provided algorithms in toy problems can be found here. Note that the default option for the alg keyword offers a reasonable tradeoff between accuracy and computational efficiency, and in cases where one simply wants to draw an irregular histogram quickly for a given dataset, these default choices will typically yield a reasonable density estimate within a reasonable amount of time. However, if better performance or additional accuracy is desired, then a more fine-tuned approach to selecting the algorithm used and its hyperparameters is needed.","category":"page"},{"location":"algorithms/#Problem-description","page":"Algorithms","title":"Problem description","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"All the irregular histogram methods supported by this library are the product of solving an optimization problem of the form","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"    max_1leq kleq k_nmax_boldsymbolt_0k Bigsum_j=1^k Phi(tau_n t_j-1 tau_n t_j + Psi(k)Big","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"where the inner maximum is over integer vectors boldsymbolt_0k satisfying 0 = t_0  t_1  cdots  t_k-1  t_k = k_n and tau_njcolon 0leq j leq k_n are the candidate cutpoints between the partition intervals. While it is generally desirable to use a moderate number of possible cutpoints relative to the number of samples, this comes at a heavy computational cost if an exact solution of the above optimization is desired as the runtime complexity of the algorithm is cubic in the number of candidates. We note that for the special case where Psi(k) = 0 for all k, a more efficient algorithm is available, allowing for a quadratic-time solution. In both cases, computing the exact solution becomes unfeasible for larger sample sizes.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"To ease the computational burden we adopt a greedy search heuristic to construct a subset of possible cutpoints when the number of candidates is large, and then run the optimization algorithm of choice on this smaller set. To find the optimal partition for a given set of candidate cutpoints, this package includes two solvers based on dynamic programming. The algorithm used can be controlled via the alg keyword passed to the rule argument of fit, see the methods page.","category":"page"},{"location":"algorithms/#Segment-neighbourhood","page":"Algorithms","title":"Segment neighbourhood","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"The choice alg = SegNeig() results in the use of the exact dynamic programming algorithm of Kanazawa (1988) to find the optimal partition. Runtime complexity is cubic in the number of candidate cutpoints.","category":"page"},{"location":"algorithms/#AutoHist.SegNeig","page":"Algorithms","title":"AutoHist.SegNeig","text":"SegNeig(; greedy::Bool=true, gr_maxbins::Union{Int, Symbol}=:default)\n\nThe segment neighbourhood algorithm for constructing an irregular histogram.\n\nKeyword arguments\n\ngreedy: Boolean indicating whether or not the greedy cutpoint selection strategy of Rozenholc et al. (2010) should be used to select a smaller number of candidate cutpoints prior to running the dynamic programming algorithm. Defaults to true.\ngr_maxbins: Number of candidate cutpoints chosen by the greedy algorithm. Supplying gr_maxbins=:default results in the selection of at most max 500 n^13 +1 candidate cutpoints (including edges).\n\nnote: Note\nThis algorithm can be quite slow for large datasets when the greedy keyword is set to false.\n\nExamples\n\njulia> x = LinRange(eps(), 1.0-eps(), 5000) .^(1.0/4.0);\n\njulia> h = fit(AutomaticHistogram, x, RIH(alg = SegNeig(greedy=true, gr_maxbins=200)))\nAutomaticHistogram{Vector{Float64}, Vector{Float64}, Vector{Int64}}\nbreaks: [0.0001220703125, 0.17763663029325183, 0.29718725232110504, 0.4022468898607337, 0.4928155429121377, 0.5797614498414855, 0.6667073567708333, 0.7572760098222373, 0.8405991706295289, 0.9239223314368207, 1.0]\ndensity: [0.006626835974128547, 0.057821970706400425, 0.17596277991076312, 0.36279353706969375, 0.6214544825215076, 0.9730458529384184, 1.4481767793920146, 2.0440057561776532, 2.7513848134529346, 3.5648421829491657]\ncounts: [5, 34, 92, 164, 270, 423, 656, 852, 1147, 1357]\ntype: irregular\nclosed: right\na: 5.0\n\njulia> h = fit(AutomaticHistogram, x, RIH(alg = SegNeig(greedy=false)))\nAutomaticHistogram{Vector{Float64}, Vector{Float64}, Vector{Int64}}\nbreaks: [0.0001220703125, 0.17763663029325183, 0.29718725232110504, 0.4022468898607337, 0.4928155429121377, 0.5797614498414855, 0.6667073567708333, 0.7572760098222373, 0.8405991706295289, 0.9202995853147645, 1.0]\ndensity: [0.006626835974128547, 0.057821970706400425, 0.17596277991076312, 0.36279353706969375, 0.6214544825215076, 0.9730458529384184, 1.4481767793920146, 2.0440057561776532, 2.733509595364622, 3.545742066060377]\ncounts: [5, 34, 92, 164, 270, 423, 656, 852, 1090, 1414]\ntype: irregular\nclosed: right\na: 5.0\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Optimal-partitioning","page":"Algorithms","title":"Optimal partitioning","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"The choice alg = OptPart() results in the use of the exact optimal partitioning algorithm of Jackson et al. (2005) to find the optimal histogram partition. This algorithm solves a less general optimization problem than the SegNeig algorithm. Runtime complexity is quadratic in the number of candidate cutpoints.","category":"page"},{"location":"algorithms/#AutoHist.OptPart","page":"Algorithms","title":"AutoHist.OptPart","text":"OptPart(; greedy::Bool=true, gr_maxbins::Union{Int, Symbol}=:default)\n\nThe optimal partitioning algorithm for constructing an irregular histogram.\n\nKeyword arguments\n\ngreedy: Boolean indicating whether or not the greedy cutpoint selection strategy of Rozenholc et al. (2010) should be used to select a smaller number of candidate cutpoints prior to running the dynamic programming algorithm. Defaults to true.\ngr_maxbins: Number of candidate cutpoints chosen by the greedy algorithm. Supplying gr_maxbins=:default results in the selection of at most max  3000 n^12 +1 candidate cutpoints (including edges).\n\nnote: Note\nThis algorithm can be quite slow for large datasets when the greedy keyword is set to false.\n\nExamples\n\njulia> x = LinRange(eps(), 1.0-eps(), 1000) .^(1.0/4.0);\n\njulia> h = fit(AutomaticHistogram, x, L2CV_I(alg = OptPart(greedy=true, gr_maxbins=100)))\nAutomaticHistogram{Vector{Float64}, Vector{Float64}, Vector{Int64}}\nbreaks: [0.0001220703125, 0.16676839192708331, 0.2977047874813988, 0.4048345656622024, 0.5119643438430059, 0.6071908133370535, 0.6786106654575893, 0.7619338262648809, 0.8452569870721726, 0.9285801478794643, 1.0]\ndensity: [0.006000732511292883, 0.05346107146424569, 0.17735498311154516, 0.3920478574044684, 0.7035858869490909, 1.0641298986692698, 1.5001831278232214, 2.0762534489073383, 2.7963413502624808, 3.5984392626052997]\ncounts: [1, 7, 19, 42, 67, 76, 125, 173, 233, 257]\ntype: irregular\nclosed: right\na: NaN\n\njulia> h = fit(AutomaticHistogram, x, L2CV_I(alg = OptPart(greedy=false)))\nAutomaticHistogram{Vector{Float64}, Vector{Float64}, Vector{Int64}}\nbreaks: [0.0001220703125, 0.16676839192708331, 0.2977047874813988, 0.4048345656622024, 0.5119643438430059, 0.6071908133370535, 0.6786106654575893, 0.7619338262648809, 0.8452569870721726, 0.9285801478794643, 1.0]\ndensity: [0.006000732511292883, 0.05346107146424569, 0.17735498311154516, 0.3920478574044684, 0.7035858869490909, 1.0641298986692698, 1.5001831278232214, 2.0762534489073383, 2.7963413502624808, 3.5984392626052997]\ncounts: [1, 7, 19, 42, 67, 76, 125, 173, 233, 257]\ntype: irregular\nclosed: right\na: NaN\n\n\n\n\n\n","category":"type"}]
}
